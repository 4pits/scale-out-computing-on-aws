{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Scale-Out Computing on AWS is a solution that helps customers more easily deploy and operate a multiuser environment for computationally intensive workflows. The solution features a large selection of compute resources; fast network backbone; unlimited storage; and budget and cost management directly integrated within AWS. The solution also deploys a user interface (UI) and automation tools that allows you to create your own queues, scheduler resources, Amazon Machine Images (AMIs), software, and libraries. This solution is designed to provide a production ready reference implementation to be a starting point for deploying an AWS environment to run scale-out workloads, allowing you to focus on running simulations designed to solve complex computational problems. Easy installation \u00b6 Installation of your Scale-Out Computing on AWS cluster is fully automated and managed by CloudFormation Did you know? You can have multiple Scale-Out Computing on AWS clusters on the same AWS account Scale-Out Computing on AWS comes with a list of unique tags, making resource tracking easy for AWS Administrators Access your cluster in 1 click \u00b6 You can access your Scale-Out Computing on AWS cluster either using DCV (Desktop Cloud Visualization) 1 or through SSH. Simple Job Submission \u00b6 Scale-Out Computing on AWS supports a list of parameters designed to simplify your job submission on AWS. Advanced users can either manually choose compute/storage/network configuration for their job or simply ignore these parameters and let Scale-Out Computing on AWS choose the most optimal hardware (defined by the HPC administrator) # Advanced Configuration user@host$ qsub -l instance_type = c5n.18xlarge \\ -l instance_ami = ami-123abcde -l nodes = 2 -l scratch_size = 300 -l efa_support = true -l spot_price = 1 .55 myscript.sh # Basic Configuration user@host$ qsub myscript.sh Refer to this page for tutorial and examples Refer to this page to list all supported parameters OS agnostic and support for custom AMI \u00b6 Customers can integrate their Centos7/Rhel7/AmazonLinux2 AMI automatically by simply using -l instance_ami=<ami_id> at job submission. There is no limitation in term of AMI numbers (you can have 10 jobs running simultaneously using 10 different AMIs) AMI using OS different than the scheduler In case your AMI is different than your scheduler host, you can specify the OS manually to ensure packages will be installed based on the node distribution. In this example, we assume your Scale-Out Computing on AWS deployment was done using AmazonLinux2, but you want to submit a job on your personal RHEL7 AMI user@host$ qsub -l instance_ami = <ami_id> -l base_os = rhel7 myscript.sh Scale-Out Computing on AWS AMI requirements When you use a custom AMI, just make sure that your AMI does not use /apps, /scratch or /data partitions as Scale-Out Computing on AWS will need to use these locations during the deployment Budgets and Cost Management \u00b6 You can review your HPC costs filtered by user/team/project/queue very easily using AWS Cost Explorer. Scale-Out Computing on AWS also supports AWS Budget and let you create budgets assigned to user/team/project or queue. To prevent over-spend, Scale-Out Computing on AWS includes hooks to restrict job submission when customer-defined budget has expired. Detailed Cluster Analytics \u00b6 Scale-Out Computing on AWS includes ElasticSearch and automatically ingest job and hosts data in real-time for accurate visualization of your cluster activity. Don't know where to start? Scale-Out Computing on AWS includes dashboard examples if you are not familiar with ElasticSearch or Kibana. 100% Customizable \u00b6 Scale-Out Computing on AWS is built entirely on top of AWS and can be customized by users as needed. Most of the logic is based of CloudFormation templates and EC2 User Data scripts. More importantly, the entire Scale-Out Computing on AWS codebase is open-source and available on Github . Persistent and Unlimited Storage \u00b6 Scale-Out Computing on AWS includes two unlimited EFS storage (/apps and /data). Customers also have the ability to deploy high-speed SSD EBS disks or FSx for Lustre as scratch location on their compute nodes. Refer to this page to learn more about the various storage options offered by Scale-Out Computing on AWS Centralized user-management \u00b6 Customers can create unlimited LDAP users and groups . By default Scale-Out Computing on AWS includes a default LDAP account provisioned during installation as well as a \"Sudoers\" LDAP group which manage SUDO permission on the cluster. Support for network licenses \u00b6 Scale-Out Computing on AWS includes a FlexLM-enabled script which calculate the number of licenses for a given features and only start the job/provision the capacity when enough licenses are available. Automatic Errors Handling \u00b6 Scale-Out Computing on AWS performs various dry run checks before provisioning the capacity. However, it may happen than AWS can't fullfill all requests (eg: need 5 instances but only 3 can be provisioned due to capacity shortage within a placement group). In this case, Scale-Out Computing on AWS will try to provision the capacity for 30 minutes. After 30 minutes, and if the capacity is still not available, Scale-Out Computing on AWS will automatically reset the request and try to provision capacity in a different availability zone. Web UI \u00b6 Scale-Out Computing on AWS includes a simple web ui designed to simplify user interactions such as: Start/Stop DCV sessions in 1 click Download private key in both PEM or PPK format Check the queue status in real-time Add/Remove LDAP users Access the analytic dashboard Custom fair-share \u00b6 Each user is given a score which vary based on: Number of job in the queue Time each job is queued Priority of each job Type of instance Job that belong to the user with the highest score will start next DCV is a remote visualization technology that enables users to easily and securely connect to graphic-intensive 3D applications hosted on a remote high-performance server.* \u21a9","title":"What is Scale-Out Computing on AWS ?"},{"location":"#easy-installation","text":"Installation of your Scale-Out Computing on AWS cluster is fully automated and managed by CloudFormation Did you know? You can have multiple Scale-Out Computing on AWS clusters on the same AWS account Scale-Out Computing on AWS comes with a list of unique tags, making resource tracking easy for AWS Administrators","title":"Easy installation"},{"location":"#access-your-cluster-in-1-click","text":"You can access your Scale-Out Computing on AWS cluster either using DCV (Desktop Cloud Visualization) 1 or through SSH.","title":"Access your cluster in 1 click"},{"location":"#simple-job-submission","text":"Scale-Out Computing on AWS supports a list of parameters designed to simplify your job submission on AWS. Advanced users can either manually choose compute/storage/network configuration for their job or simply ignore these parameters and let Scale-Out Computing on AWS choose the most optimal hardware (defined by the HPC administrator) # Advanced Configuration user@host$ qsub -l instance_type = c5n.18xlarge \\ -l instance_ami = ami-123abcde -l nodes = 2 -l scratch_size = 300 -l efa_support = true -l spot_price = 1 .55 myscript.sh # Basic Configuration user@host$ qsub myscript.sh Refer to this page for tutorial and examples Refer to this page to list all supported parameters","title":"Simple Job Submission"},{"location":"#os-agnostic-and-support-for-custom-ami","text":"Customers can integrate their Centos7/Rhel7/AmazonLinux2 AMI automatically by simply using -l instance_ami=<ami_id> at job submission. There is no limitation in term of AMI numbers (you can have 10 jobs running simultaneously using 10 different AMIs) AMI using OS different than the scheduler In case your AMI is different than your scheduler host, you can specify the OS manually to ensure packages will be installed based on the node distribution. In this example, we assume your Scale-Out Computing on AWS deployment was done using AmazonLinux2, but you want to submit a job on your personal RHEL7 AMI user@host$ qsub -l instance_ami = <ami_id> -l base_os = rhel7 myscript.sh Scale-Out Computing on AWS AMI requirements When you use a custom AMI, just make sure that your AMI does not use /apps, /scratch or /data partitions as Scale-Out Computing on AWS will need to use these locations during the deployment","title":"OS agnostic and support for custom AMI"},{"location":"#budgets-and-cost-management","text":"You can review your HPC costs filtered by user/team/project/queue very easily using AWS Cost Explorer. Scale-Out Computing on AWS also supports AWS Budget and let you create budgets assigned to user/team/project or queue. To prevent over-spend, Scale-Out Computing on AWS includes hooks to restrict job submission when customer-defined budget has expired.","title":"Budgets and Cost Management"},{"location":"#detailed-cluster-analytics","text":"Scale-Out Computing on AWS includes ElasticSearch and automatically ingest job and hosts data in real-time for accurate visualization of your cluster activity. Don't know where to start? Scale-Out Computing on AWS includes dashboard examples if you are not familiar with ElasticSearch or Kibana.","title":"Detailed Cluster Analytics"},{"location":"#100-customizable","text":"Scale-Out Computing on AWS is built entirely on top of AWS and can be customized by users as needed. Most of the logic is based of CloudFormation templates and EC2 User Data scripts. More importantly, the entire Scale-Out Computing on AWS codebase is open-source and available on Github .","title":"100% Customizable"},{"location":"#persistent-and-unlimited-storage","text":"Scale-Out Computing on AWS includes two unlimited EFS storage (/apps and /data). Customers also have the ability to deploy high-speed SSD EBS disks or FSx for Lustre as scratch location on their compute nodes. Refer to this page to learn more about the various storage options offered by Scale-Out Computing on AWS","title":"Persistent and Unlimited Storage"},{"location":"#centralized-user-management","text":"Customers can create unlimited LDAP users and groups . By default Scale-Out Computing on AWS includes a default LDAP account provisioned during installation as well as a \"Sudoers\" LDAP group which manage SUDO permission on the cluster.","title":"Centralized user-management"},{"location":"#support-for-network-licenses","text":"Scale-Out Computing on AWS includes a FlexLM-enabled script which calculate the number of licenses for a given features and only start the job/provision the capacity when enough licenses are available.","title":"Support for network licenses"},{"location":"#automatic-errors-handling","text":"Scale-Out Computing on AWS performs various dry run checks before provisioning the capacity. However, it may happen than AWS can't fullfill all requests (eg: need 5 instances but only 3 can be provisioned due to capacity shortage within a placement group). In this case, Scale-Out Computing on AWS will try to provision the capacity for 30 minutes. After 30 minutes, and if the capacity is still not available, Scale-Out Computing on AWS will automatically reset the request and try to provision capacity in a different availability zone.","title":"Automatic Errors Handling"},{"location":"#web-ui","text":"Scale-Out Computing on AWS includes a simple web ui designed to simplify user interactions such as: Start/Stop DCV sessions in 1 click Download private key in both PEM or PPK format Check the queue status in real-time Add/Remove LDAP users Access the analytic dashboard","title":"Web UI"},{"location":"#custom-fair-share","text":"Each user is given a score which vary based on: Number of job in the queue Time each job is queued Priority of each job Type of instance Job that belong to the user with the highest score will start next DCV is a remote visualization technology that enables users to easily and securely connect to graphic-intensive 3D applications hosted on a remote high-performance server.* \u21a9","title":"Custom fair-share"},{"location":"access-soca-cluster/","text":"Info Backend storage on Scale-Out Computing on AWS is persistent. You will have access to the same filesystem ($HOME, /data and /apps) whether you access your cluster using SSH, Web Remote Desktop or Native Remote Desktop SSH access \u00b6 To access your Scale-Out Computing on AWS cluster using SSH protocol, simply click \"SSH Access\" on the left sidebar and follow the instructions. Scale-Out Computing on AWS will let you download your private key either in PEM or PPK format. SSH to an instance in a Private Subnet \u00b6 If you need to access an instance that is in a Private (non-routable) Subnet, you can use ssh-agent to do this: $ ssh-add -K ~/Keys/my_key_region.pem Identity added: /Users/username/Keys/my_key_region.pem ( /Users/username/Keys/my_key_region.pem ) $ ssh-add -L <you should see your ssh key here> Now use - A with ssh and this will forward the key with your ssh login: $ ssh -A -i ~/Keys/my_key_region.pem centos@111.222.333.444 Now that you have your key forwarded, you can login to an instance that is in the Private Subnet: $ ssh centos@ip-10-20-111-222 Graphical access using DCV \u00b6 To access your Scale-Out Computing on AWS cluster using a full remote desktop experience, click \"Graphical Access\" on the left sidebar. By default you are authorized to have 4 sessions (EC2 instances). Session Validity \u00b6 You can choose how long your session will be valid. This parameter can be customized as needed Session type \u00b6 You can choose the type of session you want to deploy, depending your needs. This parameter can be customized as needed Access your session \u00b6 After you click \"Launch my session\", a new \"desktop\" job is sent to the queue. Scale-Out Computing on AWS will then provision the capacity and install all required packages including Gnome. You will see an informational message asking you to wait up to 20 minutes before being able to access your remote desktop. Once your session is ready, the message will automatically be updated with the connection information You can access your session directly on your browser You can also download the NICE DCV Native Clients for Mac / Linux and Windows and access your session directly through them","title":"How to access Scale-Out Computing on AWS"},{"location":"access-soca-cluster/#ssh-access","text":"To access your Scale-Out Computing on AWS cluster using SSH protocol, simply click \"SSH Access\" on the left sidebar and follow the instructions. Scale-Out Computing on AWS will let you download your private key either in PEM or PPK format.","title":"SSH access"},{"location":"access-soca-cluster/#ssh-to-an-instance-in-a-private-subnet","text":"If you need to access an instance that is in a Private (non-routable) Subnet, you can use ssh-agent to do this: $ ssh-add -K ~/Keys/my_key_region.pem Identity added: /Users/username/Keys/my_key_region.pem ( /Users/username/Keys/my_key_region.pem ) $ ssh-add -L <you should see your ssh key here> Now use - A with ssh and this will forward the key with your ssh login: $ ssh -A -i ~/Keys/my_key_region.pem centos@111.222.333.444 Now that you have your key forwarded, you can login to an instance that is in the Private Subnet: $ ssh centos@ip-10-20-111-222","title":"SSH to an instance in a Private Subnet"},{"location":"access-soca-cluster/#graphical-access-using-dcv","text":"To access your Scale-Out Computing on AWS cluster using a full remote desktop experience, click \"Graphical Access\" on the left sidebar. By default you are authorized to have 4 sessions (EC2 instances).","title":"Graphical access using DCV"},{"location":"access-soca-cluster/#session-validity","text":"You can choose how long your session will be valid. This parameter can be customized as needed","title":"Session Validity"},{"location":"access-soca-cluster/#session-type","text":"You can choose the type of session you want to deploy, depending your needs. This parameter can be customized as needed","title":"Session type"},{"location":"access-soca-cluster/#access-your-session","text":"After you click \"Launch my session\", a new \"desktop\" job is sent to the queue. Scale-Out Computing on AWS will then provision the capacity and install all required packages including Gnome. You will see an informational message asking you to wait up to 20 minutes before being able to access your remote desktop. Once your session is ready, the message will automatically be updated with the connection information You can access your session directly on your browser You can also download the NICE DCV Native Clients for Mac / Linux and Windows and access your session directly through them","title":"Access your session"},{"location":"install-soca-cluster/","text":"Download Scale-Out Computing on AWS \u00b6 Scale-Out Computing on AWS is open-source and available on Github ( https://github.com/awslabs/scale-out-computing-on-aws ). To get started, simply clone the repository: # Clone using HTTPS user@host: git clone https://github.com/awslabs/scale-out-computing-on-aws . # Clone using SSH user@host: git clone git@github.com:awslabs/scale-out-computing-on-aws.git . Build your release \u00b6 Once you have cloned your repository, execute source / manual_build . py using either python2 or python3 user@host: python3 source/manual_build.py ====== Scale-Out Computing on AWS Build ====== > Generated unique ID for build: r6l1 > Creating temporary build folder ... > Copying required files ... > Creating archive for build id: r6l1 ====== Build COMPLETE ====== ====== Installation Instructions ====== 1 : Create or use an existing S3 bucket on your AWS account ( eg: 'mysocacluster' ) 2 : Drag & Drop source/dist/r6l1 to your S3 bucket ( eg: ' mysocacluster/dist/r6l1 ) 3 : Launch CloudFormation and use scale-out-computing-on-aws.template as base template 4 : Enter your cluster information. Press Enter key to close .. This command create a build ( r6l1 in this example) under source / dist /< build_id > Upload to S3 \u00b6 Go to your Amazon S3 bash and click \"Create Bucket\" Choose a name and a region then click \"Create\" Avoid un-necessary charge It's recommended to create your bucket in the same region as your are planning to use Scale-Out Computing on AWS to avoid Cross-Regions charge ( See Data Transfer ) Once your bucket is created, select it and click \"Upload\". Simply drag and drop your build folder ( r6l1 in this example) to upload the content of the folder to S3. Info You can use the same bucket to host multiple Scale-Out Computing on AWS clusters Locate the install template \u00b6 On your S3 bucket, click on the folder you just uploaded. Your install template is located under < S3_BUCKET_NAME >/< BUILD_ID >/ scale - out - computing - on - aws . template . Click on the object to retrieve the \"Object URL\" Install Scale-Out Computing on AWS \u00b6 Open CloudFormation bash and select \"Create Stack\". Copy the URL of your install template and click \"Next\". Requirements No uppercase in stack name Stack name is limited to 20 characters maximum (note: we automatically add soca- prefix) Not supported on regions with less than 3 AZs (Canada / Northern California) If you hit any issue during the installation, refer to the 'CREATE_FAILED' component and find the root cause by referring at \"Physical ID\" Under stack details, choose the stack name (do not use uppercase or it will break your ElasticSearch cluster). Install Parameters: Specify your S3 bucket you have uploaded your build ( my - soca - hpc - test in this example) as well as the name of your build ( r6l1 in this example). If you haven't uploaded your build on your S3 root level, make sure you specify the entire file hierarchy. Environment Parameters: Choose your Linux Distribution, instance type for your master host, VPC CIDR, your IP which will be whitelisted for port 22, 80 and 443 as well as the root SSH keypair you want to use LDAP Parameters: Create a default LDAP user Disable Rollback on Failure if needed If you face any challenge during the installation and need to do some troubleshooting, it's recommended to disable \"Rollback On Failure\" (under Advanced section) Click Next two times and make sure to check \"Capabilities\" section. One done simply click \"Create Stack\". The installation procedure will take about 45 minutes. Post Install Verifications \u00b6 Wait for CloudFormation stacks to be \"CREATE_COMPLETE\", then select your base stack and click \"Outputs\" Output tabs give you information about the SSH IP for the master, link to the web interface or ElasticSearch. Even though Cloudformation resources are created, your environment might not be completely ready. To confirm whether or not Scale-Out Computing on AWS is ready, try to SSH to the scheduler IP. If your Scale-Out Computing on AWS cluster is not ready, your SSH will be rejected as shown below: 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> ************* Scale-Out Computing on AWS FIRST TIME CONFIGURATION ************* Hold on, cluster is not ready yet. Please wait ~30 minutes as Scale-Out Computing on AWS is being installed. Once cluster is ready to use, this message will be replaced automatically and you will be able to SSH. ********************************************************* Connection Closed. If your Scale-Out Computing on AWS cluster is ready, your SSH session will be accepted. 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> Last login: Mon Oct 7 21 :37:21 2019 from <IP> _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-soca-cluster-v1 > source /etc/environment to load Scale-Out Computing on AWS paths [ ec2-user@ip-20-0-28-184 ~ ] $ At this point, you will be able to access the web interface and log in with the default LDAP user you specified at launch creation What's next ? \u00b6 Learn how to access your cluster , how to submit your first job or even how to change your Scale-Out Computing on AWS DNS to match your personal domain name. Project Structure \u00b6 Refer to the project structure below if you want to deploy your own customization . \u251c\u2500\u2500 solution-for-scale-out-computing-on-aws.template [ Soca Install Template ] \u251c\u2500\u2500 soca \u2502 \u251c\u2500\u2500 cluster_analytics [ Scripts to ingest cluster/job data into ELK ] \u2502 \u251c\u2500\u2500 cluster_hooks [ Scheduler Hooks ] \u2502 \u251c\u2500\u2500 cluster_logs_management [ Scripts to manage cluster log rotation ] \u2502 \u251c\u2500\u2500 cluster_manager [ Scripts to control Soca cluster ] \u2502 \u2514\u2500\u2500 cluster_web_ui [ Web Interface ] \u251c\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 ComputeNode.sh [ Configure Compute Node ] \u2502 \u251c\u2500\u2500 ComputeNodeInstallDCV.sh [ Configure DCV Host ] \u2502 \u251c\u2500\u2500 ComputeNodePostReboot.sh [ Post Reboot Compute Node actions ] \u2502 \u251c\u2500\u2500 ComputeNodeUserCustomization.sh [ User customization ] \u2502 \u251c\u2500\u2500 config.cfg [ List of all packages to install ] \u2502 \u251c\u2500\u2500 Scheduler.sh [ Configure Schedule Node ] \u2502 \u2514\u2500\u2500 SchedulerPostReboot.sh [ Post Reboot Scheduler Node actions ] \u2514\u2500\u2500 templates \u251c\u2500\u2500 Analytics.template [ Manage ELK stack for your cluster ] \u251c\u2500\u2500 ComputeNode.template [ Manage simulation nodes ] \u251c\u2500\u2500 Configuration.template [ Centralize cluster configuration ] \u251c\u2500\u2500 Network.template [ Manage VPC configuration ] \u251c\u2500\u2500 Scheduler.template [ Manage Scheduler host ] \u251c\u2500\u2500 Security.template [ Manage ACL, IAM and SGs ] \u251c\u2500\u2500 Storage.template [ Manage backend storage ] \u2514\u2500\u2500 Viewer.template [ Manage DCV sessions ]","title":"Install your Scale-Out Computing on AWS cluster"},{"location":"install-soca-cluster/#download-scale-out-computing-on-aws","text":"Scale-Out Computing on AWS is open-source and available on Github ( https://github.com/awslabs/scale-out-computing-on-aws ). To get started, simply clone the repository: # Clone using HTTPS user@host: git clone https://github.com/awslabs/scale-out-computing-on-aws . # Clone using SSH user@host: git clone git@github.com:awslabs/scale-out-computing-on-aws.git .","title":"Download Scale-Out Computing on AWS"},{"location":"install-soca-cluster/#build-your-release","text":"Once you have cloned your repository, execute source / manual_build . py using either python2 or python3 user@host: python3 source/manual_build.py ====== Scale-Out Computing on AWS Build ====== > Generated unique ID for build: r6l1 > Creating temporary build folder ... > Copying required files ... > Creating archive for build id: r6l1 ====== Build COMPLETE ====== ====== Installation Instructions ====== 1 : Create or use an existing S3 bucket on your AWS account ( eg: 'mysocacluster' ) 2 : Drag & Drop source/dist/r6l1 to your S3 bucket ( eg: ' mysocacluster/dist/r6l1 ) 3 : Launch CloudFormation and use scale-out-computing-on-aws.template as base template 4 : Enter your cluster information. Press Enter key to close .. This command create a build ( r6l1 in this example) under source / dist /< build_id >","title":"Build your release"},{"location":"install-soca-cluster/#upload-to-s3","text":"Go to your Amazon S3 bash and click \"Create Bucket\" Choose a name and a region then click \"Create\" Avoid un-necessary charge It's recommended to create your bucket in the same region as your are planning to use Scale-Out Computing on AWS to avoid Cross-Regions charge ( See Data Transfer ) Once your bucket is created, select it and click \"Upload\". Simply drag and drop your build folder ( r6l1 in this example) to upload the content of the folder to S3. Info You can use the same bucket to host multiple Scale-Out Computing on AWS clusters","title":"Upload to S3"},{"location":"install-soca-cluster/#locate-the-install-template","text":"On your S3 bucket, click on the folder you just uploaded. Your install template is located under < S3_BUCKET_NAME >/< BUILD_ID >/ scale - out - computing - on - aws . template . Click on the object to retrieve the \"Object URL\"","title":"Locate the install template"},{"location":"install-soca-cluster/#install-scale-out-computing-on-aws","text":"Open CloudFormation bash and select \"Create Stack\". Copy the URL of your install template and click \"Next\". Requirements No uppercase in stack name Stack name is limited to 20 characters maximum (note: we automatically add soca- prefix) Not supported on regions with less than 3 AZs (Canada / Northern California) If you hit any issue during the installation, refer to the 'CREATE_FAILED' component and find the root cause by referring at \"Physical ID\" Under stack details, choose the stack name (do not use uppercase or it will break your ElasticSearch cluster). Install Parameters: Specify your S3 bucket you have uploaded your build ( my - soca - hpc - test in this example) as well as the name of your build ( r6l1 in this example). If you haven't uploaded your build on your S3 root level, make sure you specify the entire file hierarchy. Environment Parameters: Choose your Linux Distribution, instance type for your master host, VPC CIDR, your IP which will be whitelisted for port 22, 80 and 443 as well as the root SSH keypair you want to use LDAP Parameters: Create a default LDAP user Disable Rollback on Failure if needed If you face any challenge during the installation and need to do some troubleshooting, it's recommended to disable \"Rollback On Failure\" (under Advanced section) Click Next two times and make sure to check \"Capabilities\" section. One done simply click \"Create Stack\". The installation procedure will take about 45 minutes.","title":"Install Scale-Out Computing on AWS"},{"location":"install-soca-cluster/#post-install-verifications","text":"Wait for CloudFormation stacks to be \"CREATE_COMPLETE\", then select your base stack and click \"Outputs\" Output tabs give you information about the SSH IP for the master, link to the web interface or ElasticSearch. Even though Cloudformation resources are created, your environment might not be completely ready. To confirm whether or not Scale-Out Computing on AWS is ready, try to SSH to the scheduler IP. If your Scale-Out Computing on AWS cluster is not ready, your SSH will be rejected as shown below: 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> ************* Scale-Out Computing on AWS FIRST TIME CONFIGURATION ************* Hold on, cluster is not ready yet. Please wait ~30 minutes as Scale-Out Computing on AWS is being installed. Once cluster is ready to use, this message will be replaced automatically and you will be able to SSH. ********************************************************* Connection Closed. If your Scale-Out Computing on AWS cluster is ready, your SSH session will be accepted. 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> Last login: Mon Oct 7 21 :37:21 2019 from <IP> _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-soca-cluster-v1 > source /etc/environment to load Scale-Out Computing on AWS paths [ ec2-user@ip-20-0-28-184 ~ ] $ At this point, you will be able to access the web interface and log in with the default LDAP user you specified at launch creation","title":"Post Install Verifications"},{"location":"install-soca-cluster/#whats-next","text":"Learn how to access your cluster , how to submit your first job or even how to change your Scale-Out Computing on AWS DNS to match your personal domain name.","title":"What's next ?"},{"location":"install-soca-cluster/#project-structure","text":"Refer to the project structure below if you want to deploy your own customization . \u251c\u2500\u2500 solution-for-scale-out-computing-on-aws.template [ Soca Install Template ] \u251c\u2500\u2500 soca \u2502 \u251c\u2500\u2500 cluster_analytics [ Scripts to ingest cluster/job data into ELK ] \u2502 \u251c\u2500\u2500 cluster_hooks [ Scheduler Hooks ] \u2502 \u251c\u2500\u2500 cluster_logs_management [ Scripts to manage cluster log rotation ] \u2502 \u251c\u2500\u2500 cluster_manager [ Scripts to control Soca cluster ] \u2502 \u2514\u2500\u2500 cluster_web_ui [ Web Interface ] \u251c\u2500\u2500 scripts \u2502 \u251c\u2500\u2500 ComputeNode.sh [ Configure Compute Node ] \u2502 \u251c\u2500\u2500 ComputeNodeInstallDCV.sh [ Configure DCV Host ] \u2502 \u251c\u2500\u2500 ComputeNodePostReboot.sh [ Post Reboot Compute Node actions ] \u2502 \u251c\u2500\u2500 ComputeNodeUserCustomization.sh [ User customization ] \u2502 \u251c\u2500\u2500 config.cfg [ List of all packages to install ] \u2502 \u251c\u2500\u2500 Scheduler.sh [ Configure Schedule Node ] \u2502 \u2514\u2500\u2500 SchedulerPostReboot.sh [ Post Reboot Scheduler Node actions ] \u2514\u2500\u2500 templates \u251c\u2500\u2500 Analytics.template [ Manage ELK stack for your cluster ] \u251c\u2500\u2500 ComputeNode.template [ Manage simulation nodes ] \u251c\u2500\u2500 Configuration.template [ Centralize cluster configuration ] \u251c\u2500\u2500 Network.template [ Manage VPC configuration ] \u251c\u2500\u2500 Scheduler.template [ Manage Scheduler host ] \u251c\u2500\u2500 Security.template [ Manage ACL, IAM and SGs ] \u251c\u2500\u2500 Storage.template [ Manage backend storage ] \u2514\u2500\u2500 Viewer.template [ Manage DCV sessions ]","title":"Project Structure"},{"location":"analytics/build-kibana-dashboards/","text":"On your Kibana cluster , click \"Visualize\" to create a new visualization. Below are some example to help you get started Note: For each dashboard, you can get detailed data at user, queue, job or project level by simply using the \"Filters\" section Money spent by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand Jobs per user \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: user.keyword Order By: metric: Count Jobs per user split by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count Most active projects \u00b6 Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count If needed, you can filter by project name (note: this type of filtering can be applied to all type of dashboard) Instance type launched by user \u00b6 Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count Number of nodes in the cluster \u00b6 Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute Detailed information per user \u00b6 Configuration Select \"Datatables\" and \"jobs\" index Metric (Metrics): Aggregation: Count Split Rows (Buckets): Aggregation: Term Field: user.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: instance_type_used.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: price_ondemand.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: job_name.keyword Order By: metric: Count Find the price for a given simulation \u00b6 Each job comes with price_ondemand and price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate ) Have something cool to share? \u00b6 Want to share something? Click here to contribute","title":"Create your own analytics dashboard"},{"location":"analytics/build-kibana-dashboards/#money-spent-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand","title":"Money spent by instance type"},{"location":"analytics/build-kibana-dashboards/#jobs-per-user","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: user.keyword Order By: metric: Count","title":"Jobs per user"},{"location":"analytics/build-kibana-dashboards/#jobs-per-user-split-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count","title":"Jobs per user split by instance type"},{"location":"analytics/build-kibana-dashboards/#most-active-projects","text":"Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count If needed, you can filter by project name (note: this type of filtering can be applied to all type of dashboard)","title":"Most active projects"},{"location":"analytics/build-kibana-dashboards/#instance-type-launched-by-user","text":"Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count","title":"Instance type launched by user"},{"location":"analytics/build-kibana-dashboards/#number-of-nodes-in-the-cluster","text":"Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute","title":"Number of nodes in the cluster"},{"location":"analytics/build-kibana-dashboards/#detailed-information-per-user","text":"Configuration Select \"Datatables\" and \"jobs\" index Metric (Metrics): Aggregation: Count Split Rows (Buckets): Aggregation: Term Field: user.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: instance_type_used.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: price_ondemand.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: job_name.keyword Order By: metric: Count","title":"Detailed information per user"},{"location":"analytics/build-kibana-dashboards/#find-the-price-for-a-given-simulation","text":"Each job comes with price_ondemand and price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Find the price for a given simulation"},{"location":"analytics/build-kibana-dashboards/#have-something-cool-to-share","text":"Want to share something? Click here to contribute","title":"Have something cool to share?"},{"location":"analytics/monitor-cluster-activity/","text":"Dashboard URL \u00b6 Open your AWS console and navigate to CloudFormation. Select your parent Stack, click Output, and retrieve \"WebUserInterface\" First Time User \u00b6 If you are not already familiar with Kibana, read the instructions below. Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Select the Index you want to query Click next, and then specify the Time Filter key (refer to section below for timestamp id). Once done, click Create Index Pattern< Once your Index is configured, go to Kibana, select \"Discover\" tab to start visualizing the data Index Information \u00b6 Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"stime\" when you create the index for the first time Data uploaded Examples \u00b6 Cluster Node \u00b6 Job Metadata \u00b6 Troubleshooting access permission \u00b6 Access to ElasticSearch is restricted to the IP you have specified during the installation. If your IP change for any reason, you won't be able to access the analytics dashboard and will get the following error message: { \"Message\" : \"User: anonymous is not authorized to perform: es:ESHttpGet\" } To solve this issue, log in to AWS Console and go to ElasticSearch Service dashboard. Select your ElasticSearch cluster and click \"Modify Access Policy\" Finally, simply add your new IP under the \"Condition\" block, then click Submit Please note it may take up to 5 minutes for your IP to be whitelisted Create your own dashboard \u00b6","title":"Monitor your cluster and job activity"},{"location":"analytics/monitor-cluster-activity/#dashboard-url","text":"Open your AWS console and navigate to CloudFormation. Select your parent Stack, click Output, and retrieve \"WebUserInterface\"","title":"Dashboard URL"},{"location":"analytics/monitor-cluster-activity/#first-time-user","text":"If you are not already familiar with Kibana, read the instructions below. Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Select the Index you want to query Click next, and then specify the Time Filter key (refer to section below for timestamp id). Once done, click Create Index Pattern< Once your Index is configured, go to Kibana, select \"Discover\" tab to start visualizing the data","title":"First Time User"},{"location":"analytics/monitor-cluster-activity/#index-information","text":"Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"stime\" when you create the index for the first time Data uploaded","title":"Index Information"},{"location":"analytics/monitor-cluster-activity/#examples","text":"","title":"Examples"},{"location":"analytics/monitor-cluster-activity/#cluster-node","text":"","title":"Cluster Node"},{"location":"analytics/monitor-cluster-activity/#job-metadata","text":"","title":"Job Metadata"},{"location":"analytics/monitor-cluster-activity/#troubleshooting-access-permission","text":"Access to ElasticSearch is restricted to the IP you have specified during the installation. If your IP change for any reason, you won't be able to access the analytics dashboard and will get the following error message: { \"Message\" : \"User: anonymous is not authorized to perform: es:ESHttpGet\" } To solve this issue, log in to AWS Console and go to ElasticSearch Service dashboard. Select your ElasticSearch cluster and click \"Modify Access Policy\" Finally, simply add your new IP under the \"Condition\" block, then click Submit Please note it may take up to 5 minutes for your IP to be whitelisted","title":"Troubleshooting access permission"},{"location":"analytics/monitor-cluster-activity/#create-your-own-dashboard","text":"","title":"Create your own dashboard"},{"location":"analytics/review-hpc-costs/","text":"AWS Cost Explorer \u00b6 Any EC2 resource launched by Scale-Out Computing on AWS comes with an extensive list of EC2 tags that can be used to get detailed information about your cluster usage. List includes (but not limited to): Project Name Job Owner Job Name Job Queue Job Id These are the default tags and you can add your own tags if needed. Step1: Enable Cost Allocation Tags \u00b6 Be patient It could take up to 24 hours for the tags to be active Click on your account name (1) then select \"My Billing Dashboard\" (2) Then click Cost Allocation tag Finally, search all \"Scale-Out Computing on AWS\" tags then click \"Activate\" Step 2: Enable Cost Explorer \u00b6 In your billing dashboard, select \"Cost Explorer\" (1) and click \"Enable Cost Explorer\" (2). Step 3: Query Cost Explorer \u00b6 Open your Cost Explorer tab and specify your filters. In this example I want to get the EC2 cost (1), group by day for my queue named \"cpus\" (2). To get more detailed information, select 'Group By' and apply additional filters. Here is an example if I want user level information for \"cpus\" queue Click \"Tag\" section under \"Group By\" horizontal label (1) and select \"soca:JobOwner\" tag. Your graph will automatically be updated with a cost breakdown by users for \"cpus\" queue","title":"Review your HPC costs"},{"location":"analytics/review-hpc-costs/#aws-cost-explorer","text":"Any EC2 resource launched by Scale-Out Computing on AWS comes with an extensive list of EC2 tags that can be used to get detailed information about your cluster usage. List includes (but not limited to): Project Name Job Owner Job Name Job Queue Job Id These are the default tags and you can add your own tags if needed.","title":"AWS Cost Explorer"},{"location":"analytics/review-hpc-costs/#step1-enable-cost-allocation-tags","text":"Be patient It could take up to 24 hours for the tags to be active Click on your account name (1) then select \"My Billing Dashboard\" (2) Then click Cost Allocation tag Finally, search all \"Scale-Out Computing on AWS\" tags then click \"Activate\"","title":"Step1: Enable Cost Allocation Tags"},{"location":"analytics/review-hpc-costs/#step-2-enable-cost-explorer","text":"In your billing dashboard, select \"Cost Explorer\" (1) and click \"Enable Cost Explorer\" (2).","title":"Step 2: Enable Cost Explorer"},{"location":"analytics/review-hpc-costs/#step-3-query-cost-explorer","text":"Open your Cost Explorer tab and specify your filters. In this example I want to get the EC2 cost (1), group by day for my queue named \"cpus\" (2). To get more detailed information, select 'Group By' and apply additional filters. Here is an example if I want user level information for \"cpus\" queue Click \"Tag\" section under \"Group By\" horizontal label (1) and select \"soca:JobOwner\" tag. Your graph will automatically be updated with a cost breakdown by users for \"cpus\" queue","title":"Step 3: Query Cost Explorer"},{"location":"analytics/set-up-budget-project/","text":"On this page, I will demonstrate how to configure a budget for a given project and reject job if you exceed the allocated budget For this example, I will create a budget named \"Project 1\" and prevent user to submit job if (1) they do not belong to the project and (2) if the budget has expired. First, read this link to understand how to monitor your cluster cost and budgets on AWS. Configure the scheduler hook \u00b6 To enable this feature, you will first need to verify the project assigned to each job during submission time. The script managing this can be found on your Scale-Out Computing on AWS cluster at / apps / soca / cluster_hooks / queuejob / check_project_budget . py First, edit this file and manually enter your AWS account id: # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/cluster_manager/settings/project_cost_manager.txt' user_must_belong_to_project = True # Change if you don't want to restrict project to a list of users allow_job_no_project = False # Change if you do not want to enforce project at job submission allow_user_multiple_projects = True # Change if you want to restrict a user to one project Then enable the hook by running the following commands as root (on the scheduler host): user@host: qmgr -c \"create hook check_project_budget event=queuejob\" user@host: qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/cluster_hooks/queuejob/check_project_budget.py\" Test it \u00b6 Submit a job when budget is valid \u00b6 Go to AWS Billing , click Budget on the left sidebar and create a new budget Select \"Cost Budget\". Name your budget \"Project 1\" and configure the Period/Budget based on your requirements. For my example I will allocate a $100 per month recurring budget for my project called \"Project 1\" (use Tag: soca:JobProject ) Set up a email notification when your budget exceed 80% then click \"Confirm Budget\" As you can see, I have still money available for this project (budgeted $100 but only used $15). Let's try to submit a job user@host$ qsub -- /bin/echo Hello qsub: Error. You tried to submit job without project. Specify project using -P parameter This does not work because the job was submitted without project defined. If you still want to let your users do that, edit allow_job_no_project = False on the hook file. Let's try the same request but specify - P \"Project 1\" during submission: user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: User mickael is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt This time, the hook complains thant my user \"mickael\" is not mapped the the project. This is because (1) the budget does not exist on the HPC cluster or (2) my user has not been approved to use this project. Edit / apps / soca / cluster_manager / settings / project_cost_manager . txt and configure your budget for this user: # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https: //soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [ Project 1 ] mickael Important The config section (\"Project 1\") MUST match the name of the budget your created on AWS Budget (it's case sensitive) Save this file and try to submit a job, this time the job should go to the queue user@host$ qsub -P \"Project 1\" -- /bin/echo Hello 5 .ip-10-0-1-223 Submit a job when budget is invalid \u00b6 Now let's go back to your AWS budget, and let's simulate we are over-budget Now try to submit a job for \"Project 1\", your request should be rejected user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: Error. Budget for Project 1 exceed allocated threshold. Update it on AWS Budget bash The hook query the AWS Budget in real-time. So if your users are blocked because of budget restriction, you can at any time edit the value on AWS Budget and unblock them (assuming you still have some money left in your pocket :P ) As mentioned above, the project name on / apps / soca / cluster_manager / settings / project_cost_manager . txt and the name of your AWS Budget must match (case sensitive). If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist.","title":"Set up budget per project"},{"location":"analytics/set-up-budget-project/#configure-the-scheduler-hook","text":"To enable this feature, you will first need to verify the project assigned to each job during submission time. The script managing this can be found on your Scale-Out Computing on AWS cluster at / apps / soca / cluster_hooks / queuejob / check_project_budget . py First, edit this file and manually enter your AWS account id: # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/cluster_manager/settings/project_cost_manager.txt' user_must_belong_to_project = True # Change if you don't want to restrict project to a list of users allow_job_no_project = False # Change if you do not want to enforce project at job submission allow_user_multiple_projects = True # Change if you want to restrict a user to one project Then enable the hook by running the following commands as root (on the scheduler host): user@host: qmgr -c \"create hook check_project_budget event=queuejob\" user@host: qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/cluster_hooks/queuejob/check_project_budget.py\"","title":"Configure the scheduler hook"},{"location":"analytics/set-up-budget-project/#test-it","text":"","title":"Test it"},{"location":"analytics/set-up-budget-project/#submit-a-job-when-budget-is-valid","text":"Go to AWS Billing , click Budget on the left sidebar and create a new budget Select \"Cost Budget\". Name your budget \"Project 1\" and configure the Period/Budget based on your requirements. For my example I will allocate a $100 per month recurring budget for my project called \"Project 1\" (use Tag: soca:JobProject ) Set up a email notification when your budget exceed 80% then click \"Confirm Budget\" As you can see, I have still money available for this project (budgeted $100 but only used $15). Let's try to submit a job user@host$ qsub -- /bin/echo Hello qsub: Error. You tried to submit job without project. Specify project using -P parameter This does not work because the job was submitted without project defined. If you still want to let your users do that, edit allow_job_no_project = False on the hook file. Let's try the same request but specify - P \"Project 1\" during submission: user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: User mickael is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt This time, the hook complains thant my user \"mickael\" is not mapped the the project. This is because (1) the budget does not exist on the HPC cluster or (2) my user has not been approved to use this project. Edit / apps / soca / cluster_manager / settings / project_cost_manager . txt and configure your budget for this user: # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https: //soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [ Project 1 ] mickael Important The config section (\"Project 1\") MUST match the name of the budget your created on AWS Budget (it's case sensitive) Save this file and try to submit a job, this time the job should go to the queue user@host$ qsub -P \"Project 1\" -- /bin/echo Hello 5 .ip-10-0-1-223","title":"Submit a job when budget is valid"},{"location":"analytics/set-up-budget-project/#submit-a-job-when-budget-is-invalid","text":"Now let's go back to your AWS budget, and let's simulate we are over-budget Now try to submit a job for \"Project 1\", your request should be rejected user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: Error. Budget for Project 1 exceed allocated threshold. Update it on AWS Budget bash The hook query the AWS Budget in real-time. So if your users are blocked because of budget restriction, you can at any time edit the value on AWS Budget and unblock them (assuming you still have some money left in your pocket :P ) As mentioned above, the project name on / apps / soca / cluster_manager / settings / project_cost_manager . txt and the name of your AWS Budget must match (case sensitive). If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist.","title":"Submit a job when budget is invalid"},{"location":"storage/backend-storage-options/","text":"Scale-Out Computing on AWS gives you the flexibility to customize your storage backend based on your requirements You can customize the root partition size You can provision a local scratch partition You can deploy standard SSD (gp2) or IO Optimized SSD (io1) volumes Scale-Out Computing on AWS automatically leverages instance store disk(s) as scratch partition when applicable In term of performance: Instance Store > EBS SSD IO > EBS SSD Standard > EFS Refer to this link to learn more about EBS volumes. EFS (shared) partitions \u00b6 /data partition \u00b6 /data is an Elastic File System partition mounted on all hosts. This contains the home directory of your LDAP users ($HOME = / data / home /< USERNAME > ). This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead) /apps partition \u00b6 /apps is an Elastic File System partition mounted on all hosts. This partition is designed to host all your CFD/FEA/EDA/Mathematical applications. This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead) FSx \u00b6 Scale-Out Computing on AWS supports FSx natively. Click here to learn how to use FSx as backend storage for your jobs . Instance (local) partitions \u00b6 Below are the storage options you can configure at an instance level for your jobs. If needed, add/remove/modify the storage logic by editing ComputeNode . sh script to match your requirements. Root partition \u00b6 By default Scale-Out Computing on AWS provision a 10GB EBS disk for the root partition. This may be an issue if you are using a custom AMI configured with a bigger root disk size or if you simply want to allocate additional storage for the root partition. To expand the size of the volume, submit a simulation using - l root_size =< SIZE_IN_GB > parameter. user@host:qsub -l root_size = 25 -- /bin/sleep 600 Result: Root partition is now 25GB user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259 :0 0 25G 0 disk \u251c\u2500nvme0n1p1 259 :1 0 25G 0 part / \u2514\u2500nvme0n1p128 259 :2 0 1M 0 part user@host: df -h / Filesystem Size Used Avail Use% Mounted on /dev/nvme0n1p1 25G 2 .2G 23G 9 % / Scratch Partition \u00b6 Info It's recommended to provision / scratch directory whenever your simulation is I/O intensive. / scratch is a local partition and will be deleted when you job complete. Make sure to copy the job output back to your $HOME / scratch is automatically created when Instance supports local ephemeral storage Request a /scratch partition with SSD disk \u00b6 During job submission, specify - l scratch_size =< SIZE_IN_GB > to provision a new EBS disk ( / dev / sdj ) mounted as / scratch user@host: qsub -l scratch_size = 150 -- /bin/sleep 600 Result: a 150 GB /scratch partition is available on all nodes user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 150G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 148G 61M 140G 1 % /scratch To verify the type of your EBS disk, simply go to your AWS bash > EC2 > Volumes and verify your EBS type is \"gp2\" (SSD). Refer to this link for more information about the various EBS types available. Request a /scratch partition with IO optimized disk \u00b6 To request an optimized SSD disk, use - l scratch_iops =< IOPS > along with - l scratch_size =< SIZE_IN_GB > . Refer to this link to get more details about burstable/IO EBS disks. user@host: qsub -l scratch_iops = 6000 -l scratch_size = 200 -- /bin/sleep 600 Looking at the EBS bash, the disk type is now \"io1\" and the number of IOPS match the value specified at job submission. Free storage is always good You are not charged for instance storage (included in the price of the instance) Some instances come with default instance storage . An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer and is removed as soon as the node is deleted. Scale-Out Computing on AWS automatically detects instance store disk and will use them as /scratch unless you specify - l scratch_size parameter for your job . In this case, Scale-Out Computing on AWS honors the user request and ignore the instance store volume(s). When node has 1 instance store volume \u00b6 For this example, I will use a \"c5d.9xlarge\" instance which is coming with a 900GB instance store disk. user@host: qsub -l instance_type = c5d.9xlarge -- /bin/sleep 600 Result: Default /scratch partition has been provisioned automatically using local instance storage user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 825G 77M 783G 1 % /scratch When node has more than 1 instance store volumes \u00b6 In this special case, ComputeNode . sh script will create a raid0 partition using all instance store volumes available. For this example, I will use a \"m5dn.12xlarge\" instance which is shipped with a 2 * 900GB instance store disks (total 1.8Tb). user@host: qsub -l instance_type = m5dn.12xlarge -- /bin/sleep 600 Result: /scratch is a 1.7TB raid0 partition (using 2 instance store volumes) user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme2n1 259 :1 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme0n1 259 :2 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :3 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :4 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/md127 1 .7T 77M 1 .6T 1 % /scratch Combine custom scratch and root size \u00b6 You can combine parameters as needed. For example, qsub - l root_size = 150 - l scratch_size = 200 - l nodes = 2 will provision 2 nodes with 150GB / and 200GB SSD /scratch Change the storage parameters at queue level \u00b6 Edit / apps / soca / cluster_manager / settings / queue_mapping . yml to configure default storage settings at a queue level: queue_type : compute : # /root will be 30 GB and /scratch will be a standard 100GB SSD queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"c5.large\" scratch_size : \"100\" root_size : \"30\" # .. Refer to the doc for more supported parameters memory : # /scratch will be a SSD with provisioned IO queues : [ \"queue4\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"r5.large\" scratch_size : \"300\" scratch_iops : \"5000\" instancestore : # /scratch will use the default instance store queues : [ \"queue5\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"m5dn.12large\" root_size : \"300\"","title":"Understand backend storage"},{"location":"storage/backend-storage-options/#efs-shared-partitions","text":"","title":"EFS (shared) partitions"},{"location":"storage/backend-storage-options/#data-partition","text":"/data is an Elastic File System partition mounted on all hosts. This contains the home directory of your LDAP users ($HOME = / data / home /< USERNAME > ). This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead)","title":"/data partition"},{"location":"storage/backend-storage-options/#apps-partition","text":"/apps is an Elastic File System partition mounted on all hosts. This partition is designed to host all your CFD/FEA/EDA/Mathematical applications. This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead)","title":"/apps partition"},{"location":"storage/backend-storage-options/#fsx","text":"Scale-Out Computing on AWS supports FSx natively. Click here to learn how to use FSx as backend storage for your jobs .","title":"FSx"},{"location":"storage/backend-storage-options/#instance-local-partitions","text":"Below are the storage options you can configure at an instance level for your jobs. If needed, add/remove/modify the storage logic by editing ComputeNode . sh script to match your requirements.","title":"Instance (local) partitions"},{"location":"storage/backend-storage-options/#root-partition","text":"By default Scale-Out Computing on AWS provision a 10GB EBS disk for the root partition. This may be an issue if you are using a custom AMI configured with a bigger root disk size or if you simply want to allocate additional storage for the root partition. To expand the size of the volume, submit a simulation using - l root_size =< SIZE_IN_GB > parameter. user@host:qsub -l root_size = 25 -- /bin/sleep 600 Result: Root partition is now 25GB user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259 :0 0 25G 0 disk \u251c\u2500nvme0n1p1 259 :1 0 25G 0 part / \u2514\u2500nvme0n1p128 259 :2 0 1M 0 part user@host: df -h / Filesystem Size Used Avail Use% Mounted on /dev/nvme0n1p1 25G 2 .2G 23G 9 % /","title":"Root partition"},{"location":"storage/backend-storage-options/#scratch-partition","text":"Info It's recommended to provision / scratch directory whenever your simulation is I/O intensive. / scratch is a local partition and will be deleted when you job complete. Make sure to copy the job output back to your $HOME / scratch is automatically created when Instance supports local ephemeral storage","title":"Scratch Partition"},{"location":"storage/backend-storage-options/#request-a-scratch-partition-with-ssd-disk","text":"During job submission, specify - l scratch_size =< SIZE_IN_GB > to provision a new EBS disk ( / dev / sdj ) mounted as / scratch user@host: qsub -l scratch_size = 150 -- /bin/sleep 600 Result: a 150 GB /scratch partition is available on all nodes user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 150G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 148G 61M 140G 1 % /scratch To verify the type of your EBS disk, simply go to your AWS bash > EC2 > Volumes and verify your EBS type is \"gp2\" (SSD). Refer to this link for more information about the various EBS types available.","title":"Request a /scratch partition with SSD disk"},{"location":"storage/backend-storage-options/#request-a-scratch-partition-with-io-optimized-disk","text":"To request an optimized SSD disk, use - l scratch_iops =< IOPS > along with - l scratch_size =< SIZE_IN_GB > . Refer to this link to get more details about burstable/IO EBS disks. user@host: qsub -l scratch_iops = 6000 -l scratch_size = 200 -- /bin/sleep 600 Looking at the EBS bash, the disk type is now \"io1\" and the number of IOPS match the value specified at job submission. Free storage is always good You are not charged for instance storage (included in the price of the instance) Some instances come with default instance storage . An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer and is removed as soon as the node is deleted. Scale-Out Computing on AWS automatically detects instance store disk and will use them as /scratch unless you specify - l scratch_size parameter for your job . In this case, Scale-Out Computing on AWS honors the user request and ignore the instance store volume(s).","title":"Request a /scratch partition with IO optimized disk"},{"location":"storage/backend-storage-options/#when-node-has-1-instance-store-volume","text":"For this example, I will use a \"c5d.9xlarge\" instance which is coming with a 900GB instance store disk. user@host: qsub -l instance_type = c5d.9xlarge -- /bin/sleep 600 Result: Default /scratch partition has been provisioned automatically using local instance storage user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 825G 77M 783G 1 % /scratch","title":"When node has 1 instance store volume"},{"location":"storage/backend-storage-options/#when-node-has-more-than-1-instance-store-volumes","text":"In this special case, ComputeNode . sh script will create a raid0 partition using all instance store volumes available. For this example, I will use a \"m5dn.12xlarge\" instance which is shipped with a 2 * 900GB instance store disks (total 1.8Tb). user@host: qsub -l instance_type = m5dn.12xlarge -- /bin/sleep 600 Result: /scratch is a 1.7TB raid0 partition (using 2 instance store volumes) user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme2n1 259 :1 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme0n1 259 :2 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :3 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :4 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/md127 1 .7T 77M 1 .6T 1 % /scratch","title":"When node has more than 1 instance store volumes"},{"location":"storage/backend-storage-options/#combine-custom-scratch-and-root-size","text":"You can combine parameters as needed. For example, qsub - l root_size = 150 - l scratch_size = 200 - l nodes = 2 will provision 2 nodes with 150GB / and 200GB SSD /scratch","title":"Combine custom scratch and root size"},{"location":"storage/backend-storage-options/#change-the-storage-parameters-at-queue-level","text":"Edit / apps / soca / cluster_manager / settings / queue_mapping . yml to configure default storage settings at a queue level: queue_type : compute : # /root will be 30 GB and /scratch will be a standard 100GB SSD queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"c5.large\" scratch_size : \"100\" root_size : \"30\" # .. Refer to the doc for more supported parameters memory : # /scratch will be a SSD with provisioned IO queues : [ \"queue4\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"r5.large\" scratch_size : \"300\" scratch_iops : \"5000\" instancestore : # /scratch will use the default instance store queues : [ \"queue5\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"m5dn.12large\" root_size : \"300\"","title":"Change the storage parameters at queue level"},{"location":"storage/launch-job-with-fsx/","text":"What is FSx \u00b6 Amazon FSx provides you with the native compatibility of third-party file systems with feature sets for workloads such as high-performance computing (HPC), machine learning and electronic design automation (EDA). You don\u2019t have to worry about managing file servers and storage, as Amazon FSx automates the time-consuming administration tasks such as hardware provisioning, software configuration, patching, and backups. Amazon FSx provides FSx for Lustre for compute-intensive workloads. Please note the following when using FSx on Scale-Out Computing on AWS FSx is supported natively (Linux clients, security groups and backend configuration is automatically managed by Scale-Out Computing on AWS) You can launch an ephemeral FSx filesystem for your job You can connect to an existing FSx filesystem You can dynamically adjust the storage capacity of your FSx filesystem Exported files (if any) from FSx to S3 will be stored under s3 : //< YOUR_BUCKET_NAME >/< CLUSTER_ID >- fsxoutput / job -< JOB_ID >/ Pre-requisite for FSx for Lustre \u00b6 You need to give Scale-Out Computing on AWS the permission to map the S3 bucket you want to mount on FSx. To do that, add a new inline policy to the scheduler IAM role . The Scheduler IAM role can be found on the IAM bash and is named < Scale - Out Computing on AWS_STACK_NAME >- Security -< UUID >- SchedulerIAMRole -< UUID > . To create an inline policy, select your IAM role, click \"Add Inline Policy\": Select \"JSON\" tab Finally copy/paste the JSON policy listed below (make sure to adjust to your bucket name), click \"Review\" and \"Create Policy\". { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowAccessFSxtoS3\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*\" , \"Resource\" : [ \"arn:aws:s3:::<YOUR_BUCKET_NAME>\" , \"arn:aws:s3:::<YOUR_BUCKET_NAME>/*\" ] } ] } To validate your policy is effective, access the scheduler host and run the following commmand: ## Example when IAM policy is not correct user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> An error occurred ( AccessDenied ) when calling the ListObjectsV2 operation: Access Denied ## Example when IAM policy is valid (output will list content of your bucket) user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> 2019 -11-02 04 :26:27 2209 dataset1.txt 2019 -11-02 04 :26:39 10285 dataset2.csv Warning This permission will give scheduler host access to your S3 bucket, therefore you want to limit access to this host to approved users only. DCV sessions or other compute nodes will not have access to the S3 bucket. Provision FSx for your job \u00b6 For this example, let's say I have my dataset available on S3 and I want to access them for my simulation. Submit a job using - l fsx_lustre_bucket = s3 : //< YOUR_BUCKET_NAME > . The bucket will then be mounted on all nodes provisioned for the job under / fsx mountpoint. user@host: qsub -l fsx_lustre_bucket = s3://<YOUR_BUCKET_NAME> -- /bin/sleep 600 This command will provision a new 1200 GB (smallest capacity available) FSx filesystem for your job: Your job will automatically start as soon as both your FSx filesystem and compute nodes are available. Your filesystem will be available on all nodes allocated to your job under / fsx user@host: df -h /fsx Filesystem Size Used Avail Use% Mounted on 200 .0.170.60@tcp:/fsx 1 .1T 4 .4M 1 .1T 1 % /fsx ## Verify the content of your bucket is accessible user@host: ls -ltr /fsx total 1 -rwxr-xr-x 1 root root 2209 Nov 2 04 :26 dataset1.txt -rwxr-xr-x 1 root root 10285 Nov 2 04 :26 dataset2.csv Your FSx filesystem will automatically be terminated when your job complete. Refer to this link to learn how to interact with FSx data repositories. Change FSx capacity \u00b6 Use - l fsx_lustre_size =< SIZE_IN_GB > to specify the size of your FSx filesystem. Please note the following informations: - If not specified, Scale-Out Computing on AWS deploy the smallest possible capacity (1200GB) - Valid sizes (in GB) are 1200, 2400, 3600 and increments of 3600 user@host: qsub -l fsx_lustre_size = 3600 -l fsx_lustre_bucket = s3://<YOUR_S3_BUCKET> -- /bin/sleep 600 This command will mount a 3.6TB FSx filesystem on all nodes provisioned for your simulation. How to connect to a permanent/existing FSx \u00b6 If you already have a running FSx, you can mount it using - l fsx_lustre_dns variable. user@host: qsub -l fsx_lustre_dns = <MY_FSX_DNS> -- /bin/sleep 60 To retrieve your FSx DNS, select your filesystem and select \"Network & Security\" Warning Make sure your FSx is running on the same VPC as Scale-Out Computing on AWS Make sure your FSx security group allow traffic from/to Scale-Out Computing on AWS ComputeNodes SG If you specify both \"fsx_lustre_bucket\" and \"fsx_lustre_dns\", only \"fsx_lustre_dns\" will be mounted. How to change the mountpoint \u00b6 By default Scale-Out Computing on AWS mounts fsx on / fsx . If you need to change this value, edit scripts / ComputeNode . sh update the value of FSX_MOUNTPOINT . ... if [[ $Scale -Out Computing on AWS_FSX_LUSTRE_BUCKET ! = 'false' ]] ; then echo \"FSx request detected, installing FSX Lustre client ... \" FSX_MOUNTPOINT = \"/fsx\" ## <-- Update mountpoint here mkdir -p $FSX_MOUNTPOINT ... Learn about the other storage options on Scale-Out Computing on AWS \u00b6 Click here to learn about the other storage options offered by Scale-Out Computing on AWS. Troubleshooting and most common errors \u00b6 Like any other parameter, FSx options can be debugged using / apps / soca / cluster_manager / logs /< QUEUE_NAME > . log [ Error while trying to create ASG: Scale-Out Computing on AWS does not have access to this bucket. Update IAM policy as described on https://soca.dev/tutorials/job-fsx-lustre-backend/ ] Resolution : Scale-Out Computing on AWS does not have access to this S3 bucket. Update your IAM role with the policy listed above [ Error while trying to create ASG: fsx_lustre_size must be: 1200 , 2400 , 3600 , 7200 , 10800 ] Resolution : fsx_lustre_size must be 1200, 2400, 3600 and increments of 3600 [ Error while trying to create ASG: fsx_lustre_bucket must start with s3:// ] Resolution : fsx_lustre_bucket must start with s3://. Use s3://mybucket/mypath if you want to mount mybucket/mypath.","title":"Launch a job with FSx for Lustre"},{"location":"storage/launch-job-with-fsx/#what-is-fsx","text":"Amazon FSx provides you with the native compatibility of third-party file systems with feature sets for workloads such as high-performance computing (HPC), machine learning and electronic design automation (EDA). You don\u2019t have to worry about managing file servers and storage, as Amazon FSx automates the time-consuming administration tasks such as hardware provisioning, software configuration, patching, and backups. Amazon FSx provides FSx for Lustre for compute-intensive workloads. Please note the following when using FSx on Scale-Out Computing on AWS FSx is supported natively (Linux clients, security groups and backend configuration is automatically managed by Scale-Out Computing on AWS) You can launch an ephemeral FSx filesystem for your job You can connect to an existing FSx filesystem You can dynamically adjust the storage capacity of your FSx filesystem Exported files (if any) from FSx to S3 will be stored under s3 : //< YOUR_BUCKET_NAME >/< CLUSTER_ID >- fsxoutput / job -< JOB_ID >/","title":"What is FSx"},{"location":"storage/launch-job-with-fsx/#pre-requisite-for-fsx-for-lustre","text":"You need to give Scale-Out Computing on AWS the permission to map the S3 bucket you want to mount on FSx. To do that, add a new inline policy to the scheduler IAM role . The Scheduler IAM role can be found on the IAM bash and is named < Scale - Out Computing on AWS_STACK_NAME >- Security -< UUID >- SchedulerIAMRole -< UUID > . To create an inline policy, select your IAM role, click \"Add Inline Policy\": Select \"JSON\" tab Finally copy/paste the JSON policy listed below (make sure to adjust to your bucket name), click \"Review\" and \"Create Policy\". { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowAccessFSxtoS3\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*\" , \"Resource\" : [ \"arn:aws:s3:::<YOUR_BUCKET_NAME>\" , \"arn:aws:s3:::<YOUR_BUCKET_NAME>/*\" ] } ] } To validate your policy is effective, access the scheduler host and run the following commmand: ## Example when IAM policy is not correct user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> An error occurred ( AccessDenied ) when calling the ListObjectsV2 operation: Access Denied ## Example when IAM policy is valid (output will list content of your bucket) user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> 2019 -11-02 04 :26:27 2209 dataset1.txt 2019 -11-02 04 :26:39 10285 dataset2.csv Warning This permission will give scheduler host access to your S3 bucket, therefore you want to limit access to this host to approved users only. DCV sessions or other compute nodes will not have access to the S3 bucket.","title":"Pre-requisite for FSx for Lustre"},{"location":"storage/launch-job-with-fsx/#provision-fsx-for-your-job","text":"For this example, let's say I have my dataset available on S3 and I want to access them for my simulation. Submit a job using - l fsx_lustre_bucket = s3 : //< YOUR_BUCKET_NAME > . The bucket will then be mounted on all nodes provisioned for the job under / fsx mountpoint. user@host: qsub -l fsx_lustre_bucket = s3://<YOUR_BUCKET_NAME> -- /bin/sleep 600 This command will provision a new 1200 GB (smallest capacity available) FSx filesystem for your job: Your job will automatically start as soon as both your FSx filesystem and compute nodes are available. Your filesystem will be available on all nodes allocated to your job under / fsx user@host: df -h /fsx Filesystem Size Used Avail Use% Mounted on 200 .0.170.60@tcp:/fsx 1 .1T 4 .4M 1 .1T 1 % /fsx ## Verify the content of your bucket is accessible user@host: ls -ltr /fsx total 1 -rwxr-xr-x 1 root root 2209 Nov 2 04 :26 dataset1.txt -rwxr-xr-x 1 root root 10285 Nov 2 04 :26 dataset2.csv Your FSx filesystem will automatically be terminated when your job complete. Refer to this link to learn how to interact with FSx data repositories.","title":"Provision FSx for your job"},{"location":"storage/launch-job-with-fsx/#change-fsx-capacity","text":"Use - l fsx_lustre_size =< SIZE_IN_GB > to specify the size of your FSx filesystem. Please note the following informations: - If not specified, Scale-Out Computing on AWS deploy the smallest possible capacity (1200GB) - Valid sizes (in GB) are 1200, 2400, 3600 and increments of 3600 user@host: qsub -l fsx_lustre_size = 3600 -l fsx_lustre_bucket = s3://<YOUR_S3_BUCKET> -- /bin/sleep 600 This command will mount a 3.6TB FSx filesystem on all nodes provisioned for your simulation.","title":"Change FSx capacity"},{"location":"storage/launch-job-with-fsx/#how-to-connect-to-a-permanentexisting-fsx","text":"If you already have a running FSx, you can mount it using - l fsx_lustre_dns variable. user@host: qsub -l fsx_lustre_dns = <MY_FSX_DNS> -- /bin/sleep 60 To retrieve your FSx DNS, select your filesystem and select \"Network & Security\" Warning Make sure your FSx is running on the same VPC as Scale-Out Computing on AWS Make sure your FSx security group allow traffic from/to Scale-Out Computing on AWS ComputeNodes SG If you specify both \"fsx_lustre_bucket\" and \"fsx_lustre_dns\", only \"fsx_lustre_dns\" will be mounted.","title":"How to connect to a permanent/existing FSx"},{"location":"storage/launch-job-with-fsx/#how-to-change-the-mountpoint","text":"By default Scale-Out Computing on AWS mounts fsx on / fsx . If you need to change this value, edit scripts / ComputeNode . sh update the value of FSX_MOUNTPOINT . ... if [[ $Scale -Out Computing on AWS_FSX_LUSTRE_BUCKET ! = 'false' ]] ; then echo \"FSx request detected, installing FSX Lustre client ... \" FSX_MOUNTPOINT = \"/fsx\" ## <-- Update mountpoint here mkdir -p $FSX_MOUNTPOINT ...","title":"How to change the mountpoint"},{"location":"storage/launch-job-with-fsx/#learn-about-the-other-storage-options-on-scale-out-computing-on-aws","text":"Click here to learn about the other storage options offered by Scale-Out Computing on AWS.","title":"Learn about the other storage options on Scale-Out Computing on AWS"},{"location":"storage/launch-job-with-fsx/#troubleshooting-and-most-common-errors","text":"Like any other parameter, FSx options can be debugged using / apps / soca / cluster_manager / logs /< QUEUE_NAME > . log [ Error while trying to create ASG: Scale-Out Computing on AWS does not have access to this bucket. Update IAM policy as described on https://soca.dev/tutorials/job-fsx-lustre-backend/ ] Resolution : Scale-Out Computing on AWS does not have access to this S3 bucket. Update your IAM role with the policy listed above [ Error while trying to create ASG: fsx_lustre_size must be: 1200 , 2400 , 3600 , 7200 , 10800 ] Resolution : fsx_lustre_size must be 1200, 2400, 3600 and increments of 3600 [ Error while trying to create ASG: fsx_lustre_bucket must start with s3:// ] Resolution : fsx_lustre_bucket must start with s3://. Use s3://mybucket/mypath if you want to mount mybucket/mypath.","title":"Troubleshooting and most common errors"},{"location":"tutorials/create-your-own-queue/","text":"Things to know before you start By default, Scale-Out Computing on AWS creates 4 queues: high, normal (default), low and alwayson. Queue with automatic instance provisioning \u00b6 Create the queue \u00b6 On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Set default compute node - See below for more information Qmgr:set queue <queue_name> default_chunk.compute_node = tbd # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit What is compute_node=tbd On Scale-Out Computing on AWS, unless you configure queue with AlwaysOn instances, nodes will be provisioned based on queue status. When you submit a job, Scale-Out Computing on AWS will automatically provision capacity for this job and compute_node is the scheduler making sure only one job can run on this instance. compute_node=tbd is the default value, making sure any new jobs won't run on existing (if any) nodes Configure automatic host provisioning \u00b6 If you want to enable automatic host provisioning, edit this file: / apps / soca / cluster_manager / settings / queue_mapping . yml Option1: I want to use the same settings as an existing queue \u00b6 In this case, simply update the array with your new queue queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] # <- Add your queue to the array instance_ami : \"ami-1234567\" instance_type : \"c5.large\" ... Option2: I want to configure specific settings \u00b6 In this case, you will first need to create a new section on the YAML file (see example with memory) queue_type : compute : queues : [ \"queue1\" ] instance_ami : \"ami-1234567\" instance_type : \"c5.large\" scratch_size : \"100\" memory : # <- Add new section queues : [ \"queue2\" ] instance_ami : \"ami-9876543\" instance_type : \"r5.24xlarge\" scratch_size : \"600\" Finally, add a new crontab on the scheduler machine (as root). Use -c to path to the YAML file and -t to the YAML section you just created */3 * * * * source /etc/environment ; /apps/python/latest/bin/python3 /apps/soca/cluster_manager/dispatcher.py -c /apps/soca/cluster_manager/settings/queue_mapping.yml -t memory Automatic Host provisioning logs \u00b6 All logs queues are stored under / apps / soca / cluster_manager / logs /< queue_name > Queue with AlwaysOn instances \u00b6 Important Scale-Out Computing on AWS automatically created one AlwaysOn queue for you called \"alwayson\" during the first installation In this mode, instances will never be stopped programmatically. You are responsible to terminate the capacity manually by deleting the associated CloudFormation stack Create the queue \u00b6 On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit Start provisioning some capacity \u00b6 Run python3 apps / soca / cluster_manager / add_nodes . py and enable --keep_forever flag # Launch 1 c5.large always on python3 /apps/soca/cluster_manager/add_nodes.py --instance_type c5.large \\ --desired_capacity 1 \\ --queue cpus \\ --job_name instancealwayson \\ --job_owner mcrozes \\ --keep_forever IMPORTANT: You specified --keep-forever flag. This instance will be running 24 /7 until you MANUALLY terminate the Cloudformation Stack If you need help with this script, run python3 add_nodes . py - h Delete AlwaysOn capacity \u00b6 Simply go to your CloudFormation console, locate the stack following the naming convention: soca- cluster-name -keepforever- queue_name -uniqueid and terminate it.","title":"Create your own queue"},{"location":"tutorials/create-your-own-queue/#queue-with-automatic-instance-provisioning","text":"","title":"Queue with automatic instance provisioning"},{"location":"tutorials/create-your-own-queue/#create-the-queue","text":"On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Set default compute node - See below for more information Qmgr:set queue <queue_name> default_chunk.compute_node = tbd # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit What is compute_node=tbd On Scale-Out Computing on AWS, unless you configure queue with AlwaysOn instances, nodes will be provisioned based on queue status. When you submit a job, Scale-Out Computing on AWS will automatically provision capacity for this job and compute_node is the scheduler making sure only one job can run on this instance. compute_node=tbd is the default value, making sure any new jobs won't run on existing (if any) nodes","title":"Create the queue"},{"location":"tutorials/create-your-own-queue/#configure-automatic-host-provisioning","text":"If you want to enable automatic host provisioning, edit this file: / apps / soca / cluster_manager / settings / queue_mapping . yml","title":"Configure automatic host provisioning"},{"location":"tutorials/create-your-own-queue/#option1-i-want-to-use-the-same-settings-as-an-existing-queue","text":"In this case, simply update the array with your new queue queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] # <- Add your queue to the array instance_ami : \"ami-1234567\" instance_type : \"c5.large\" ...","title":"Option1: I want to use the same settings as an existing queue"},{"location":"tutorials/create-your-own-queue/#option2-i-want-to-configure-specific-settings","text":"In this case, you will first need to create a new section on the YAML file (see example with memory) queue_type : compute : queues : [ \"queue1\" ] instance_ami : \"ami-1234567\" instance_type : \"c5.large\" scratch_size : \"100\" memory : # <- Add new section queues : [ \"queue2\" ] instance_ami : \"ami-9876543\" instance_type : \"r5.24xlarge\" scratch_size : \"600\" Finally, add a new crontab on the scheduler machine (as root). Use -c to path to the YAML file and -t to the YAML section you just created */3 * * * * source /etc/environment ; /apps/python/latest/bin/python3 /apps/soca/cluster_manager/dispatcher.py -c /apps/soca/cluster_manager/settings/queue_mapping.yml -t memory","title":"Option2: I want to configure specific settings"},{"location":"tutorials/create-your-own-queue/#automatic-host-provisioning-logs","text":"All logs queues are stored under / apps / soca / cluster_manager / logs /< queue_name >","title":"Automatic Host provisioning logs"},{"location":"tutorials/create-your-own-queue/#queue-with-alwayson-instances","text":"Important Scale-Out Computing on AWS automatically created one AlwaysOn queue for you called \"alwayson\" during the first installation In this mode, instances will never be stopped programmatically. You are responsible to terminate the capacity manually by deleting the associated CloudFormation stack","title":"Queue with AlwaysOn instances"},{"location":"tutorials/create-your-own-queue/#create-the-queue_1","text":"On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit","title":"Create the queue"},{"location":"tutorials/create-your-own-queue/#start-provisioning-some-capacity","text":"Run python3 apps / soca / cluster_manager / add_nodes . py and enable --keep_forever flag # Launch 1 c5.large always on python3 /apps/soca/cluster_manager/add_nodes.py --instance_type c5.large \\ --desired_capacity 1 \\ --queue cpus \\ --job_name instancealwayson \\ --job_owner mcrozes \\ --keep_forever IMPORTANT: You specified --keep-forever flag. This instance will be running 24 /7 until you MANUALLY terminate the Cloudformation Stack If you need help with this script, run python3 add_nodes . py - h","title":"Start provisioning some capacity"},{"location":"tutorials/create-your-own-queue/#delete-alwayson-capacity","text":"Simply go to your CloudFormation console, locate the stack following the naming convention: soca- cluster-name -keepforever- queue_name -uniqueid and terminate it.","title":"Delete AlwaysOn capacity"},{"location":"tutorials/integration-ec2-job-parameters/","text":"Scale-Out Computing on AWS made job submission on EC2 very easy and is fully integrated with EC2. Below is a list of parameters you can specify when you request your simulation to ensure the hardware provisioned will exactly match your simulation requirements. Info If you don't specify them, your job will use the default values configured for your queue (see / apps / soca / cluster_manager / settings / queue_mapping . yml ) Compute \u00b6 Parameter Note Example -l instance_type Reference to the type of instance you want to provision for the job -l instance_type=c5.xlarge -l instance_ami Reference to a custom AMI you want to use -l instance_ami=ami-12345 -l base_os Select the base OS of the AMI (centos7, rhel7 or amazonlinux2) -l base_os=centos7 -l spot_price Enable support for spot instance with your maximum bid -l spot_price=1.5 -l subnet_id Deploy capacity in a specific private subnet -l subnet_id=sub-12345 -l ht_support Enable or disable support for Hyper Threading (disabled by default) -l ht_support=yes Storage \u00b6 Parameter Note Example -l root_size Reference to the size of the local root volume you want to allocate (in GB) -l root_size=600 -l scratch_size Reference to the size of the local /scratch (SSD - gp2 type) you want to allocate (in GB). -l scratch_size=600 -l scratch_iops IOps to specify for scratch disk. When used, EBS default to io1 type -l scratch_iops=3000 -l fsx_lustre_bucket Create an epehmeral FSx for your job and mount the S3 bucket specified -l fsx_lustre_bucket=s3://my_bucket/mypath -l fsx_lustre_size Size in GB of your FSx. Default to the smallest (1200GB) option. Must be 1200, 2400 or increment of 3600 -l fsx_lustre_size=7200 -l fsx_lustre_dns DNS of the FSx you want to mount -l fsx_lustre_dns=fs-abcde.fsx.us-west-2.amazonaws.com Network \u00b6 Parameter Note Example -l placement_group Enable or disable placement group (enabled by default when using more than 1 node unless placement_group=false is specified) -l placement_group=true -l efa_support Enable Elastic Fabric Adapter (when instance supports it) -l efa_support=yes","title":"Job customization for EC2"},{"location":"tutorials/integration-ec2-job-parameters/#compute","text":"Parameter Note Example -l instance_type Reference to the type of instance you want to provision for the job -l instance_type=c5.xlarge -l instance_ami Reference to a custom AMI you want to use -l instance_ami=ami-12345 -l base_os Select the base OS of the AMI (centos7, rhel7 or amazonlinux2) -l base_os=centos7 -l spot_price Enable support for spot instance with your maximum bid -l spot_price=1.5 -l subnet_id Deploy capacity in a specific private subnet -l subnet_id=sub-12345 -l ht_support Enable or disable support for Hyper Threading (disabled by default) -l ht_support=yes","title":"Compute"},{"location":"tutorials/integration-ec2-job-parameters/#storage","text":"Parameter Note Example -l root_size Reference to the size of the local root volume you want to allocate (in GB) -l root_size=600 -l scratch_size Reference to the size of the local /scratch (SSD - gp2 type) you want to allocate (in GB). -l scratch_size=600 -l scratch_iops IOps to specify for scratch disk. When used, EBS default to io1 type -l scratch_iops=3000 -l fsx_lustre_bucket Create an epehmeral FSx for your job and mount the S3 bucket specified -l fsx_lustre_bucket=s3://my_bucket/mypath -l fsx_lustre_size Size in GB of your FSx. Default to the smallest (1200GB) option. Must be 1200, 2400 or increment of 3600 -l fsx_lustre_size=7200 -l fsx_lustre_dns DNS of the FSx you want to mount -l fsx_lustre_dns=fs-abcde.fsx.us-west-2.amazonaws.com","title":"Storage"},{"location":"tutorials/integration-ec2-job-parameters/#network","text":"Parameter Note Example -l placement_group Enable or disable placement group (enabled by default when using more than 1 node unless placement_group=false is specified) -l placement_group=true -l efa_support Enable Elastic Fabric Adapter (when instance supports it) -l efa_support=yes","title":"Network"},{"location":"tutorials/job-licenses-flexlm/","text":"In this page, we will see how Scale-Out Computing on AWS manages job and capacity provisioning based on license availabilities. Example configuration Test settings used for all examples: License Server Hostname: licenses . soca . dev License Server port: 5000 License Daemon port: 5001 Feature to check: Audio_System_Toolbox Scale-Out Computing on AWS cluster name: rctest Firewall Configuration \u00b6 Depending your configuration, you may need to edit the security groups to allow traffic to/from your license servers. FlexLM server installed on Scheduler host No further actions are required if you have installed your FlexLM server on the scheduler host as Scale-Out Computing on AWS automatically whitelist all traffic between the scheduler and the compute nodes. Warning FlexLM configure two ports for each application (DAEMON and SERVER ports). You need to whitelist both of them. Allow traffic from your license server IP to Scale-Out Computing on AWS Assuming my license server IP is 10.0.15.18 , simply go to the EC2 console, locate your Scheduler and ComputeNode security groups (filter by your Scale-Out Computing on AWS cluster name) associated to your Scale-Out Computing on AWS cluster and whitelist both SERVER and DAEMON ports: Allow traffic from Scale-Out Computing on AWS to your license server Since FlexLM use client/server protocol, you will need to authorize traffic coming from Scale-Out Computing on AWS to your license servers for both SERVER and DAEMON ports. You will need to whitelist the IP for your scheduler as well as the NAT Gateway used by the compute nodes. Your Scheduler Public IP is listed on CloudFormation, to retrieve your NAT Gateway IP, visit VPC console, select NAT Gateway and find the NAT Gateway IP associated to your Scale-Out Computing on AWS cluster. Upload your lmutil \u00b6 lmutil binary is not included with Scale-Out Computing on AWS. You are required to upload it manually and update / apps / soca / cluster_manager / license_check . py with the location of your file. arg = parser . parse_args () lmstat_path = \"PATH_TO_LMUTIL\" if lmstat_path == \"PATH_TO_LMUTIL\" : print ( 'Please specify a link to your lmutil binary (edit line 19 of this file' ) sys . exit ( 1 ) How to retrieve number of licenses available \u00b6 Scale-Out Computing on AWS includes a script ( / apps / soca / cluster_manager / license_check . py ) which output the number of FlexLM available for a given feature. This script takes the following arguments: -s: The license server hostname -p: The port used by your flexlm deamon -f: The feature name (case sensitive) (Optional) -m: Reserve licenses number for non HPC usage Let say you have 30 Audio_System_Toolbox licenses and 4 are currently in use. The command below will list how many licenses are currently available to use for your jobs: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 26 Now let's say you want to reserve 15 licenses for non HPC/Scale-Out Computing on AWS usage: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox -m 15 11 Info license_check . py is simply a lmutil wrapper. You can get the same output by running the command below and adding some regex validations. You can edit the script to match your own requirements if needed lmutil lmstat -a -c 5000 @licenses-soca.dev | grep \"Users of Audio_System_Toolbox:\" Integration with Scale-Out Computing on AWS \u00b6 IMPORTANT The name of the resource must be * _lic_ * . We recommend using < application > _lic_ < feature_name > Update your / apps / soca / cluster_manager / settings / licenses_mapping . yml and create a new resource. This file must follow the YAML syntax. # There is no requirements for section names, but I recommend having 1 section = 1 application matlab : matlab_lic_audiosystemtoolbox : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" # Example for other daemons/features comsol : comsol_lic_acoustic : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f ACOUSTICS\" comsol_lic_cadimport : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f CADIMPORT\" synopsys : synopsys_lic_testbenchruntime : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VT_TestbenchRuntime\" synopsys_lic_vcsruntime : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VCSRuntime_Net\" synopsys_lic_vipambaaxisvt : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VIP-AMBA-AXI-SVT\" This parameter will let Scale-Out Computing on AWS knows your license mapping and capacity will only be provisioned if enough licenses are available based on job's requirements. Since you are about to create a new custom resource, additional configuration is required at the scheduler level. On the scheduler host, edit / var / spool / pbs / sched_priv / sched_config and add a new server_dyn_res server_dyn_res: \"matlab_lic_audiosystemtoolbox !/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" On the same file, add your new resource under resources section. This section will not allow a job to run if the amount of assigned resources exceeds the available amount. resources: \"matlab_lic_audiosystemtoolbox, ncpus, mem, arch, host, vnode, aoe, eoe, compute_node\" Finally, edit / var / spool / pbs / server_priv / resourcedef and add your new resource with type = long ... ht_support type = string base_os type = string fsx_lustre_bucket type = string fsx_lustre_size type = string fsx_lustre_dns type = string matlab_lic_audiosystemtoolbox type = long Once done, restart the scheduler using service pbs restart Test \u00b6 For this example, let's assume we do have 3 \"Audio_System_Toolbox\" licenses available /apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 3 Let's try to submit a job which require 5 licenses qsub -l matlab_lic_audiosystemtoolbox = 5 -- /bin/sleep 600 31 .ip-20-0-2-69 Let's check the log files under / apps / soca / cluster_manager / log /< QUEUE_NAME > . Scale-Out Computing on AWS will ignore this job due to the lack of licenses available [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 3}] [157] [INFO] [Next User is mickael] [157] [INFO] [Next Job for user is ['31']] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_31 as we we dont have enough: matlab_lic_audiosystemtoolbox] If you have multiple jobs in the queue, the license counter is dynamically updated each time the dispatcher script is running (every 3 minutes): [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 10}] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 5}] [157] [INFO] [Checking if we have enough resources available to run job_32] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 0}] [157] [INFO] [Checking if we have enough resources available to run job_33] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_33 as we we dont have enough: matlab_lic_audiosystemtoolbox] Scale-Out Computing on AWS ensures licenses provisioned for given jobs are in use before provisioning capacity for new jobs Let say you have 10 licenses available and you submit job1 and job2 which both have a requirement of 5 licenses. Scale-Out Computing on AWS will determine licenses are available and will start provision the capacity. Shortly after you submit job3 which require another 5 licenses. The first 2 jobs may not have started yet, meaning you still have 10 licenses available (even though the 10 licenses will be used by job1 and job2 as soon as they start). In that case we skip job3 until both job1 and job2 and in running state. Invalid Resource You can not submit if you are using an invalid resource (aka: resource not recognized by the scheduler). If you are getting this error, refer to section Integration with Scale-Out Computing on AWS ) qsub -l matlab_lic_fakeresource = 3 -- /bin/sleep 60 qsub: Unknown resource Resource_List.matlab_lic_fakeresource","title":"Job with FlexLM licenses requirements"},{"location":"tutorials/job-licenses-flexlm/#firewall-configuration","text":"Depending your configuration, you may need to edit the security groups to allow traffic to/from your license servers. FlexLM server installed on Scheduler host No further actions are required if you have installed your FlexLM server on the scheduler host as Scale-Out Computing on AWS automatically whitelist all traffic between the scheduler and the compute nodes. Warning FlexLM configure two ports for each application (DAEMON and SERVER ports). You need to whitelist both of them. Allow traffic from your license server IP to Scale-Out Computing on AWS Assuming my license server IP is 10.0.15.18 , simply go to the EC2 console, locate your Scheduler and ComputeNode security groups (filter by your Scale-Out Computing on AWS cluster name) associated to your Scale-Out Computing on AWS cluster and whitelist both SERVER and DAEMON ports: Allow traffic from Scale-Out Computing on AWS to your license server Since FlexLM use client/server protocol, you will need to authorize traffic coming from Scale-Out Computing on AWS to your license servers for both SERVER and DAEMON ports. You will need to whitelist the IP for your scheduler as well as the NAT Gateway used by the compute nodes. Your Scheduler Public IP is listed on CloudFormation, to retrieve your NAT Gateway IP, visit VPC console, select NAT Gateway and find the NAT Gateway IP associated to your Scale-Out Computing on AWS cluster.","title":"Firewall Configuration"},{"location":"tutorials/job-licenses-flexlm/#upload-your-lmutil","text":"lmutil binary is not included with Scale-Out Computing on AWS. You are required to upload it manually and update / apps / soca / cluster_manager / license_check . py with the location of your file. arg = parser . parse_args () lmstat_path = \"PATH_TO_LMUTIL\" if lmstat_path == \"PATH_TO_LMUTIL\" : print ( 'Please specify a link to your lmutil binary (edit line 19 of this file' ) sys . exit ( 1 )","title":"Upload your lmutil"},{"location":"tutorials/job-licenses-flexlm/#how-to-retrieve-number-of-licenses-available","text":"Scale-Out Computing on AWS includes a script ( / apps / soca / cluster_manager / license_check . py ) which output the number of FlexLM available for a given feature. This script takes the following arguments: -s: The license server hostname -p: The port used by your flexlm deamon -f: The feature name (case sensitive) (Optional) -m: Reserve licenses number for non HPC usage Let say you have 30 Audio_System_Toolbox licenses and 4 are currently in use. The command below will list how many licenses are currently available to use for your jobs: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 26 Now let's say you want to reserve 15 licenses for non HPC/Scale-Out Computing on AWS usage: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox -m 15 11 Info license_check . py is simply a lmutil wrapper. You can get the same output by running the command below and adding some regex validations. You can edit the script to match your own requirements if needed lmutil lmstat -a -c 5000 @licenses-soca.dev | grep \"Users of Audio_System_Toolbox:\"","title":"How to retrieve number of licenses available"},{"location":"tutorials/job-licenses-flexlm/#integration-with-scale-out-computing-on-aws","text":"IMPORTANT The name of the resource must be * _lic_ * . We recommend using < application > _lic_ < feature_name > Update your / apps / soca / cluster_manager / settings / licenses_mapping . yml and create a new resource. This file must follow the YAML syntax. # There is no requirements for section names, but I recommend having 1 section = 1 application matlab : matlab_lic_audiosystemtoolbox : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" # Example for other daemons/features comsol : comsol_lic_acoustic : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f ACOUSTICS\" comsol_lic_cadimport : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f CADIMPORT\" synopsys : synopsys_lic_testbenchruntime : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VT_TestbenchRuntime\" synopsys_lic_vcsruntime : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VCSRuntime_Net\" synopsys_lic_vipambaaxisvt : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VIP-AMBA-AXI-SVT\" This parameter will let Scale-Out Computing on AWS knows your license mapping and capacity will only be provisioned if enough licenses are available based on job's requirements. Since you are about to create a new custom resource, additional configuration is required at the scheduler level. On the scheduler host, edit / var / spool / pbs / sched_priv / sched_config and add a new server_dyn_res server_dyn_res: \"matlab_lic_audiosystemtoolbox !/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" On the same file, add your new resource under resources section. This section will not allow a job to run if the amount of assigned resources exceeds the available amount. resources: \"matlab_lic_audiosystemtoolbox, ncpus, mem, arch, host, vnode, aoe, eoe, compute_node\" Finally, edit / var / spool / pbs / server_priv / resourcedef and add your new resource with type = long ... ht_support type = string base_os type = string fsx_lustre_bucket type = string fsx_lustre_size type = string fsx_lustre_dns type = string matlab_lic_audiosystemtoolbox type = long Once done, restart the scheduler using service pbs restart","title":"Integration with Scale-Out Computing on AWS"},{"location":"tutorials/job-licenses-flexlm/#test","text":"For this example, let's assume we do have 3 \"Audio_System_Toolbox\" licenses available /apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 3 Let's try to submit a job which require 5 licenses qsub -l matlab_lic_audiosystemtoolbox = 5 -- /bin/sleep 600 31 .ip-20-0-2-69 Let's check the log files under / apps / soca / cluster_manager / log /< QUEUE_NAME > . Scale-Out Computing on AWS will ignore this job due to the lack of licenses available [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 3}] [157] [INFO] [Next User is mickael] [157] [INFO] [Next Job for user is ['31']] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_31 as we we dont have enough: matlab_lic_audiosystemtoolbox] If you have multiple jobs in the queue, the license counter is dynamically updated each time the dispatcher script is running (every 3 minutes): [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 10}] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 5}] [157] [INFO] [Checking if we have enough resources available to run job_32] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 0}] [157] [INFO] [Checking if we have enough resources available to run job_33] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_33 as we we dont have enough: matlab_lic_audiosystemtoolbox] Scale-Out Computing on AWS ensures licenses provisioned for given jobs are in use before provisioning capacity for new jobs Let say you have 10 licenses available and you submit job1 and job2 which both have a requirement of 5 licenses. Scale-Out Computing on AWS will determine licenses are available and will start provision the capacity. Shortly after you submit job3 which require another 5 licenses. The first 2 jobs may not have started yet, meaning you still have 10 licenses available (even though the 10 licenses will be used by job1 and job2 as soon as they start). In that case we skip job3 until both job1 and job2 and in running state. Invalid Resource You can not submit if you are using an invalid resource (aka: resource not recognized by the scheduler). If you are getting this error, refer to section Integration with Scale-Out Computing on AWS ) qsub -l matlab_lic_fakeresource = 3 -- /bin/sleep 60 qsub: Unknown resource Resource_List.matlab_lic_fakeresource","title":"Test"},{"location":"tutorials/job-start-stop-email-notification/","text":"In this page, I will show you how to configure email notification when your job start/stop. For this example, I will use Simple Email Service (SES) , but you can use any SMTP provider. Info Note1: By default, the Scale-Out Computing on AWS admin user you created during the installation does not have any associated email address. If you want to use this account you must edit LDAP and add the \"mail\" attribute. Note2: All qmgr command must be executed on the scheduler host Configure SES sender domain \u00b6 Open the SES console and verify your domain (or specific email addresses). For this example I will verify my entire domain (soca.dev) and enable DKIM support to prevent email spoofing. Click 'Verify this domain', you will get list of DNS records to update for verification. Once done, wait a couple of hours and you will receive a confirmation when your DNS are validated. Configure Recipients addresses \u00b6 By default SES limits you to send email to unique recipients which you will need verify manually If you want to be able to send email to any addresses, you need to request production access . Notification code \u00b6 Create a hook file (note: this file can be found under / apps / soca / cluster_hooks / job_notification . py on your Scale-Out Computing on AWS cluster) Edit the following section to match your SES settings ses_sender_email = '<SES_SENDER_EMAIL_ADDRESS_HERE>' ses_region = '<YOUR_SES_REGION_HERE>' Create the hooks \u00b6 Once your script is created, configure your scheduler hooks by running the following commands: user@host: qmgr -c \"create hook notify_job_start event=runjob\" user@host: qmgr -c \"create hook notify_job_complete event=execjob_end\" user@host: qmgr -c \"import hook notify_job_start application/x-python default /apps/soca/cluster_hooks/job_notifications.py\" user@host: qmgr -c \"import hook notify_job_complete application/x-python default /apps/soca/cluster_hooks/job_notifications.py\" Note: If you make any change to the python file, you must re-run the import hook command Test \u00b6 Let's submit a test job which will last 5 minutes qsub -N mytestjob -- /bin/sleep 300 Now let's verify if I received the alerts correctly. Job start: 5 minutes later: Add/Update email \u00b6 Run ldapsearch - x uid =< USER > command to verify if your user has a valid mail attribute and if this attribute is pointing to the correct email address. The example below shows a user without email attribute. user@host: ldapsearch -x uid = mickael ## mickael, People, soca.local dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael To add/update an email address, create a new ldif file (eg: update_email.ldif) and add the following content dn : uid = mickael , ou = People , dc = soca , dc = local changetype : modify add : mail mail : mickael @ soca . dev Then execute the ldapadd as root user@host: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f update_email.ldif modifying entry \"uid=mickael,ou=People,dc=soca,dc=local\" Finally re-run the ldapsearch command and validate your user now has mail attribute user@host: ldapsearch -x uid = mickael dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael mail: mickael@soca.dev Check the logs \u00b6 Scheduler hooks are located: /var/spool/pbs/server_logs/ for notify_job_start on the Scheduler /var/spool/pbs/mom_logs/ for notify_job_complete on the Execution Host(s)","title":"Automatic emails when your job start/stop"},{"location":"tutorials/job-start-stop-email-notification/#configure-ses-sender-domain","text":"Open the SES console and verify your domain (or specific email addresses). For this example I will verify my entire domain (soca.dev) and enable DKIM support to prevent email spoofing. Click 'Verify this domain', you will get list of DNS records to update for verification. Once done, wait a couple of hours and you will receive a confirmation when your DNS are validated.","title":"Configure SES sender domain"},{"location":"tutorials/job-start-stop-email-notification/#configure-recipients-addresses","text":"By default SES limits you to send email to unique recipients which you will need verify manually If you want to be able to send email to any addresses, you need to request production access .","title":"Configure Recipients addresses"},{"location":"tutorials/job-start-stop-email-notification/#notification-code","text":"Create a hook file (note: this file can be found under / apps / soca / cluster_hooks / job_notification . py on your Scale-Out Computing on AWS cluster) Edit the following section to match your SES settings ses_sender_email = '<SES_SENDER_EMAIL_ADDRESS_HERE>' ses_region = '<YOUR_SES_REGION_HERE>'","title":"Notification code"},{"location":"tutorials/job-start-stop-email-notification/#create-the-hooks","text":"Once your script is created, configure your scheduler hooks by running the following commands: user@host: qmgr -c \"create hook notify_job_start event=runjob\" user@host: qmgr -c \"create hook notify_job_complete event=execjob_end\" user@host: qmgr -c \"import hook notify_job_start application/x-python default /apps/soca/cluster_hooks/job_notifications.py\" user@host: qmgr -c \"import hook notify_job_complete application/x-python default /apps/soca/cluster_hooks/job_notifications.py\" Note: If you make any change to the python file, you must re-run the import hook command","title":"Create the hooks"},{"location":"tutorials/job-start-stop-email-notification/#test","text":"Let's submit a test job which will last 5 minutes qsub -N mytestjob -- /bin/sleep 300 Now let's verify if I received the alerts correctly. Job start: 5 minutes later:","title":"Test"},{"location":"tutorials/job-start-stop-email-notification/#addupdate-email","text":"Run ldapsearch - x uid =< USER > command to verify if your user has a valid mail attribute and if this attribute is pointing to the correct email address. The example below shows a user without email attribute. user@host: ldapsearch -x uid = mickael ## mickael, People, soca.local dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael To add/update an email address, create a new ldif file (eg: update_email.ldif) and add the following content dn : uid = mickael , ou = People , dc = soca , dc = local changetype : modify add : mail mail : mickael @ soca . dev Then execute the ldapadd as root user@host: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f update_email.ldif modifying entry \"uid=mickael,ou=People,dc=soca,dc=local\" Finally re-run the ldapsearch command and validate your user now has mail attribute user@host: ldapsearch -x uid = mickael dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael mail: mickael@soca.dev","title":"Add/Update email"},{"location":"tutorials/job-start-stop-email-notification/#check-the-logs","text":"Scheduler hooks are located: /var/spool/pbs/server_logs/ for notify_job_start on the Scheduler /var/spool/pbs/mom_logs/ for notify_job_complete on the Execution Host(s)","title":"Check the logs"},{"location":"tutorials/launch-always-on-instances/","text":"Why AlwaysOn instances? \u00b6 By default, Scale-Out Computing on AWS provision on-demand capacity when there are jobs in the queue. This mean any job submitted will wait in the queue 5 to 8 minutes until EC2 capacity is ready. If you want to avoid this penalty, you can provision \"AlwaysOn instance\". Please note you will be charged until you manually terminate it. How launch an AlwaysOn instance \u00b6 On your scheduler host, sudo as root and run source / etc / environment to load Scale-Out Computing on AWS shell and then execute / apps / soca / cluster_manager / add_nodes . py [ root@ip-40-0-22-232 ~ ] # python3 /apps/soca/cluster_manager/add_nodes.py -h usage: add_nodes.py [ -h ] --instance_type [ INSTANCE_TYPE ] --desired_capacity [ DESIRED_CAPACITY ] --queue [ QUEUE ] [ --instance_ami [ INSTANCE_AMI ]] [ --subnet_id SUBNET_ID ] [ --job_id [ JOB_ID ]] --job_name [ JOB_NAME ] --job_owner [ JOB_OWNER ] [ --job_project [ JOB_PROJECT ]] [ --scratch_size [ SCRATCH_SIZE ]] [ --placement_group PLACEMENT_GROUP ] [ --tags [ TAGS ]] [ --keep_forever ] [ --base_os BASE_OS ] [ --efa ] [ --spot_price [ SPOT_PRICE ]] optional arguments: -h, --help show this help message and exit --instance_type [ INSTANCE_TYPE ] Instance type you want to deploy --desired_capacity [ DESIRED_CAPACITY ] Number of EC2 instances to deploy --queue [ QUEUE ] Queue to map the capacity --instance_ami [ INSTANCE_AMI ] AMI to use --subnet_id SUBNET_ID Launch capacity in a special subnet --job_id [ JOB_ID ] Job ID for which the capacity is being provisioned --job_name [ JOB_NAME ] Job Name for which the capacity is being provisioned --job_owner [ JOB_OWNER ] Job Owner for which the capacity is being provisioned --job_project [ JOB_PROJECT ] Job Owner for which the capacity is being provisioned --scratch_size [ SCRATCH_SIZE ] Size of /scratch in GB --placement_group PLACEMENT_GROUP Enable or disable placement group --tags [ TAGS ] Tags, format must be { 'Key' : 'Value' } --keep_forever Wheter or not capacity will stay forever --base_os BASE_OS Specify custom Base OK --efa Support for EFA --spot_price [ SPOT_PRICE ] Spot Price To enable \"AlwaysOn\" instance, make sure to use --keep_forever tag and use alwayson queue. If you do not want to use alwayson instance, make sure the queue you have created has been configured correctly to support AlwaysOn ( see instructions ) See example below (note: you can use additional parameters if needed) python3 /apps/soca/cluster_manager/add_nodes.py --instance_type = c5.large \\ --desired_capacity = 1 \\ --keep_forever \\ --job_owner mickael --job_name always_on_capacity --queue alwayson When the capacity is available, simply run a job and specify alwayson as queue name How terminate an AlwaysOn instance \u00b6 Simply go to your CloudFormation console, locate the stack following the naming convention: soca -< cluster_name >- keepforever -< queue_name >- uniqueid and terminate it.","title":"Launch AlwaysOn nodes"},{"location":"tutorials/launch-always-on-instances/#why-alwayson-instances","text":"By default, Scale-Out Computing on AWS provision on-demand capacity when there are jobs in the queue. This mean any job submitted will wait in the queue 5 to 8 minutes until EC2 capacity is ready. If you want to avoid this penalty, you can provision \"AlwaysOn instance\". Please note you will be charged until you manually terminate it.","title":"Why AlwaysOn instances?"},{"location":"tutorials/launch-always-on-instances/#how-launch-an-alwayson-instance","text":"On your scheduler host, sudo as root and run source / etc / environment to load Scale-Out Computing on AWS shell and then execute / apps / soca / cluster_manager / add_nodes . py [ root@ip-40-0-22-232 ~ ] # python3 /apps/soca/cluster_manager/add_nodes.py -h usage: add_nodes.py [ -h ] --instance_type [ INSTANCE_TYPE ] --desired_capacity [ DESIRED_CAPACITY ] --queue [ QUEUE ] [ --instance_ami [ INSTANCE_AMI ]] [ --subnet_id SUBNET_ID ] [ --job_id [ JOB_ID ]] --job_name [ JOB_NAME ] --job_owner [ JOB_OWNER ] [ --job_project [ JOB_PROJECT ]] [ --scratch_size [ SCRATCH_SIZE ]] [ --placement_group PLACEMENT_GROUP ] [ --tags [ TAGS ]] [ --keep_forever ] [ --base_os BASE_OS ] [ --efa ] [ --spot_price [ SPOT_PRICE ]] optional arguments: -h, --help show this help message and exit --instance_type [ INSTANCE_TYPE ] Instance type you want to deploy --desired_capacity [ DESIRED_CAPACITY ] Number of EC2 instances to deploy --queue [ QUEUE ] Queue to map the capacity --instance_ami [ INSTANCE_AMI ] AMI to use --subnet_id SUBNET_ID Launch capacity in a special subnet --job_id [ JOB_ID ] Job ID for which the capacity is being provisioned --job_name [ JOB_NAME ] Job Name for which the capacity is being provisioned --job_owner [ JOB_OWNER ] Job Owner for which the capacity is being provisioned --job_project [ JOB_PROJECT ] Job Owner for which the capacity is being provisioned --scratch_size [ SCRATCH_SIZE ] Size of /scratch in GB --placement_group PLACEMENT_GROUP Enable or disable placement group --tags [ TAGS ] Tags, format must be { 'Key' : 'Value' } --keep_forever Wheter or not capacity will stay forever --base_os BASE_OS Specify custom Base OK --efa Support for EFA --spot_price [ SPOT_PRICE ] Spot Price To enable \"AlwaysOn\" instance, make sure to use --keep_forever tag and use alwayson queue. If you do not want to use alwayson instance, make sure the queue you have created has been configured correctly to support AlwaysOn ( see instructions ) See example below (note: you can use additional parameters if needed) python3 /apps/soca/cluster_manager/add_nodes.py --instance_type = c5.large \\ --desired_capacity = 1 \\ --keep_forever \\ --job_owner mickael --job_name always_on_capacity --queue alwayson When the capacity is available, simply run a job and specify alwayson as queue name","title":"How launch an AlwaysOn instance"},{"location":"tutorials/launch-always-on-instances/#how-terminate-an-alwayson-instance","text":"Simply go to your CloudFormation console, locate the stack following the naming convention: soca -< cluster_name >- keepforever -< queue_name >- uniqueid and terminate it.","title":"How terminate an AlwaysOn instance"},{"location":"tutorials/launch-your-first-job/","text":"Submit your job \u00b6 Things to know before you start Jobs start on average 6 minutes after submission (this value may differ depending on the number and type of compute resource you need to be provisioned) You can launch 'AlwaysOn' instances if you want to avoid the ColdStart penalty If your simulation requires a lot of disk I/O, it's recommended to use high performance SSD-NVMe disks (using /scratch location) and not default $HOME path Create a simple text file and name it \"job_submit.que\". See below for a simple template (you will be required to edit whatever is between **) # !/bin/bash # # BEGIN PBS SETTINGS: Note PBS lines MUST start with # # PBS -N **your_job_name** # PBS -V -j oe -o **your_job_name**.qlog # PBS -P **your_project** # PBS -q **your_queue** # PBS -l nodes = **number_of_nodes_for_this_job** # # END PBS SETTINGS # # BEGIN ACTUAL CODE ** your code goes here ** # # END ACTUAL CODE Run your job \u00b6 Run qsub job_submit . que to submit your job to the queue. user@host:~$ qsub job_submit.que 3323.ip-10-10-10-28 If your qsub command succeed, you will receive an id for your job (3323 in this example). To get more information about this job, run qstat - f 3323 (or qstat - f 3323 - x is the job is already terminated). Your job will start as soon as resources are available (usually within 5 minutes after job submission) Delete a job from the queue \u00b6 Run qdel < job_id > to remove a job from the queue. If the job was running, associated capacity will be terminated within 4 minutes. Custom AWS scheduler resources (optional) \u00b6 Here is a list of scheduler resources specially designed for workloads running on AWS. The line starting with -l (lowercase L) is meant to define scheduler resources which will be used by this job. Syntax is as follow: In a script: # PBS - l parameter_name = parameter_value , parameter_name_2 = parameter_value_2 Using qsub: qsub - l parameter_name = parameter_value - l parameter_name_2 = parameter_value_2 myscript . sh If you don't specify them, your job will use the default values configured for your queue (see / apps / soca / cluster_manager / settings / queue_mapping . yml ) Specify an EC2 Instance Type (optional) \u00b6 Scale-Out Computing on AWS supports all type of EC2 instance. If you don't specify it, job will use a default type which may not be optimal (eg: simulation is memory intensive but default EC2 is compute optimized) If you are not familiar with EC2 instances, take some time to review https://aws.amazon.com/ec2/instance-types/ If you want to force utilization of a specific instance type (and not use the default one), simply change the line and modify instance_type value # PBS - l [ existing_parameters ...], instance_type =** instance_type_value ** Specify a license restriction (optional) \u00b6 License Mapping Please refer to /apps/soca/cluster_manager/settings/licenses_mapping.yml for a list of licenses you can restrict. Contact your Administrator if your license is not available yet. If your job needs to check-out a specific license to run, you want to make sure enough licenses are available before provisioning capacity for the job. To do so, you can add a new resource which will be your license name and the number of license you need. Example: Your job will only start if we have at least 2 Synopsys VCSRuntime_Net licenses available. # PBS - l [ existing_parameters ...], synopsys_lic_vcsruntimenet = 2 Manage your application logs \u00b6 PBS will automatically generate a .qlog file once the job is complete as shown below. # PBS - V - j oe - o ** your_job_name ** . qlog If you need more verbose log, we recommend you using STDERR/STDOUT redirection on your code My job is queued. What next? (AWS orchestration) \u00b6 First, let's make sure your jobs have been sent to the queue. You can run default qstat or use aligoqstat which is a custom wrapper developed for Scale-Out Computing on AWS. Web Based CLI As soon as jobs are sent to the queue, our in-house dispatcher script which will decide if the job can start based on hardware availabilities, priorities or license requirements. Run qstat - f ** job_id ** | grep Resource . Web Based CLI If you see stack_id or compute_node resource (under select), that means all requirements are met and capacity is being provisioned (aka: CloudFormation stack is created and capacity is being provisioned). Look at your EC2 console. This is what you will see (syntax is **cluster_id**-compute-node-**job_id** ): Instances are being provisioned successfully, now let's make sure they are correctly being added to the scheduler by running pbsnodes - a Note: PBS is updated as soon as the host are being added to EC2. You will need to wait a couple of minutes before the state change from \"down\" to \"free\" as Scale-Out Computing on AWS has to configure each node (install libraries, scheduler ...) user @host : ~ $ pbsnodes - a #Host Ready ip - 90 - 0 - 118 - 49 Mom = ip - 90 - 0 - 118 - 49. us - west - 2. compute . internal ntype = PBS state = free pcpus = 16 jobs = 1. ip - 90 - 0 - 24 - 214 / 0 resources_available . arch = linux resources_available . availability_zone = us - west - 2 a resources_available . compute_node = job1 resources_available . host = ip - 90 - 0 - 118 - 49 resources_available . instance_type = c5 .4 xlarge resources_available . mem = 31890060 kb resources_available . ncpus = 16 resources_available . subnet_id = subnet - 055 c0dcdd6ddbb020 resources_available . vnode = ip - 90 - 0 - 118 - 49 resources_assigned . accelerator_memory = 0 kb resources_assigned . hbmem = 0 kb resources_assigned . mem = 0 kb resources_assigned . naccelerators = 0 resources_assigned . ncpus = 1 resources_assigned . vmem = 0 kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23 : 30 : 05 2019 # Host not ready yet ip - 90 - 0 - 188 - 37 Mom = ip - 90 - 0 - 188 - 37. us - west - 2. compute . internal ntype = PBS state = state - unknown , down resources_available . availability_zone = us - west - 2 c resources_available . compute_node = job2 resources_available . host = ip - 90 - 0 - 188 - 37 resources_available . instance_type = r5 . xlarge resources_available . subnet_id = subnet - 0 d046c8668ccfdcb0 resources_available . vnode = ip - 90 - 0 - 188 - 37 resources_assigned . accelerator_memory = 0 kb resources_assigned . hbmem = 0 kb resources_assigned . mem = 0 kb resources_assigned . naccelerators = 0 resources_assigned . ncpus = 0 resources_assigned . vmem = 0 kb queue = normal comment = node down : communication closed resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23 : 28 : 05 2019 ` Simply wait a couple of minutes. Your jobs will start as soon as the PBS nodes are configured. The web ui will also reflect this change. Examples \u00b6 Refer to this page to get a list of all supported parameters For the rest of the examples below, I will run a simple script named \"script.sh\" with the following content: #!/bin/bash # Will output the hostname of the host where the script is executed # If using MPI (more than 1 node), you will get the hostname of all the hosts allocated for your job echo ` hostname ` Run a simple script on 1 node using default settings on 'normal' queue #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1 Run a simple script on 1 node using default settings on 'normal' queue #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1 Run a simple MPI script on 3 nodes using custom EC2 instance type This job will use a 3 c5.18xlarge instances #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log Run a simple script on 3 nodes using custom License Restriction This job will only start if we have at least 4 Comsol Acoustic licenses available #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge,comsol_lic_acoustic=4 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log Run a simple script on 5 nodes using custom AMI This job will use a user-specified AMI ID #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Run a simple script on 5 nodes using custom AMI using a different OS This job will use a user-specified AMI ID which use a operating system different than the scheduler #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde,base_os=centos7 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Run a simple script on 5 m5.24xlarge SPOT instances as long as bid price is lower than $2.5 per hour This job will use SPOT instances. Instances will be automatically terminated if BID price is higher than $2.5 / per hour per instance #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=2.5 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # m5.24xlarge is 48 cores so -np is 48 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh > my_output.log Submit a job with EFA Make sure to use an instance type supported by EFA https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=true ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Combine everything Custom AMI running on a different distribution than the scheduler, with EFA enable, without placement group and within a specific subnet_id #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal ## Resources can be specified on multiple lines #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=yes #PBS -l placement_group=false,base_os=rhel7,ami_id=ami-12345,subnet_id=sub-abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Launch your first job"},{"location":"tutorials/launch-your-first-job/#submit-your-job","text":"Things to know before you start Jobs start on average 6 minutes after submission (this value may differ depending on the number and type of compute resource you need to be provisioned) You can launch 'AlwaysOn' instances if you want to avoid the ColdStart penalty If your simulation requires a lot of disk I/O, it's recommended to use high performance SSD-NVMe disks (using /scratch location) and not default $HOME path Create a simple text file and name it \"job_submit.que\". See below for a simple template (you will be required to edit whatever is between **) # !/bin/bash # # BEGIN PBS SETTINGS: Note PBS lines MUST start with # # PBS -N **your_job_name** # PBS -V -j oe -o **your_job_name**.qlog # PBS -P **your_project** # PBS -q **your_queue** # PBS -l nodes = **number_of_nodes_for_this_job** # # END PBS SETTINGS # # BEGIN ACTUAL CODE ** your code goes here ** # # END ACTUAL CODE","title":"Submit your job"},{"location":"tutorials/launch-your-first-job/#run-your-job","text":"Run qsub job_submit . que to submit your job to the queue. user@host:~$ qsub job_submit.que 3323.ip-10-10-10-28 If your qsub command succeed, you will receive an id for your job (3323 in this example). To get more information about this job, run qstat - f 3323 (or qstat - f 3323 - x is the job is already terminated). Your job will start as soon as resources are available (usually within 5 minutes after job submission)","title":"Run your job"},{"location":"tutorials/launch-your-first-job/#delete-a-job-from-the-queue","text":"Run qdel < job_id > to remove a job from the queue. If the job was running, associated capacity will be terminated within 4 minutes.","title":"Delete a job from the queue"},{"location":"tutorials/launch-your-first-job/#custom-aws-scheduler-resources-optional","text":"Here is a list of scheduler resources specially designed for workloads running on AWS. The line starting with -l (lowercase L) is meant to define scheduler resources which will be used by this job. Syntax is as follow: In a script: # PBS - l parameter_name = parameter_value , parameter_name_2 = parameter_value_2 Using qsub: qsub - l parameter_name = parameter_value - l parameter_name_2 = parameter_value_2 myscript . sh If you don't specify them, your job will use the default values configured for your queue (see / apps / soca / cluster_manager / settings / queue_mapping . yml )","title":"Custom AWS scheduler resources (optional)"},{"location":"tutorials/launch-your-first-job/#specify-an-ec2-instance-type-optional","text":"Scale-Out Computing on AWS supports all type of EC2 instance. If you don't specify it, job will use a default type which may not be optimal (eg: simulation is memory intensive but default EC2 is compute optimized) If you are not familiar with EC2 instances, take some time to review https://aws.amazon.com/ec2/instance-types/ If you want to force utilization of a specific instance type (and not use the default one), simply change the line and modify instance_type value # PBS - l [ existing_parameters ...], instance_type =** instance_type_value **","title":"Specify an EC2 Instance Type (optional)"},{"location":"tutorials/launch-your-first-job/#specify-a-license-restriction-optional","text":"License Mapping Please refer to /apps/soca/cluster_manager/settings/licenses_mapping.yml for a list of licenses you can restrict. Contact your Administrator if your license is not available yet. If your job needs to check-out a specific license to run, you want to make sure enough licenses are available before provisioning capacity for the job. To do so, you can add a new resource which will be your license name and the number of license you need. Example: Your job will only start if we have at least 2 Synopsys VCSRuntime_Net licenses available. # PBS - l [ existing_parameters ...], synopsys_lic_vcsruntimenet = 2","title":"Specify a license restriction (optional)"},{"location":"tutorials/launch-your-first-job/#manage-your-application-logs","text":"PBS will automatically generate a .qlog file once the job is complete as shown below. # PBS - V - j oe - o ** your_job_name ** . qlog If you need more verbose log, we recommend you using STDERR/STDOUT redirection on your code","title":"Manage your application logs"},{"location":"tutorials/launch-your-first-job/#my-job-is-queued-what-next-aws-orchestration","text":"First, let's make sure your jobs have been sent to the queue. You can run default qstat or use aligoqstat which is a custom wrapper developed for Scale-Out Computing on AWS. Web Based CLI As soon as jobs are sent to the queue, our in-house dispatcher script which will decide if the job can start based on hardware availabilities, priorities or license requirements. Run qstat - f ** job_id ** | grep Resource . Web Based CLI If you see stack_id or compute_node resource (under select), that means all requirements are met and capacity is being provisioned (aka: CloudFormation stack is created and capacity is being provisioned). Look at your EC2 console. This is what you will see (syntax is **cluster_id**-compute-node-**job_id** ): Instances are being provisioned successfully, now let's make sure they are correctly being added to the scheduler by running pbsnodes - a Note: PBS is updated as soon as the host are being added to EC2. You will need to wait a couple of minutes before the state change from \"down\" to \"free\" as Scale-Out Computing on AWS has to configure each node (install libraries, scheduler ...) user @host : ~ $ pbsnodes - a #Host Ready ip - 90 - 0 - 118 - 49 Mom = ip - 90 - 0 - 118 - 49. us - west - 2. compute . internal ntype = PBS state = free pcpus = 16 jobs = 1. ip - 90 - 0 - 24 - 214 / 0 resources_available . arch = linux resources_available . availability_zone = us - west - 2 a resources_available . compute_node = job1 resources_available . host = ip - 90 - 0 - 118 - 49 resources_available . instance_type = c5 .4 xlarge resources_available . mem = 31890060 kb resources_available . ncpus = 16 resources_available . subnet_id = subnet - 055 c0dcdd6ddbb020 resources_available . vnode = ip - 90 - 0 - 118 - 49 resources_assigned . accelerator_memory = 0 kb resources_assigned . hbmem = 0 kb resources_assigned . mem = 0 kb resources_assigned . naccelerators = 0 resources_assigned . ncpus = 1 resources_assigned . vmem = 0 kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23 : 30 : 05 2019 # Host not ready yet ip - 90 - 0 - 188 - 37 Mom = ip - 90 - 0 - 188 - 37. us - west - 2. compute . internal ntype = PBS state = state - unknown , down resources_available . availability_zone = us - west - 2 c resources_available . compute_node = job2 resources_available . host = ip - 90 - 0 - 188 - 37 resources_available . instance_type = r5 . xlarge resources_available . subnet_id = subnet - 0 d046c8668ccfdcb0 resources_available . vnode = ip - 90 - 0 - 188 - 37 resources_assigned . accelerator_memory = 0 kb resources_assigned . hbmem = 0 kb resources_assigned . mem = 0 kb resources_assigned . naccelerators = 0 resources_assigned . ncpus = 0 resources_assigned . vmem = 0 kb queue = normal comment = node down : communication closed resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23 : 28 : 05 2019 ` Simply wait a couple of minutes. Your jobs will start as soon as the PBS nodes are configured. The web ui will also reflect this change.","title":"My job is queued. What next? (AWS orchestration)"},{"location":"tutorials/launch-your-first-job/#examples","text":"Refer to this page to get a list of all supported parameters For the rest of the examples below, I will run a simple script named \"script.sh\" with the following content: #!/bin/bash # Will output the hostname of the host where the script is executed # If using MPI (more than 1 node), you will get the hostname of all the hosts allocated for your job echo ` hostname ` Run a simple script on 1 node using default settings on 'normal' queue #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1 Run a simple script on 1 node using default settings on 'normal' queue #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1 Run a simple MPI script on 3 nodes using custom EC2 instance type This job will use a 3 c5.18xlarge instances #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log Run a simple script on 3 nodes using custom License Restriction This job will only start if we have at least 4 Comsol Acoustic licenses available #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge,comsol_lic_acoustic=4 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log Run a simple script on 5 nodes using custom AMI This job will use a user-specified AMI ID #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Run a simple script on 5 nodes using custom AMI using a different OS This job will use a user-specified AMI ID which use a operating system different than the scheduler #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde,base_os=centos7 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Run a simple script on 5 m5.24xlarge SPOT instances as long as bid price is lower than $2.5 per hour This job will use SPOT instances. Instances will be automatically terminated if BID price is higher than $2.5 / per hour per instance #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=2.5 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # m5.24xlarge is 48 cores so -np is 48 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh > my_output.log Submit a job with EFA Make sure to use an instance type supported by EFA https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=true ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Combine everything Custom AMI running on a different distribution than the scheduler, with EFA enable, without placement group and within a specific subnet_id #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal ## Resources can be specified on multiple lines #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=yes #PBS -l placement_group=false,base_os=rhel7,ami_id=ami-12345,subnet_id=sub-abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Examples"},{"location":"tutorials/manage-ldap-users/","text":"Using Web UI \u00b6 Log in to the Web UI with an admin account and locate \"Users\" section on the left sidebar Add users \u00b6 To create a new user, simply fill out the \"Create New User\" form. Select whether or not the user will be an admin by checking \"Enable Sudo Access\" checkbox You will see a success message if the user is created correctly What is a SUDO user? Users will SUDO permissions will be admin on the cluster and authorized to run any sudo command. Make sure to limit this ability to HPC/AWS/Linux admins and other power users. Delete users \u00b6 To delete a user, select the user you want to delete and check the checkbox You will see a success message if the user is deleted correctly Non-Admins users Users without \"Sudo\" are not authorized to manage LDAP accounts. Using command-line interface \u00b6 If you need to manage the permission programatically, access the scheduler host and execute / apps / soca / cluster_manager / ldap_manager . py python3 /apps/soca/cluster_manager/ldap_manager.py add-user -u newuser -p mynottoosecurepassword Created User: newuser id: 5002 Created group successfully Home directory created correctly Users created via CLI are visible to the web-ui and vice versa Other LDAP operations \u00b6 Scale-Out Computing on AWS uses OpenLDAP and you can interact with your directory using LDIF directly. Scale-Out Computing on AWS LDAP Schema People: OU=People,DC=soca,DC=local Groups: OU=Group,DC=soca,DC=local Sudoers: OU=Sudoers,DC=soca,DC=local (This OU manages sudo permission on the cluster) Admin LDAP account credentials Bind DN (-D): cn=admin,dc=soca,dc=local Password (-y) /root/OpenLdapAdminPassword.txt For example, if you want to create a new group, create a new LDIF file (mynewgroup.ldif) and add the following content: dn : cn = mynewgroup , ou = Group , dc = soca , dc = local objectClass : top objectClass : posixGroup cn : mynewgroup gidNumber : 6000 memberUid : mytestuser Run the following ldapadd command to add your new group: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif adding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\" Finally valid your group has been created correctly using ldapsearch # Validate with Ldapsearch ~ ldapsearch -x cn = mynewgroup #Extended LDIF # # LDAPv3 # base DC=soca,DC=local (default) with scope subtree # filter: cn=mynewgroup # requesting: ALL # # mynewgroup, Group, soca.local dn: cn = mynewgroup,ou = Group,dc = soca,dc = local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: mytestuser Example for LDIF modify operation dn : cn = mynewgroup , ou = Group , dc = soca , dc = local changetype : modify add : memberUid memberUid : anotheruser Example for LDIF delete operation dn : cn = mynewgroup , ou = Group , dc = soca , dc = local changetype : modify delete : memberUid memberUid :: anotheruser # you get the memberUid by running a simple ldapsearch first","title":"Centralized user management"},{"location":"tutorials/manage-ldap-users/#using-web-ui","text":"Log in to the Web UI with an admin account and locate \"Users\" section on the left sidebar","title":"Using Web UI"},{"location":"tutorials/manage-ldap-users/#add-users","text":"To create a new user, simply fill out the \"Create New User\" form. Select whether or not the user will be an admin by checking \"Enable Sudo Access\" checkbox You will see a success message if the user is created correctly What is a SUDO user? Users will SUDO permissions will be admin on the cluster and authorized to run any sudo command. Make sure to limit this ability to HPC/AWS/Linux admins and other power users.","title":"Add users"},{"location":"tutorials/manage-ldap-users/#delete-users","text":"To delete a user, select the user you want to delete and check the checkbox You will see a success message if the user is deleted correctly Non-Admins users Users without \"Sudo\" are not authorized to manage LDAP accounts.","title":"Delete users"},{"location":"tutorials/manage-ldap-users/#using-command-line-interface","text":"If you need to manage the permission programatically, access the scheduler host and execute / apps / soca / cluster_manager / ldap_manager . py python3 /apps/soca/cluster_manager/ldap_manager.py add-user -u newuser -p mynottoosecurepassword Created User: newuser id: 5002 Created group successfully Home directory created correctly Users created via CLI are visible to the web-ui and vice versa","title":"Using command-line interface"},{"location":"tutorials/manage-ldap-users/#other-ldap-operations","text":"Scale-Out Computing on AWS uses OpenLDAP and you can interact with your directory using LDIF directly. Scale-Out Computing on AWS LDAP Schema People: OU=People,DC=soca,DC=local Groups: OU=Group,DC=soca,DC=local Sudoers: OU=Sudoers,DC=soca,DC=local (This OU manages sudo permission on the cluster) Admin LDAP account credentials Bind DN (-D): cn=admin,dc=soca,dc=local Password (-y) /root/OpenLdapAdminPassword.txt For example, if you want to create a new group, create a new LDIF file (mynewgroup.ldif) and add the following content: dn : cn = mynewgroup , ou = Group , dc = soca , dc = local objectClass : top objectClass : posixGroup cn : mynewgroup gidNumber : 6000 memberUid : mytestuser Run the following ldapadd command to add your new group: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif adding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\" Finally valid your group has been created correctly using ldapsearch # Validate with Ldapsearch ~ ldapsearch -x cn = mynewgroup #Extended LDIF # # LDAPv3 # base DC=soca,DC=local (default) with scope subtree # filter: cn=mynewgroup # requesting: ALL # # mynewgroup, Group, soca.local dn: cn = mynewgroup,ou = Group,dc = soca,dc = local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: mytestuser Example for LDIF modify operation dn : cn = mynewgroup , ou = Group , dc = soca , dc = local changetype : modify add : memberUid memberUid : anotheruser Example for LDIF delete operation dn : cn = mynewgroup , ou = Group , dc = soca , dc = local changetype : modify delete : memberUid memberUid :: anotheruser # you get the memberUid by running a simple ldapsearch first","title":"Other LDAP operations"},{"location":"tutorials/troubleshoot-job-queue/","text":"Jobs in dynamic queue \u00b6 First of all, unless you submit a job on the \"alwayson\" queue, it will usually take between 5 to 10 minutes before your job can start as Scale-Out Computing on AWS needs to provision your capacity. This can vary based on the type and number of EC2 instances you have requested for your job. Verify the log \u00b6 If your job is not starting, first verify the queue log under / apps / soca / cluster_manager / logs /< queue_name > . log Verify the job resource \u00b6 This guide assume you have created your queue correctly Run qstat - f < job_id > | grep - i resource and try to locate compute_node or stack_id resource. When your job is launched, these resources does not exist. The script dispatcher . py . running as a crontab and executed every 3 minutes will create these resources automatically. Example of job having all resources configured correctly # Job with Scale-Out Computing on AWS resources bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 :compute_node = job2 Resource_List.stack_id = soca-fpgaami-job-2 Please note these resources are created by dispatcher . py so allow a maximum of 3 minutes between job is submitted and resources are visibles on qstat output # Job without Scale-Out Computing on AWS resources created yet bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 If you see a compute_node different than tbd as well as stack_id , that means Scale-Out Computing on AWS triggered capacity provisioning by creating a new CloudFormation stack. If you go to your CloudFormation console, you should see a new stack being created using the following naming convention: soca -< cluster_name >- job -< job_id > If CloudFormation stack is NOT \"CREATE_COMPLETE\" \u00b6 Click on the stack name then check the \"Events\" tab and refer to any \"CREATE_FAILED\" errors In this example, the size of root device is too small and can be fixed by specify a bigger EBS disk using - l root_size = 75 If CloudFormation stack is \"CREATE_COMPLETE\" \u00b6 First, make sure CloudFormation has created a new \"Launch Template\" for your job. Then navigate to AutoScaling console, select your AutoScaling group and click \"Activity\". You will see any EC2 errors related in this tab. Here is an example of capacity being provisioned correctly Here is an example of capacity provisioning errors: If capacity is being provisioned correctly, go back to Scale-Out Computing on AWS and run pbsnodes - a . Verify the capacity assigned to your job ID (refer to resources_available . compute_node ) is in state = free . pbsnodes - a ip - 60 - 0 - 174 - 166 Mom = ip - 60 - 0 - 174 - 166 . us - west - 2 . compute . internal Port = 15002 pbs_version = 18 . 1 . 4 ntype = PBS state = free pcpus = 1 resources_available . arch = linux resources_available . availability_zone = us - west - 2 c resources_available . compute_node = job2 resources_available . host = ip - 60 - 0 - 174 - 166 resources_available . instance_type = m5 . large resources_available . mem = 7706180 kb resources_available . ncpus = 1 resources_available . subnet_id = subnet - 0 af93e96ed9c4377d resources_available . vnode = ip - 60 - 0 - 174 - 166 resources_assigned . accelerator_memory = 0 kb resources_assigned . hbmem = 0 kb resources_assigned . mem = 0 kb resources_assigned . naccelerators = 0 resources_assigned . ncpus = 0 resources_assigned . vmem = 0 kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sat Oct 12 17 : 37 : 28 2019 If host is not in state = free after 10 minutes, SSH to the host, sudo as root and check the log file located under / root as well as / var / log / message | grep cloud - init","title":"Debug why your jobs are not starting"},{"location":"tutorials/troubleshoot-job-queue/#jobs-in-dynamic-queue","text":"First of all, unless you submit a job on the \"alwayson\" queue, it will usually take between 5 to 10 minutes before your job can start as Scale-Out Computing on AWS needs to provision your capacity. This can vary based on the type and number of EC2 instances you have requested for your job.","title":"Jobs in dynamic queue"},{"location":"tutorials/troubleshoot-job-queue/#verify-the-log","text":"If your job is not starting, first verify the queue log under / apps / soca / cluster_manager / logs /< queue_name > . log","title":"Verify the log"},{"location":"tutorials/troubleshoot-job-queue/#verify-the-job-resource","text":"This guide assume you have created your queue correctly Run qstat - f < job_id > | grep - i resource and try to locate compute_node or stack_id resource. When your job is launched, these resources does not exist. The script dispatcher . py . running as a crontab and executed every 3 minutes will create these resources automatically. Example of job having all resources configured correctly # Job with Scale-Out Computing on AWS resources bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 :compute_node = job2 Resource_List.stack_id = soca-fpgaami-job-2 Please note these resources are created by dispatcher . py so allow a maximum of 3 minutes between job is submitted and resources are visibles on qstat output # Job without Scale-Out Computing on AWS resources created yet bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 If you see a compute_node different than tbd as well as stack_id , that means Scale-Out Computing on AWS triggered capacity provisioning by creating a new CloudFormation stack. If you go to your CloudFormation console, you should see a new stack being created using the following naming convention: soca -< cluster_name >- job -< job_id >","title":"Verify the job resource"},{"location":"tutorials/troubleshoot-job-queue/#if-cloudformation-stack-is-not-create_complete","text":"Click on the stack name then check the \"Events\" tab and refer to any \"CREATE_FAILED\" errors In this example, the size of root device is too small and can be fixed by specify a bigger EBS disk using - l root_size = 75","title":"If CloudFormation stack is NOT \"CREATE_COMPLETE\""},{"location":"tutorials/troubleshoot-job-queue/#if-cloudformation-stack-is-create_complete","text":"First, make sure CloudFormation has created a new \"Launch Template\" for your job. Then navigate to AutoScaling console, select your AutoScaling group and click \"Activity\". You will see any EC2 errors related in this tab. Here is an example of capacity being provisioned correctly Here is an example of capacity provisioning errors: If capacity is being provisioned correctly, go back to Scale-Out Computing on AWS and run pbsnodes - a . Verify the capacity assigned to your job ID (refer to resources_available . compute_node ) is in state = free . pbsnodes - a ip - 60 - 0 - 174 - 166 Mom = ip - 60 - 0 - 174 - 166 . us - west - 2 . compute . internal Port = 15002 pbs_version = 18 . 1 . 4 ntype = PBS state = free pcpus = 1 resources_available . arch = linux resources_available . availability_zone = us - west - 2 c resources_available . compute_node = job2 resources_available . host = ip - 60 - 0 - 174 - 166 resources_available . instance_type = m5 . large resources_available . mem = 7706180 kb resources_available . ncpus = 1 resources_available . subnet_id = subnet - 0 af93e96ed9c4377d resources_available . vnode = ip - 60 - 0 - 174 - 166 resources_assigned . accelerator_memory = 0 kb resources_assigned . hbmem = 0 kb resources_assigned . mem = 0 kb resources_assigned . naccelerators = 0 resources_assigned . ncpus = 0 resources_assigned . vmem = 0 kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sat Oct 12 17 : 37 : 28 2019 If host is not in state = free after 10 minutes, SSH to the host, sudo as root and check the log file located under / root as well as / var / log / message | grep cloud - init","title":"If CloudFormation stack is \"CREATE_COMPLETE\""},{"location":"tutorials/update-soca-dns-ssl-certificate/","text":"By default, Scale-Out Computing on AWS will use a non-friendly DNS name and create a unique certificate to enable access through your HTTPS endpoint. Because it's a self-signed certificate, browsers won't recognized it and you will get a security warning on your first connection. In this page, we will see how you can update Scale-Out Computing on AWS to match your company domain name. Create a new DNS record for Scale-Out Computing on AWS \u00b6 For this example, let's assume I want to use https : // demo . soca . dev . First locate the DNS associated to your ALB endpoint using the AWS console. Create a new CNAME record which point to your ALB endpoint. Once done, validate your DNS is working properly using the nslookup command. user@host: nslookup demo.soca.dev Non-authoritative answer: demo.soca.dev canonical name = soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com. Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .40.2.185 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 54 .68.240.4 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .27.180.89 Upload your SSL certificate to ACM \u00b6 Now that your friendly DNS is running, you will need to update the default ALB certificate to match your new domain. This assume you have a valid SSL certificate signed by a valid Certificate Authority (Symantec, Digicert ...) To upload your certificate, visit the AWS Certificate Manager (ACM) bash and click \"Import a Certificate\". Enter your private key, certificate and certificate chain (optional), then click Import. Once the import is complete, note your certificate identifier. Update your ALB with the new certificate \u00b6 Navigate to your Scale-Out Computing on AWS Load Balancer and choose \"Listeners\" tab. Select your HTTPS listener and click 'Edit' button. Change the default certificate to point to your new certificate and save your change. Update your default domain for DCV \u00b6 Now that you have updated your domain, you must also update DCV to point to the new DNS. Open your Secret Manager bash and select your Scale-Out Computing on AWS cluster configuration. Click \"Retrieve Secret Value\" and then \"Edit\". Find the entry \"LoadBalancerName\" and update the value with your new DNS name (demo.soca.dev in my case) then click Save Validate everything \u00b6 Now that you have your friendly DNS and SSL certificate configured, it's time to test. Visit your new DNS ( https : // demo . soca . dev in my case) and make sure you can access Scale-Out Computing on AWS correctly. Make sure your browser is detecting your new SSL certificate correctly. Finally, create a new DCV session and verify the endpoint is using your new DNS name","title":"Change your DNS name and SSL certificate"},{"location":"tutorials/update-soca-dns-ssl-certificate/#create-a-new-dns-record-for-scale-out-computing-on-aws","text":"For this example, let's assume I want to use https : // demo . soca . dev . First locate the DNS associated to your ALB endpoint using the AWS console. Create a new CNAME record which point to your ALB endpoint. Once done, validate your DNS is working properly using the nslookup command. user@host: nslookup demo.soca.dev Non-authoritative answer: demo.soca.dev canonical name = soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com. Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .40.2.185 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 54 .68.240.4 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .27.180.89","title":"Create a new DNS record for Scale-Out Computing on AWS"},{"location":"tutorials/update-soca-dns-ssl-certificate/#upload-your-ssl-certificate-to-acm","text":"Now that your friendly DNS is running, you will need to update the default ALB certificate to match your new domain. This assume you have a valid SSL certificate signed by a valid Certificate Authority (Symantec, Digicert ...) To upload your certificate, visit the AWS Certificate Manager (ACM) bash and click \"Import a Certificate\". Enter your private key, certificate and certificate chain (optional), then click Import. Once the import is complete, note your certificate identifier.","title":"Upload your SSL certificate to ACM"},{"location":"tutorials/update-soca-dns-ssl-certificate/#update-your-alb-with-the-new-certificate","text":"Navigate to your Scale-Out Computing on AWS Load Balancer and choose \"Listeners\" tab. Select your HTTPS listener and click 'Edit' button. Change the default certificate to point to your new certificate and save your change.","title":"Update your ALB with the new certificate"},{"location":"tutorials/update-soca-dns-ssl-certificate/#update-your-default-domain-for-dcv","text":"Now that you have updated your domain, you must also update DCV to point to the new DNS. Open your Secret Manager bash and select your Scale-Out Computing on AWS cluster configuration. Click \"Retrieve Secret Value\" and then \"Edit\". Find the entry \"LoadBalancerName\" and update the value with your new DNS name (demo.soca.dev in my case) then click Save","title":"Update your default domain for DCV"},{"location":"tutorials/update-soca-dns-ssl-certificate/#validate-everything","text":"Now that you have your friendly DNS and SSL certificate configured, it's time to test. Visit your new DNS ( https : // demo . soca . dev in my case) and make sure you can access Scale-Out Computing on AWS correctly. Make sure your browser is detecting your new SSL certificate correctly. Finally, create a new DCV session and verify the endpoint is using your new DNS name","title":"Validate everything"},{"location":"tutorials/use-custom-ami/","text":"Step 1: Locate your Scale-Out Computing on AWS AMI \u00b6 Run cat / etc / environment | grep Scale - Out Computing on AWS_INSTALL_AMI on your scheduler host 38f9d34dde89:~ mcrozes$ ssh -i <key>> ec2-user@<IP> Last login: Wed Oct 2 20 :06:47 2019 from 205 .251.233.178 _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-uiupdates > source /etc/environment to load Scale-Out Computing on AWS paths [ ec2-user@ip-30-0-1-28 ~ ] $ cat /etc/environment | grep Scale-Out Computing on AWS_INSTALL_AMI export Scale-Out Computing on AWS_INSTALL_AMI = ami-082b5a644766e0e6f [ ec2-user@ip-30-0-1-28 ~ ] $ Step 2: Launch a temporary EC2 instance \u00b6 Launch a new EC2 instance using the Scale - Out Computing on AWS_INSTALL_AMI Step 3: Customize your AMI \u00b6 In this example, I am installing some libraries to enable EFA support Native support for EFA Scale-Out Computing on AWS supports EFA natively using --efa_support=true at job submission Pre-Requisite You can use RHEL7, Centos7 or Amazon Linux 2 distributions Do not use /scratch, /apps or /data partitions as these are restricted location for Scale-Out Computing on AWS First, let me prepare my AMI and do all the customization I need. # Sudo as Root [ ec2-user@ip-30-0-73-38 ~ ] $ sudo su - # Download EFA Installer [ root@ip-30-0-73-38 ~ ] # curl --silent -O https://s3-us-west-2.amazonaws.com/aws-efa-installer/aws-efa-installer-1.5.4.tar.gz 100 44 .5M 100 44 .5M 0 0 84 .9M 0 --:--:-- --:--:-- --:--:-- 84 .9M # Extract [ root@ip-30-0-73-38 ~ ] # tar -xf aws-efa-installer-1.5.4.tar.gz # Navigate to the folder [ root@ip-30-0-73-38 ~ ] # cd aws-efa-installer # Install [ root@ip-30-0-73-38 aws-efa-installer ] # sudo ./efa_installer.sh -y = Starting Amazon Elastic Fabric Adapter Installation Script = = EFA Installer Version: 1 .5.4 = == Installing EFA dependencies == Loaded plugins: extras_suggestions, langpacks, priorities, update-motd Package pciutils-3.5.1-3.amzn2.x86_64 already installed and latest version Resolving Dependencies --> Running transaction check ---> Package kernel-devel.x86_64 0 :4.14.123-111.109.amzn2 will be installed --> Processing Dependency: gcc > = 7 .2.1 for package: kernel-devel-4.14.123-111.109.amzn2.x86_64 ................ .... LOT OF TEXT .... ................ Limits for Elastic Fabric Adapter configured. == Testing EFA device == Starting server... Starting client... bytes #sent #ack total time MB/sec usec/xfer Mxfers/sec 64 10 = 10 1 .2k 0 .02s 0 .06 1124 .90 0 .00 256 10 = 10 5k 0 .00s 16 .15 15 .85 0 .06 1k 10 = 10 20k 0 .00s 61 .69 16 .60 0 .06 4k 10 = 10 80k 0 .00s 227 .56 18 .00 0 .06 64k 10 = 10 1 .2m 0 .00s 876 .15 74 .80 0 .01 1m 10 = 10 20m 0 .01s 2078 .45 504 .50 0 .00 =================================================== EFA installation complete. - Please logout/login to complete the installation. - Libfabric was installed in /opt/amazon/efa - Open MPI was installed in /opt/amazon/openmpi =================================================== Logout of the instance and then log back in. Finally, confirm the EFA adapter is up and running [ root@ip-30-0-82-154 ~ ] # fi_info -p efa provider: efa fabric: EFA-fe80::828:bfff:fe5c:95d4 domain: efa_0-rdm version: 2 .0 type: FI_EP_RDM protocol: FI_PROTO_EFA provider: efa fabric: EFA-fe80::828:bfff:fe5c:95d4 domain: efa_0-dgrm version: 2 .0 type: FI_EP_DGRAM protocol: FI_PROTO_EFA provider: efa ; ofi_rxd fabric: EFA-fe80::828:bfff:fe5c:95d4 domain: efa_0-dgrm version: 1 .0 type: FI_EP_RDM protocol: FI_PROTO_RXD So at this point you have your own custom AMI with EFI software installed. Let's see how we can use this AMI directly with Scale-Out Computing on AWS to run some jobs. Step 4: Create your AMI \u00b6 Go back to EC2 console, locate your instance and click \"Actions > Image > Create Image\" Choose an AMI name and click 'Create Image' Your AMI is now being created. Please note it may take a couple of minutes for the AMI to be ready. To check the status, go to EC2 Console and then click \"AMIs\" on the left sidebar Stop your temporary EC2 instance Once your AMI has been created, you can safely terminate the EC2 instance you just launched as you won't need it anymore. Step 5: Launch a job with your custom AMI \u00b6 Go back to Scale-Out Computing on AWS and prepare to launch a job with some specific parameters: - As you are planning to use a custom AMI, you will be required to specify - l instance_ami =< IMAGE > at job submission. To confirm my job using an EFA enabled AMI, the only command executed is the one which retrieve EFA device information bash-4.2$ qsub -l efa_support = true -l instance_type = c5n.18xlarge -l instance_ami = ami-0b1c9d132469d1c7d -N job-to-test-EFA -- /opt/amazon/efa/bin/fi_info -p efa 19 .ip-30-0-1-28 Wait for your job to start. You can also confirm on the EC2 console that your job is using the correct AMI. You can also check the queue log / apps / soca / cluster_manager / logs /< QUEUE_NAME > to make sure your parameters are validated by the dispatcher. [ 2019 -10-03 00 :53:52,586 ] [ 190 ] [ INFO ] [ efa_support resource is specified, will attach one EFA adapter ] [ 2019 -10-03 00 :53:52,586 ] [ 190 ] [ INFO ] [ image resource is specified, will use new ec2 AMI: ami-0b1c9d132469d1c7d ] [ 2019 -10-03 00 :53:52,586 ] [ 190 ] [ INFO ] [ instancetype resource is specified, will use new ec2 instance type: c5n.18xlarge ] Once your job complete, check the STDOUT and confirm the output of / opt / amazon / efa / bin / fi_info - p efa is correct bash-4.2$ cat job-to-test-EFA.o19 provider: efa fabric: EFA-fe80::49:adff:fef8:88b6 domain: efa_0-rdm version: 2 .0 type: FI_EP_RDM protocol: FI_PROTO_EFA provider: efa fabric: EFA-fe80::49:adff:fef8:88b6 domain: efa_0-dgrm version: 2 .0 type: FI_EP_DGRAM protocol: FI_PROTO_EFA provider: efa ; ofi_rxd fabric: EFA-fe80::49:adff:fef8:88b6 domain: efa_0-dgrm version: 1 .0 type: FI_EP_RDM protocol: FI_PROTO_RXD AMI different than scheduler OS \u00b6 By default, Scale-Out Computing on AWS will try to install the packages based on the scheduler operating system. If your AMI use a different OS, you will need to specify it during job submission using - l base_os parameters. Supported values are centos7, rhel7 or amazonlinux2 # Assuming your Scale-Out Computing on AWS default OS is Centos7 but you want to use a Amazon Linux 2 AMI bash-4.2$ qsub -l base_os = amazonlinux2 -l instance_ami = <YOUR_AMI> script.sh Update default AMI \u00b6 Single job \u00b6 Simply submit a job and use - l instance_ami parameter Entire queue \u00b6 Edit / apps / soca / cluster_manager / settings / queue_mapping . yml and update the default AMI queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"<YOUR_AMI_ID>\" # <- Add your new AMI instance_type : ... Entire cluster \u00b6 Open your Secret Manager console and select your Scale-Out Computing on AWS cluster configuration. Click \u201cRetrieve Secret Value\u201d and then \u201cEdit\u201d. Find the entry \u201cCustomAMI\u201d and update the value with your new AMI ID then click Save","title":"Use your own AMI"},{"location":"tutorials/use-custom-ami/#step-1-locate-your-scale-out-computing-on-aws-ami","text":"Run cat / etc / environment | grep Scale - Out Computing on AWS_INSTALL_AMI on your scheduler host 38f9d34dde89:~ mcrozes$ ssh -i <key>> ec2-user@<IP> Last login: Wed Oct 2 20 :06:47 2019 from 205 .251.233.178 _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-uiupdates > source /etc/environment to load Scale-Out Computing on AWS paths [ ec2-user@ip-30-0-1-28 ~ ] $ cat /etc/environment | grep Scale-Out Computing on AWS_INSTALL_AMI export Scale-Out Computing on AWS_INSTALL_AMI = ami-082b5a644766e0e6f [ ec2-user@ip-30-0-1-28 ~ ] $","title":"Step 1: Locate your Scale-Out Computing on AWS AMI"},{"location":"tutorials/use-custom-ami/#step-2-launch-a-temporary-ec2-instance","text":"Launch a new EC2 instance using the Scale - Out Computing on AWS_INSTALL_AMI","title":"Step 2: Launch a temporary EC2 instance"},{"location":"tutorials/use-custom-ami/#step-3-customize-your-ami","text":"In this example, I am installing some libraries to enable EFA support Native support for EFA Scale-Out Computing on AWS supports EFA natively using --efa_support=true at job submission Pre-Requisite You can use RHEL7, Centos7 or Amazon Linux 2 distributions Do not use /scratch, /apps or /data partitions as these are restricted location for Scale-Out Computing on AWS First, let me prepare my AMI and do all the customization I need. # Sudo as Root [ ec2-user@ip-30-0-73-38 ~ ] $ sudo su - # Download EFA Installer [ root@ip-30-0-73-38 ~ ] # curl --silent -O https://s3-us-west-2.amazonaws.com/aws-efa-installer/aws-efa-installer-1.5.4.tar.gz 100 44 .5M 100 44 .5M 0 0 84 .9M 0 --:--:-- --:--:-- --:--:-- 84 .9M # Extract [ root@ip-30-0-73-38 ~ ] # tar -xf aws-efa-installer-1.5.4.tar.gz # Navigate to the folder [ root@ip-30-0-73-38 ~ ] # cd aws-efa-installer # Install [ root@ip-30-0-73-38 aws-efa-installer ] # sudo ./efa_installer.sh -y = Starting Amazon Elastic Fabric Adapter Installation Script = = EFA Installer Version: 1 .5.4 = == Installing EFA dependencies == Loaded plugins: extras_suggestions, langpacks, priorities, update-motd Package pciutils-3.5.1-3.amzn2.x86_64 already installed and latest version Resolving Dependencies --> Running transaction check ---> Package kernel-devel.x86_64 0 :4.14.123-111.109.amzn2 will be installed --> Processing Dependency: gcc > = 7 .2.1 for package: kernel-devel-4.14.123-111.109.amzn2.x86_64 ................ .... LOT OF TEXT .... ................ Limits for Elastic Fabric Adapter configured. == Testing EFA device == Starting server... Starting client... bytes #sent #ack total time MB/sec usec/xfer Mxfers/sec 64 10 = 10 1 .2k 0 .02s 0 .06 1124 .90 0 .00 256 10 = 10 5k 0 .00s 16 .15 15 .85 0 .06 1k 10 = 10 20k 0 .00s 61 .69 16 .60 0 .06 4k 10 = 10 80k 0 .00s 227 .56 18 .00 0 .06 64k 10 = 10 1 .2m 0 .00s 876 .15 74 .80 0 .01 1m 10 = 10 20m 0 .01s 2078 .45 504 .50 0 .00 =================================================== EFA installation complete. - Please logout/login to complete the installation. - Libfabric was installed in /opt/amazon/efa - Open MPI was installed in /opt/amazon/openmpi =================================================== Logout of the instance and then log back in. Finally, confirm the EFA adapter is up and running [ root@ip-30-0-82-154 ~ ] # fi_info -p efa provider: efa fabric: EFA-fe80::828:bfff:fe5c:95d4 domain: efa_0-rdm version: 2 .0 type: FI_EP_RDM protocol: FI_PROTO_EFA provider: efa fabric: EFA-fe80::828:bfff:fe5c:95d4 domain: efa_0-dgrm version: 2 .0 type: FI_EP_DGRAM protocol: FI_PROTO_EFA provider: efa ; ofi_rxd fabric: EFA-fe80::828:bfff:fe5c:95d4 domain: efa_0-dgrm version: 1 .0 type: FI_EP_RDM protocol: FI_PROTO_RXD So at this point you have your own custom AMI with EFI software installed. Let's see how we can use this AMI directly with Scale-Out Computing on AWS to run some jobs.","title":"Step 3: Customize your AMI"},{"location":"tutorials/use-custom-ami/#step-4-create-your-ami","text":"Go back to EC2 console, locate your instance and click \"Actions > Image > Create Image\" Choose an AMI name and click 'Create Image' Your AMI is now being created. Please note it may take a couple of minutes for the AMI to be ready. To check the status, go to EC2 Console and then click \"AMIs\" on the left sidebar Stop your temporary EC2 instance Once your AMI has been created, you can safely terminate the EC2 instance you just launched as you won't need it anymore.","title":"Step 4: Create your AMI"},{"location":"tutorials/use-custom-ami/#step-5-launch-a-job-with-your-custom-ami","text":"Go back to Scale-Out Computing on AWS and prepare to launch a job with some specific parameters: - As you are planning to use a custom AMI, you will be required to specify - l instance_ami =< IMAGE > at job submission. To confirm my job using an EFA enabled AMI, the only command executed is the one which retrieve EFA device information bash-4.2$ qsub -l efa_support = true -l instance_type = c5n.18xlarge -l instance_ami = ami-0b1c9d132469d1c7d -N job-to-test-EFA -- /opt/amazon/efa/bin/fi_info -p efa 19 .ip-30-0-1-28 Wait for your job to start. You can also confirm on the EC2 console that your job is using the correct AMI. You can also check the queue log / apps / soca / cluster_manager / logs /< QUEUE_NAME > to make sure your parameters are validated by the dispatcher. [ 2019 -10-03 00 :53:52,586 ] [ 190 ] [ INFO ] [ efa_support resource is specified, will attach one EFA adapter ] [ 2019 -10-03 00 :53:52,586 ] [ 190 ] [ INFO ] [ image resource is specified, will use new ec2 AMI: ami-0b1c9d132469d1c7d ] [ 2019 -10-03 00 :53:52,586 ] [ 190 ] [ INFO ] [ instancetype resource is specified, will use new ec2 instance type: c5n.18xlarge ] Once your job complete, check the STDOUT and confirm the output of / opt / amazon / efa / bin / fi_info - p efa is correct bash-4.2$ cat job-to-test-EFA.o19 provider: efa fabric: EFA-fe80::49:adff:fef8:88b6 domain: efa_0-rdm version: 2 .0 type: FI_EP_RDM protocol: FI_PROTO_EFA provider: efa fabric: EFA-fe80::49:adff:fef8:88b6 domain: efa_0-dgrm version: 2 .0 type: FI_EP_DGRAM protocol: FI_PROTO_EFA provider: efa ; ofi_rxd fabric: EFA-fe80::49:adff:fef8:88b6 domain: efa_0-dgrm version: 1 .0 type: FI_EP_RDM protocol: FI_PROTO_RXD","title":"Step 5: Launch a job with your custom AMI"},{"location":"tutorials/use-custom-ami/#ami-different-than-scheduler-os","text":"By default, Scale-Out Computing on AWS will try to install the packages based on the scheduler operating system. If your AMI use a different OS, you will need to specify it during job submission using - l base_os parameters. Supported values are centos7, rhel7 or amazonlinux2 # Assuming your Scale-Out Computing on AWS default OS is Centos7 but you want to use a Amazon Linux 2 AMI bash-4.2$ qsub -l base_os = amazonlinux2 -l instance_ami = <YOUR_AMI> script.sh","title":"AMI different than scheduler OS"},{"location":"tutorials/use-custom-ami/#update-default-ami","text":"","title":"Update default AMI"},{"location":"tutorials/use-custom-ami/#single-job","text":"Simply submit a job and use - l instance_ami parameter","title":"Single job"},{"location":"tutorials/use-custom-ami/#entire-queue","text":"Edit / apps / soca / cluster_manager / settings / queue_mapping . yml and update the default AMI queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"<YOUR_AMI_ID>\" # <- Add your new AMI instance_type : ...","title":"Entire queue"},{"location":"tutorials/use-custom-ami/#entire-cluster","text":"Open your Secret Manager console and select your Scale-Out Computing on AWS cluster configuration. Click \u201cRetrieve Secret Value\u201d and then \u201cEdit\u201d. Find the entry \u201cCustomAMI\u201d and update the value with your new AMI ID then click Save","title":"Entire cluster"}]}