{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Scale-Out Computing on AWS is a solution that helps customers more easily deploy and operate a multiuser environment for computationally intensive workflows. The solution features a large selection of compute resources; fast network backbone; unlimited storage; and budget and cost management directly integrated within AWS. The solution also deploys a user interface (UI) and automation tools that allows you to create your own queues, scheduler resources, Amazon Machine Images (AMIs), software, and libraries. This solution is designed to provide a production ready reference implementation to be a starting point for deploying an AWS environment to run scale-out workloads, allowing you to focus on running simulations designed to solve complex computational problems. Easy installation \u00b6 Installation of your Scale-Out Computing on AWS cluster is fully automated and managed by CloudFormation Did you know? You can have multiple Scale-Out Computing on AWS clusters on the same AWS account Scale-Out Computing on AWS comes with a list of unique tags, making resource tracking easy for AWS Administrators Access your cluster in 1 click \u00b6 You can access your Scale-Out Computing on AWS cluster either using DCV (Desktop Cloud Visualization) 1 or through SSH. Simple Job Submission \u00b6 Scale-Out Computing on AWS supports a list of parameters designed to simplify your job submission on AWS . Advanced users can either manually choose compute/storage/network configuration for their job or simply ignore these parameters and let Scale-Out Computing on AWS picks the most optimal hardware (defined by the HPC administrator) # Advanced Configuration user@host$ qsub -l instance_type = c5n.18xlarge \\ -l instance_ami = ami-123abcde -l nodes = 2 -l scratch_size = 300 -l efa_support = true -l spot_price = 1 .55 myscript.sh # Basic Configuration user@host$ qsub myscript.sh Info Check our Web-Based utility to generate you submission command Refer to this page for tutorial and examples Refer to this page to list all supported parameters OS agnostic and support for custom AMI \u00b6 Customers can integrate their Centos7/Rhel7/AmazonLinux2 AMI automatically by simply using -l instance_ami=<ami_id> at job submission. There is no limitation in term of AMI numbers (you can have 10 jobs running simultaneously using 10 different AMIs). SOCA supports heterogeneous environment, so you can have concurrent jobs running different operating system on the same cluster. AMI using OS different than the scheduler In case your AMI is different than your scheduler host, you can specify the OS manually to ensure packages will be installed based on the node distribution. In this example, we assume your Scale-Out Computing on AWS deployment was done using AmazonLinux2, but you want to submit a job on your personal RHEL7 AMI user@host$ qsub -l instance_ami = <ami_id> -l base_os = rhel7 myscript.sh Scale-Out Computing on AWS AMI requirements When you use a custom AMI, just make sure that your AMI does not use /apps, /scratch or /data partitions as Scale-Out Computing on AWS will need to use these locations during the deployment. Read this page for AMI creation best practices Budgets and Cost Management \u00b6 You can review your HPC costs filtered by user/team/project/queue very easily using AWS Cost Explorer. Scale-Out Computing on AWS also supports AWS Budget and let you create budgets assigned to user/team/project or queue. To prevent over-spend, Scale-Out Computing on AWS includes hooks to restrict job submission when customer-defined budget has expired. Lastly, Scale-Out Computing on AWS let you create queue ACLs or instance restriction at a queue level. Refer to this link for all best practices in order to control your HPC cost on AWS and prevent overspend . Detailed Cluster Analytics \u00b6 Scale-Out Computing on AWS includes ElasticSearch and automatically ingest job and hosts data in real-time for accurate visualization of your cluster activity. Don't know where to start? Scale-Out Computing on AWS includes dashboard examples if you are not familiar with ElasticSearch or Kibana. 100% Customizable \u00b6 Scale-Out Computing on AWS is built entirely on top of AWS and can be customized by users as needed. Most of the logic is based of CloudFormation templates, shell scripts and python code. More importantly, the entire Scale-Out Computing on AWS codebase is open-source and available on Github . Persistent and Unlimited Storage \u00b6 Scale-Out Computing on AWS includes two unlimited EFS storage (/apps and /data). Customers also have the ability to deploy high-speed SSD EBS disks or FSx for Lustre as scratch location on their compute nodes. Refer to this page to learn more about the various storage options offered by Scale-Out Computing on AWS Centralized user-management \u00b6 Customers can create unlimited LDAP users and groups . By default Scale-Out Computing on AWS includes a default LDAP account provisioned during installation as well as a \"Sudoers\" LDAP group which manage SUDO permission on the cluster. Automatic backup \u00b6 Scale-Out Computing on AWS automatically backup your data with no additional effort required on your side. Support for network licenses \u00b6 Scale-Out Computing on AWS includes a FlexLM-enabled script which calculate the number of licenses for a given features and only start the job/provision the capacity when enough licenses are available. Automatic Errors Handling \u00b6 Scale-Out Computing on AWS performs various dry run checks before provisioning the capacity. However, it may happen than AWS can't fullfill all requests (eg: need 5 instances but only 3 can be provisioned due to capacity shortage within a placement group). In this case, Scale-Out Computing on AWS will try to provision the capacity for 30 minutes. After 30 minutes, and if the capacity is still not available, Scale-Out Computing on AWS will automatically reset the request and try to provision capacity in a different availability zone. Web UI \u00b6 Scale-Out Computing on AWS includes a simple web ui designed to simplify user interactions such as: Start/Stop DCV sessions in 1 click Download private key in both PEM or PPK format Check the queue status in real-time Add/Remove LDAP users Access the analytic dashboard Custom fair-share \u00b6 Each user is given a score which vary based on: Number of job in the queue Time each job is queued Priority of each job Type of instance Job that belong to the user with the highest score will start next And more ... \u00b6 Refer to the various sections (tutorial/security/analytics ...) to learn more about this solution DCV is a remote visualization technology that enables users to easily and securely connect to graphic-intensive 3D applications hosted on a remote high-performance server.* \u21a9","title":"What is Scale-Out Computing on AWS ?"},{"location":"#easy-installation","text":"Installation of your Scale-Out Computing on AWS cluster is fully automated and managed by CloudFormation Did you know? You can have multiple Scale-Out Computing on AWS clusters on the same AWS account Scale-Out Computing on AWS comes with a list of unique tags, making resource tracking easy for AWS Administrators","title":"Easy installation"},{"location":"#access-your-cluster-in-1-click","text":"You can access your Scale-Out Computing on AWS cluster either using DCV (Desktop Cloud Visualization) 1 or through SSH.","title":"Access your cluster in 1 click"},{"location":"#simple-job-submission","text":"Scale-Out Computing on AWS supports a list of parameters designed to simplify your job submission on AWS . Advanced users can either manually choose compute/storage/network configuration for their job or simply ignore these parameters and let Scale-Out Computing on AWS picks the most optimal hardware (defined by the HPC administrator) # Advanced Configuration user@host$ qsub -l instance_type = c5n.18xlarge \\ -l instance_ami = ami-123abcde -l nodes = 2 -l scratch_size = 300 -l efa_support = true -l spot_price = 1 .55 myscript.sh # Basic Configuration user@host$ qsub myscript.sh Info Check our Web-Based utility to generate you submission command Refer to this page for tutorial and examples Refer to this page to list all supported parameters","title":"Simple Job Submission"},{"location":"#os-agnostic-and-support-for-custom-ami","text":"Customers can integrate their Centos7/Rhel7/AmazonLinux2 AMI automatically by simply using -l instance_ami=<ami_id> at job submission. There is no limitation in term of AMI numbers (you can have 10 jobs running simultaneously using 10 different AMIs). SOCA supports heterogeneous environment, so you can have concurrent jobs running different operating system on the same cluster. AMI using OS different than the scheduler In case your AMI is different than your scheduler host, you can specify the OS manually to ensure packages will be installed based on the node distribution. In this example, we assume your Scale-Out Computing on AWS deployment was done using AmazonLinux2, but you want to submit a job on your personal RHEL7 AMI user@host$ qsub -l instance_ami = <ami_id> -l base_os = rhel7 myscript.sh Scale-Out Computing on AWS AMI requirements When you use a custom AMI, just make sure that your AMI does not use /apps, /scratch or /data partitions as Scale-Out Computing on AWS will need to use these locations during the deployment. Read this page for AMI creation best practices","title":"OS agnostic and support for custom AMI"},{"location":"#budgets-and-cost-management","text":"You can review your HPC costs filtered by user/team/project/queue very easily using AWS Cost Explorer. Scale-Out Computing on AWS also supports AWS Budget and let you create budgets assigned to user/team/project or queue. To prevent over-spend, Scale-Out Computing on AWS includes hooks to restrict job submission when customer-defined budget has expired. Lastly, Scale-Out Computing on AWS let you create queue ACLs or instance restriction at a queue level. Refer to this link for all best practices in order to control your HPC cost on AWS and prevent overspend .","title":"Budgets and Cost Management"},{"location":"#detailed-cluster-analytics","text":"Scale-Out Computing on AWS includes ElasticSearch and automatically ingest job and hosts data in real-time for accurate visualization of your cluster activity. Don't know where to start? Scale-Out Computing on AWS includes dashboard examples if you are not familiar with ElasticSearch or Kibana.","title":"Detailed Cluster Analytics"},{"location":"#100-customizable","text":"Scale-Out Computing on AWS is built entirely on top of AWS and can be customized by users as needed. Most of the logic is based of CloudFormation templates, shell scripts and python code. More importantly, the entire Scale-Out Computing on AWS codebase is open-source and available on Github .","title":"100% Customizable"},{"location":"#persistent-and-unlimited-storage","text":"Scale-Out Computing on AWS includes two unlimited EFS storage (/apps and /data). Customers also have the ability to deploy high-speed SSD EBS disks or FSx for Lustre as scratch location on their compute nodes. Refer to this page to learn more about the various storage options offered by Scale-Out Computing on AWS","title":"Persistent and Unlimited Storage"},{"location":"#centralized-user-management","text":"Customers can create unlimited LDAP users and groups . By default Scale-Out Computing on AWS includes a default LDAP account provisioned during installation as well as a \"Sudoers\" LDAP group which manage SUDO permission on the cluster.","title":"Centralized user-management"},{"location":"#automatic-backup","text":"Scale-Out Computing on AWS automatically backup your data with no additional effort required on your side.","title":"Automatic backup"},{"location":"#support-for-network-licenses","text":"Scale-Out Computing on AWS includes a FlexLM-enabled script which calculate the number of licenses for a given features and only start the job/provision the capacity when enough licenses are available.","title":"Support for network licenses"},{"location":"#automatic-errors-handling","text":"Scale-Out Computing on AWS performs various dry run checks before provisioning the capacity. However, it may happen than AWS can't fullfill all requests (eg: need 5 instances but only 3 can be provisioned due to capacity shortage within a placement group). In this case, Scale-Out Computing on AWS will try to provision the capacity for 30 minutes. After 30 minutes, and if the capacity is still not available, Scale-Out Computing on AWS will automatically reset the request and try to provision capacity in a different availability zone.","title":"Automatic Errors Handling"},{"location":"#web-ui","text":"Scale-Out Computing on AWS includes a simple web ui designed to simplify user interactions such as: Start/Stop DCV sessions in 1 click Download private key in both PEM or PPK format Check the queue status in real-time Add/Remove LDAP users Access the analytic dashboard","title":"Web UI"},{"location":"#custom-fair-share","text":"Each user is given a score which vary based on: Number of job in the queue Time each job is queued Priority of each job Type of instance Job that belong to the user with the highest score will start next","title":"Custom fair-share"},{"location":"#and-more","text":"Refer to the various sections (tutorial/security/analytics ...) to learn more about this solution DCV is a remote visualization technology that enables users to easily and securely connect to graphic-intensive 3D applications hosted on a remote high-performance server.* \u21a9","title":"And more ..."},{"location":"access-soca-cluster/","text":"Info Backend storage on Scale-Out Computing on AWS is persistent. You will have access to the same filesystem ($HOME, /data and /apps) whether you access your cluster using SSH, Web Remote Desktop or Native Remote Desktop SSH access \u00b6 To access your Scale-Out Computing on AWS cluster using SSH protocol, simply click \"SSH Access\" on the left sidebar and follow the instructions. Scale-Out Computing on AWS will let you download your private key either in PEM or PPK format. SSH to an instance in a Private Subnet If you need to access an instance that is in a Private (non-routable) Subnet, you can use ssh-agent to do this: $ ssh-add -K ~/Keys/my_key_region.pem Identity added: /Users/username/Keys/my_key_region.pem ( /Users/username/Keys/my_key_region.pem ) $ ssh-add -L <you should see your ssh key here> Now use -A with ssh and this will forward the key with your ssh login: $ ssh -A -i ~/Keys/my_key_region.pem centos@111.222.333.444 Now that you have your key forwarded, you can login to an instance that is in the Private Subnet: $ ssh <USERNAME>@<PRIVATE_IP> Graphical access using DCV \u00b6 To access your Scale-Out Computing on AWS cluster using a full remote desktop experience, click \"Graphical Access\" on the left sidebar. By default you are authorized to have 4 sessions (EC2 instances). Session Validity \u00b6 You can choose how long your session will be valid. This parameter can be customized as needed Session type \u00b6 You can choose the type of session you want to deploy, depending your needs. This parameter can be customized as needed Access your session \u00b6 After you click \"Launch my session\", a new \"desktop\" job is sent to the queue. Scale-Out Computing on AWS will then provision the capacity and install all required packages including Gnome. You will see an informational message asking you to wait up to 20 minutes before being able to access your remote desktop. Once your session is ready, the message will automatically be updated with the connection information You can access your session directly on your browser You can also download the NICE DCV Native Clients for Mac / Linux and Windows and access your session directly through them","title":"How to access Scale-Out Computing on AWS"},{"location":"access-soca-cluster/#ssh-access","text":"To access your Scale-Out Computing on AWS cluster using SSH protocol, simply click \"SSH Access\" on the left sidebar and follow the instructions. Scale-Out Computing on AWS will let you download your private key either in PEM or PPK format. SSH to an instance in a Private Subnet If you need to access an instance that is in a Private (non-routable) Subnet, you can use ssh-agent to do this: $ ssh-add -K ~/Keys/my_key_region.pem Identity added: /Users/username/Keys/my_key_region.pem ( /Users/username/Keys/my_key_region.pem ) $ ssh-add -L <you should see your ssh key here> Now use -A with ssh and this will forward the key with your ssh login: $ ssh -A -i ~/Keys/my_key_region.pem centos@111.222.333.444 Now that you have your key forwarded, you can login to an instance that is in the Private Subnet: $ ssh <USERNAME>@<PRIVATE_IP>","title":"SSH access"},{"location":"access-soca-cluster/#graphical-access-using-dcv","text":"To access your Scale-Out Computing on AWS cluster using a full remote desktop experience, click \"Graphical Access\" on the left sidebar. By default you are authorized to have 4 sessions (EC2 instances).","title":"Graphical access using DCV"},{"location":"access-soca-cluster/#session-validity","text":"You can choose how long your session will be valid. This parameter can be customized as needed","title":"Session Validity"},{"location":"access-soca-cluster/#session-type","text":"You can choose the type of session you want to deploy, depending your needs. This parameter can be customized as needed","title":"Session type"},{"location":"access-soca-cluster/#access-your-session","text":"After you click \"Launch my session\", a new \"desktop\" job is sent to the queue. Scale-Out Computing on AWS will then provision the capacity and install all required packages including Gnome. You will see an informational message asking you to wait up to 20 minutes before being able to access your remote desktop. Once your session is ready, the message will automatically be updated with the connection information You can access your session directly on your browser You can also download the NICE DCV Native Clients for Mac / Linux and Windows and access your session directly through them","title":"Access your session"},{"location":"install-soca-cluster/","text":"1-Click installer \u00b6 You can use the 1-Click installer for quick proof-of-concept (PoC), demo and/or development work . This installer is hosted on an AWS controlled S3 bucket and customization is limited, so we recommend downloading building your own SOCA (see below) for your production. Always refers to the Github repository for the latest SOCA version. Download Scale-Out Computing on AWS \u00b6 Scale-Out Computing on AWS is open-source and available on Github ( https://github.com/awslabs/scale-out-computing-on-aws ). To get started, simply clone the repository: # Clone using HTTPS user@host: git clone https://github.com/awslabs/scale-out-computing-on-aws . # Clone using SSH user@host: git clone git@github.com:awslabs/scale-out-computing-on-aws.git . Build your release \u00b6 Once you have cloned your repository, execute source/manual_build.py using either python2 or python3 user@host: python3 source/manual_build.py ====== Scale-Out Computing on AWS Build ====== > Generated unique ID for build: r6l1 > Creating temporary build folder ... > Copying required files ... > Creating archive for build id: r6l1 ====== Build COMPLETE ====== ====== Installation Instructions ====== 1 : Create or use an existing S3 bucket on your AWS account ( eg: 'mysocacluster' ) 2 : Drag & Drop source/dist/r6l1 to your S3 bucket ( eg: ' mysocacluster/dist/r6l1 ) 3 : Launch CloudFormation and use scale-out-computing-on-aws.template as base template 4 : Enter your cluster information. Press Enter key to close .. This command create a build ( r6l1 in this example) under source/dist/<build_id> Upload to S3 \u00b6 Go to your Amazon S3 console and click \"Create Bucket\" Choose a name and a region then click \"Create\" Avoid un-necessary charge It's recommended to create your bucket in the same region as your are planning to use Scale-Out Computing on AWS to avoid Cross-Regions charge ( See Data Transfer ) Once your bucket is created, select it and click \"Upload\". Simply drag and drop your build folder ( r6l1 in this example) to upload the content of the folder to S3. Info You can use the same bucket to host multiple Scale-Out Computing on AWS clusters Locate the install template \u00b6 On your S3 bucket, click on the folder you just uploaded. Your install template is located under <S3_BUCKET_NAME>/<BUILD_ID>/scale-out-computing-on-aws.template . Click on the object to retrieve the \"Object URL\" Want to use your existing AWS resources? Refer to install-with-existing-resources.template if you want to use Scale-Out Computing on AWS with your existing resources. Check out the web installer to verify your setup Install Scale-Out Computing on AWS \u00b6 Open CloudFormation console and select \"Create Stack\". Copy the URL of your install template and click \"Next\". Requirements No uppercase in stack name Stack name is limited to 20 characters maximum (note: we automatically add soca- prefix) Not supported on regions with less than 3 AZs (Canada / Northern California) If you hit any issue during the installation, refer to the 'CREATE_FAILED' component and find the root cause by referring at \"Physical ID\" Under stack details, choose the stack name (do not use uppercase or it will break your ElasticSearch cluster). Install Parameters: Specify your S3 bucket you have uploaded your build ( my-soca-hpc-test in this example) as well as the name of your build ( r6l1 in this example). If you haven't uploaded your build on your S3 root level, make sure you specify the entire file hierarchy. Environment Parameters: Choose your Linux Distribution, instance type for your master host, VPC CIDR, your IP which will be whitelisted for port 22, 80 and 443 as well as the root SSH keypair you want to use LDAP Parameters: Create a default LDAP user Disable Rollback on Failure if needed If you face any challenge during the installation and need to do some troubleshooting, it's recommended to disable \"Rollback On Failure\" (under Advanced section) Click Next two times and make sure to check \"Capabilities\" section. One done simply click \"Create Stack\". The installation procedure will take about 45 minutes. Post Install Verifications \u00b6 Wait for CloudFormation stacks to be \"CREATE_COMPLETE\", then select your base stack and click \"Outputs\" Output tabs give you information about the SSH IP for the master, link to the web interface or ElasticSearch. Even though Cloudformation resources are created, your environment might not be completely ready. To confirm whether or not Scale-Out Computing on AWS is ready, try to SSH to the scheduler IP. If your Scale-Out Computing on AWS cluster is not ready, your SSH will be rejected as shown below: 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> ************* Scale-Out Computing on AWS FIRST TIME CONFIGURATION ************* Hold on, cluster is not ready yet. Please wait ~30 minutes as Scale-Out Computing on AWS is being installed. Once cluster is ready to use, this message will be replaced automatically and you will be able to SSH. ********************************************************* Connection Closed. If your Scale-Out Computing on AWS cluster is ready, your SSH session will be accepted. 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> Last login: Mon Oct 7 21 :37:21 2019 from <IP> _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-cluster-v1 > source /etc/environment to load Scale-Out Computing on AWS paths [ ec2-user@ip-20-0-5-212 ~ ] $ At this point, you will be able to access the web interface and log in with the default LDAP user you specified at launch creation What if SSH port (22) is blocked by your IT? \u00b6 Scale-Out Computing on AWS supports AWS Session Manager in case you corporate firewall is blocking SSH port (22). SSM let you open a secure shell on your EC2 instance through a secure web-based session. First, access your AWS EC2 Console and select your Scheduler instance, then click \"Connect\" button Select \"Session Manager\" and click Connect You now have access to a secure shell directly within your browser Enable Termination Protection \u00b6 This step is optional yet higly recommended. AWS CloudFormation allows you to protect a stack from being accidently deleted. If you attempt to delete a stack with termination protection enabled, the deletion fails and the stack, including its status, will remain unchanged. To enable \"Termination Protect\" select your Primary template and click \"Stack Action\" button then \"Edit Termination Protection\". Choose \"Enabled\" and click Save. Choose \"Disabled\" if you want to be able to delete the stack again. Important Services \u00b6 Note All services on SOCA will automatically restart if you restart your scheduler instance Run the following command (as root) if you want to restart any service: Scheduler: service pbs start SSSD: service sssd start OpenLDAP: service openldap start Web UI /apps/soca/<CLUSTER_ID>/cluster_web_ui/socawebui.sh start NFS partitions mount -a (mount configuration is available on /etc/fstab ) Operational Metrics \u00b6 This solution includes an option to send anonymous operational metrics to AWS. We use this data to better understand how customers use this solution and related services and products. Note that AWS will own the data gathered via this survey. Data collection will be subject to the AWS Privacy Policy . To opt out of this feature, modify the /apps/soca/<CLUSTER_ID>/cluster_manager/cloudformation_builder and set allow_anonymous_data_collection variable to False When enabled, the following information is collected and sent to AWS: - Solution ID: The AWS solution identifier - Base Operating System: The operating system selected for the solution deployment - Unique ID (UUID): Randomly generated, unique identifier for each solution deployment - Timestamp: Data-collection timestamp - Instance Data: Type or count of the state and type of instances that are provided for by the Amazon EC2 scheduler instance for each job in each AWS Region - Keep Forever: If instances are running when no job is running - EFA Support: If EFA support was selected - Spot Support: If Spot support was invoked for new auto-scaling stacks - Stack Creation Version: The version of the stack that is created or deleted - Status: The status of the stack (stack_created or stack_deleted) - Scratch Disk Size: The size of the scratch disk selected for each solution deployment - Region: The region where the stack is deployed - FSxLustre: If the job is using FSx for Lustre What's next ? \u00b6 Learn how to access your cluster , how to submit your first job or even how to change your Scale-Out Computing on AWS DNS to match your personal domain name.","title":"Install your Scale-Out Computing on AWS cluster"},{"location":"install-soca-cluster/#1-click-installer","text":"You can use the 1-Click installer for quick proof-of-concept (PoC), demo and/or development work . This installer is hosted on an AWS controlled S3 bucket and customization is limited, so we recommend downloading building your own SOCA (see below) for your production. Always refers to the Github repository for the latest SOCA version.","title":"1-Click installer"},{"location":"install-soca-cluster/#download-scale-out-computing-on-aws","text":"Scale-Out Computing on AWS is open-source and available on Github ( https://github.com/awslabs/scale-out-computing-on-aws ). To get started, simply clone the repository: # Clone using HTTPS user@host: git clone https://github.com/awslabs/scale-out-computing-on-aws . # Clone using SSH user@host: git clone git@github.com:awslabs/scale-out-computing-on-aws.git .","title":"Download Scale-Out Computing on AWS"},{"location":"install-soca-cluster/#build-your-release","text":"Once you have cloned your repository, execute source/manual_build.py using either python2 or python3 user@host: python3 source/manual_build.py ====== Scale-Out Computing on AWS Build ====== > Generated unique ID for build: r6l1 > Creating temporary build folder ... > Copying required files ... > Creating archive for build id: r6l1 ====== Build COMPLETE ====== ====== Installation Instructions ====== 1 : Create or use an existing S3 bucket on your AWS account ( eg: 'mysocacluster' ) 2 : Drag & Drop source/dist/r6l1 to your S3 bucket ( eg: ' mysocacluster/dist/r6l1 ) 3 : Launch CloudFormation and use scale-out-computing-on-aws.template as base template 4 : Enter your cluster information. Press Enter key to close .. This command create a build ( r6l1 in this example) under source/dist/<build_id>","title":"Build your release"},{"location":"install-soca-cluster/#upload-to-s3","text":"Go to your Amazon S3 console and click \"Create Bucket\" Choose a name and a region then click \"Create\" Avoid un-necessary charge It's recommended to create your bucket in the same region as your are planning to use Scale-Out Computing on AWS to avoid Cross-Regions charge ( See Data Transfer ) Once your bucket is created, select it and click \"Upload\". Simply drag and drop your build folder ( r6l1 in this example) to upload the content of the folder to S3. Info You can use the same bucket to host multiple Scale-Out Computing on AWS clusters","title":"Upload to S3"},{"location":"install-soca-cluster/#locate-the-install-template","text":"On your S3 bucket, click on the folder you just uploaded. Your install template is located under <S3_BUCKET_NAME>/<BUILD_ID>/scale-out-computing-on-aws.template . Click on the object to retrieve the \"Object URL\" Want to use your existing AWS resources? Refer to install-with-existing-resources.template if you want to use Scale-Out Computing on AWS with your existing resources. Check out the web installer to verify your setup","title":"Locate the install template"},{"location":"install-soca-cluster/#install-scale-out-computing-on-aws","text":"Open CloudFormation console and select \"Create Stack\". Copy the URL of your install template and click \"Next\". Requirements No uppercase in stack name Stack name is limited to 20 characters maximum (note: we automatically add soca- prefix) Not supported on regions with less than 3 AZs (Canada / Northern California) If you hit any issue during the installation, refer to the 'CREATE_FAILED' component and find the root cause by referring at \"Physical ID\" Under stack details, choose the stack name (do not use uppercase or it will break your ElasticSearch cluster). Install Parameters: Specify your S3 bucket you have uploaded your build ( my-soca-hpc-test in this example) as well as the name of your build ( r6l1 in this example). If you haven't uploaded your build on your S3 root level, make sure you specify the entire file hierarchy. Environment Parameters: Choose your Linux Distribution, instance type for your master host, VPC CIDR, your IP which will be whitelisted for port 22, 80 and 443 as well as the root SSH keypair you want to use LDAP Parameters: Create a default LDAP user Disable Rollback on Failure if needed If you face any challenge during the installation and need to do some troubleshooting, it's recommended to disable \"Rollback On Failure\" (under Advanced section) Click Next two times and make sure to check \"Capabilities\" section. One done simply click \"Create Stack\". The installation procedure will take about 45 minutes.","title":"Install Scale-Out Computing on AWS"},{"location":"install-soca-cluster/#post-install-verifications","text":"Wait for CloudFormation stacks to be \"CREATE_COMPLETE\", then select your base stack and click \"Outputs\" Output tabs give you information about the SSH IP for the master, link to the web interface or ElasticSearch. Even though Cloudformation resources are created, your environment might not be completely ready. To confirm whether or not Scale-Out Computing on AWS is ready, try to SSH to the scheduler IP. If your Scale-Out Computing on AWS cluster is not ready, your SSH will be rejected as shown below: 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> ************* Scale-Out Computing on AWS FIRST TIME CONFIGURATION ************* Hold on, cluster is not ready yet. Please wait ~30 minutes as Scale-Out Computing on AWS is being installed. Once cluster is ready to use, this message will be replaced automatically and you will be able to SSH. ********************************************************* Connection Closed. If your Scale-Out Computing on AWS cluster is ready, your SSH session will be accepted. 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> Last login: Mon Oct 7 21 :37:21 2019 from <IP> _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-cluster-v1 > source /etc/environment to load Scale-Out Computing on AWS paths [ ec2-user@ip-20-0-5-212 ~ ] $ At this point, you will be able to access the web interface and log in with the default LDAP user you specified at launch creation","title":"Post Install Verifications"},{"location":"install-soca-cluster/#what-if-ssh-port-22-is-blocked-by-your-it","text":"Scale-Out Computing on AWS supports AWS Session Manager in case you corporate firewall is blocking SSH port (22). SSM let you open a secure shell on your EC2 instance through a secure web-based session. First, access your AWS EC2 Console and select your Scheduler instance, then click \"Connect\" button Select \"Session Manager\" and click Connect You now have access to a secure shell directly within your browser","title":"What if SSH port (22) is blocked by your IT?"},{"location":"install-soca-cluster/#enable-termination-protection","text":"This step is optional yet higly recommended. AWS CloudFormation allows you to protect a stack from being accidently deleted. If you attempt to delete a stack with termination protection enabled, the deletion fails and the stack, including its status, will remain unchanged. To enable \"Termination Protect\" select your Primary template and click \"Stack Action\" button then \"Edit Termination Protection\". Choose \"Enabled\" and click Save. Choose \"Disabled\" if you want to be able to delete the stack again.","title":"Enable Termination Protection"},{"location":"install-soca-cluster/#important-services","text":"Note All services on SOCA will automatically restart if you restart your scheduler instance Run the following command (as root) if you want to restart any service: Scheduler: service pbs start SSSD: service sssd start OpenLDAP: service openldap start Web UI /apps/soca/<CLUSTER_ID>/cluster_web_ui/socawebui.sh start NFS partitions mount -a (mount configuration is available on /etc/fstab )","title":"Important Services"},{"location":"install-soca-cluster/#operational-metrics","text":"This solution includes an option to send anonymous operational metrics to AWS. We use this data to better understand how customers use this solution and related services and products. Note that AWS will own the data gathered via this survey. Data collection will be subject to the AWS Privacy Policy . To opt out of this feature, modify the /apps/soca/<CLUSTER_ID>/cluster_manager/cloudformation_builder and set allow_anonymous_data_collection variable to False When enabled, the following information is collected and sent to AWS: - Solution ID: The AWS solution identifier - Base Operating System: The operating system selected for the solution deployment - Unique ID (UUID): Randomly generated, unique identifier for each solution deployment - Timestamp: Data-collection timestamp - Instance Data: Type or count of the state and type of instances that are provided for by the Amazon EC2 scheduler instance for each job in each AWS Region - Keep Forever: If instances are running when no job is running - EFA Support: If EFA support was selected - Spot Support: If Spot support was invoked for new auto-scaling stacks - Stack Creation Version: The version of the stack that is created or deleted - Status: The status of the stack (stack_created or stack_deleted) - Scratch Disk Size: The size of the scratch disk selected for each solution deployment - Region: The region where the stack is deployed - FSxLustre: If the job is using FSx for Lustre","title":"Operational Metrics"},{"location":"install-soca-cluster/#whats-next","text":"Learn how to access your cluster , how to submit your first job or even how to change your Scale-Out Computing on AWS DNS to match your personal domain name.","title":"What's next ?"},{"location":"job-configuration-generator/","text":"Automatic parameter selection You can manually specify parameters at job submission using the command below. If needed, all parameters can also be automatically configured at queue level . Job will use the default parameters configured for its queue unless the parameters are explicitly specified during submission ( job parameters override queue parameters ). Refer to this page for additional examples. * { box-sizing: border-box; } .input2 { padding: 12px; width: 85%; border: 1px solid #ccc; border-radius: 4px; resize: vertical; font-size: 15px; margin-top: 6px; } .container { border-radius: 5px; background-color: #f2f2f2; padding: 5px; } .col-25 { float: left; width: 25%; margin-top: 6px; } .col-75 { float: left; width: 75%; margin-top: 6px; } /* Responsive layout - when the screen is less than 600px wide, make the two columns stack on top of each other instead of next to each other */ @media screen and (max-width: 600px) { .col-25, .col-75, input[type=submit] { width: 100%; margin-top: 0; } } .md-content { margin-right: 0; } Your Command user@host: qsub {{qsub_instance_ami}} {{qsub_instance_type}} {{qsub_subnet_id}} {{qsub_spot_price}} {{qsub_efa_support}} {{qsub_placement_group}} {{qsub_root_size}} {{qsub_scratch_size}} {{qsub_scratch_iops}} {{qsub_fsx_lustre}} {{qsub_fsx_lustre_size}} {{qsub_ht_support}} {{qsub_spot_allocation_count}} {{qsub_spot_allocation_strategy}} {{qsub_nodes}} {{qsub_base_os}} {{qsub_keep_ebs}} myscript.sh Job Parameters Compute parameters: Documentation Must be a number greater than 0 Documentation Documentation Image name must start with \"ami-\" Documentation Must be centos7, rhel7 or amazonlinux2 {{base_os_error}} Documentation Subnet name must start with \"sub-\" Documentation Spot Price must be a float (eg 1.2) or auto (match OD price) Documentation Must be a number {{spot_allocation_error}} {{spot_allocation_error_price}} Documentation Must be either lowest-cost (default) or capacity-optimized {{spot_allocation_strategy_price}} Storage parameters: Documentation Root Size must be a number Documentation Scratch Size must be a number Documentation Provisioned IO/s must be a number Documentation Documentation Size must be a number Flags: I want to use EFA Documentation I do not want to use Placement Group (enabled by default) Documentation I want to enable HyperThreading (disabled by default) Documentation I want to retain my EBS disks (disabled by default) Documentation angular.module('myApp', ['ngMessages']) .controller('myCtrl', ['$scope', function($scope) { $scope.count = 0; $scope.myFunc = function() { if($scope.nodes){$scope.qsub_nodes = \"-l nodes=\" + $scope.nodes;}else{$scope.qsub_nodes = \"\";} if($scope.instance_ami){$scope.qsub_instance_ami = \"-l instance_ami=\" + $scope.instance_ami;}else{$scope.qsub_instance_ami = \"\";} if($scope.base_os){$scope.qsub_base_os = \"-l base_os=\" + $scope.base_os;}else{$scope.qsub_base_os = \"\";} if($scope.instance_type){$scope.qsub_instance_type = \"-l instance_type=\" + $scope.instance_type;}else{$scope.qsub_instance_type = \"\";} if($scope.subnet_id){$scope.qsub_subnet_id = \"-l subnet_id=\" + $scope.subnet_id;}else{$scope.qsub_subnet_id = \"\";} if($scope.spot_price){$scope.qsub_spot_price = \"-l spot_price=\" + $scope.spot_price;}else{$scope.qsub_spot_price= \"\";} if($scope.root_size){$scope.qsub_root_size= \"-l root_size=\" + $scope.root_size;}else{$scope.qsub_root_size= \"\";} if($scope.scratch_size){$scope.qsub_scratch_size = \"-l scratch_size=\" + $scope.scratch_size;}else{$scope.qsub_scratch_size= \"\";} if($scope.scratch_iops){$scope.qsub_scratch_iops= \"-l scratch_iops=\" + $scope.scratch_iops;}else{$scope.qsub_scratch_iops= \"\";} if($scope.spot_allocation_count){$scope.qsub_spot_allocation_count= \"-l spot_allocation_count=\" + $scope.spot_allocation_count;}else{$scope.qsub_spot_allocation_count= \"\";} if($scope.spot_allocation_strategy){$scope.qsub_spot_allocation_strategy= \"-l spot_allocation_strategy=\" + $scope.spot_allocation_strategy;}else{$scope.qsub_spot_allocation_strategy= \"\";} if($scope.keep_ebs){$scope.qsub_keep_ebs = \"-l keep_ebs=True\";}else{$scope.qsub_keep_ebs= \"\";} if($scope.efa_support){$scope.qsub_efa_support = \"-l efa_support=True\";}else{$scope.qsub_efa_support= \"\";} if($scope.placement_group){$scope.qsub_placement_group = \"-l placement_group=False\";}else{$scope.qsub_placement_group= \"\";} if($scope.ht_support){$scope.qsub_ht_support = \"-l ht_support=True\";}else{$scope.qsub_ht_support= \"\";} if($scope.fsx_lustre){$scope.qsub_fsx_lustre = \"-l fsx_lustre=\" + $scope.fsx_lustre;}else{$scope.qsub_fsx_lustre = \"\";} if($scope.fsx_lustre_size){$scope.qsub_fsx_lustre_size = \"-l fsx_lustre_size=\" + $scope.fsx_lustre_size;}else{$scope.qsub_fsx_lustre_size = \"\";} if (!$scope.nodes){$scope.nodes_count=0;}else{$scope.nodes_count=$scope.nodes} if (+$scope.spot_allocation_count >= +$scope.nodes_count) { $scope.spot_allocation_error = \"Allocated spots must be lower than the number of nodes provisioned for your job\"; $scope.qsub_spot_allocation_count= \"\"; } else { $scope.spot_allocation_error = \"\"; } if ($scope.spot_allocation_count && !$scope.spot_price) { $scope.spot_allocation_error_price = \"Spot Price must be specified\"; $scope.qsub_spot_allocation_count= \"\"; } else { $scope.spot_allocation_error_price = \"\"; } if ($scope.spot_allocation_strategy && !$scope.spot_price) { $scope.spot_allocation_strategy_price = \"Spot Price must be specified\"; $scope.qsub_spot_allocation_strategy= \"\"; } else { $scope.spot_allocation_strategy_price = \"\"; } if ($scope.base_os && !$scope.instance_ami) { $scope.base_os_error = \"No need to specify base_os if you don't use a custom AMI\"; $scope.qsub_base_os= \"\"; } else { $scope.base_os_error = \"\"; } }; }]);","title":"Job Submission Generator"},{"location":"FAQ/knowledge-base-all-errors/","text":"This page only list errors related to CloudFormation. Submit a ticket if your error is not listed here CloudFormation troubleshooting We recommend to disable \"RollBack on Failure\" to simplify CloudFormation debugging/troubleshooting Errors during Stack Creation \u00b6 C1: The following resource(s) failed to create: CheckSOCAPreRequisite. \u00b6 - Stack: Primary Template - Event: Failed to create resource. See the details in CloudWatch Log Stream: 2020/03/17/[$LATEST]xxxxx - Resolution: Refer to the Physical ID message. It's could be because you used uppercase in the stack name, have a stack name longer than 20 chars or use a non-supported AWS region Errors during Stack Deletion \u00b6 D1: Cannot delete entity, must delete policies first. \u00b6 - Stack: Security - Event: Cannot delete entity, must delete policies first. (Service: AmazonIdentityManagement; Status Code: 409; Error Code: DeleteConflict; Request ID: x) - Resolution: You have added extra policies to the IAM roles created by SOCA. Remove the policy or delete the role entirely D2: Backup vault cannot be deleted (contains recovery points) \u00b6 - Stack: Configuration - Event: Backup vault cannot be deleted (contains 3 recovery points) (Service: AWSBackup; Status Code: 400; Error Code: InvalidRequestException; Request ID: x) - Resolution: You must manually remove the recovery points from your SOCA Backup Vault Errors Post Deployment \u00b6 P1: Why do I see \"502 Bad Gateway\" when I try to log into the SOCA web UI? \u00b6 - Resolution: This error indicates that they web server is not running on the scheduler server. If you just installed SOCA, wait until you can log into the scheduler server before trying to access the management web UI.","title":"Knowledge Base - List all Errors"},{"location":"FAQ/knowledge-base-all-errors/#errors-during-stack-creation","text":"","title":"Errors during Stack Creation"},{"location":"FAQ/knowledge-base-all-errors/#c1-the-following-resources-failed-to-create-checksocaprerequisite","text":"- Stack: Primary Template - Event: Failed to create resource. See the details in CloudWatch Log Stream: 2020/03/17/[$LATEST]xxxxx - Resolution: Refer to the Physical ID message. It's could be because you used uppercase in the stack name, have a stack name longer than 20 chars or use a non-supported AWS region","title":"C1: The following resource(s) failed to create: CheckSOCAPreRequisite."},{"location":"FAQ/knowledge-base-all-errors/#errors-during-stack-deletion","text":"","title":"Errors during Stack Deletion"},{"location":"FAQ/knowledge-base-all-errors/#d1-cannot-delete-entity-must-delete-policies-first","text":"- Stack: Security - Event: Cannot delete entity, must delete policies first. (Service: AmazonIdentityManagement; Status Code: 409; Error Code: DeleteConflict; Request ID: x) - Resolution: You have added extra policies to the IAM roles created by SOCA. Remove the policy or delete the role entirely","title":"D1: Cannot delete entity, must delete policies first."},{"location":"FAQ/knowledge-base-all-errors/#d2-backup-vault-cannot-be-deleted-contains-recovery-points","text":"- Stack: Configuration - Event: Backup vault cannot be deleted (contains 3 recovery points) (Service: AWSBackup; Status Code: 400; Error Code: InvalidRequestException; Request ID: x) - Resolution: You must manually remove the recovery points from your SOCA Backup Vault","title":"D2: Backup vault cannot be deleted (contains  recovery points)"},{"location":"FAQ/knowledge-base-all-errors/#errors-post-deployment","text":"","title":"Errors Post Deployment"},{"location":"FAQ/knowledge-base-all-errors/#p1-why-do-i-see-502-bad-gateway-when-i-try-to-log-into-the-soca-web-ui","text":"- Resolution: This error indicates that they web server is not running on the scheduler server. If you just installed SOCA, wait until you can log into the scheduler server before trying to access the management web UI.","title":"P1: Why do I see \"502 Bad Gateway\" when I try to log into the SOCA web UI?"},{"location":"analytics/build-kibana-dashboards/","text":"On your Kibana cluster , click \"Visualize\" to create a new visualization. Below are some example to help you get started Note: For each dashboard, you can get detailed data at user, queue, job or project level by simply using the \"Filters\" section Money spent by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand Jobs per user \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: user.keyword Order By: metric: Count Jobs per user split by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count Most active projects \u00b6 Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count If needed, you can filter by project name (note: this type of filtering can be applied to all type of dashboard) Instance type launched by user \u00b6 Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count Number of nodes in the cluster \u00b6 Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute Detailed information per user \u00b6 Configuration Select \"Datatables\" and \"jobs\" index Metric (Metrics): Aggregation: Count Split Rows (Buckets): Aggregation: Term Field: user.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: instance_type_used.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: price_ondemand.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: job_name.keyword Order By: metric: Count Find the price for a given simulation \u00b6 Each job comes with price_ondemand and price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Create your own analytics dashboard"},{"location":"analytics/build-kibana-dashboards/#money-spent-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand","title":"Money spent by instance type"},{"location":"analytics/build-kibana-dashboards/#jobs-per-user","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: user.keyword Order By: metric: Count","title":"Jobs per user"},{"location":"analytics/build-kibana-dashboards/#jobs-per-user-split-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count","title":"Jobs per user split by instance type"},{"location":"analytics/build-kibana-dashboards/#most-active-projects","text":"Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count If needed, you can filter by project name (note: this type of filtering can be applied to all type of dashboard)","title":"Most active projects"},{"location":"analytics/build-kibana-dashboards/#instance-type-launched-by-user","text":"Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count","title":"Instance type launched by user"},{"location":"analytics/build-kibana-dashboards/#number-of-nodes-in-the-cluster","text":"Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute","title":"Number of nodes in the cluster"},{"location":"analytics/build-kibana-dashboards/#detailed-information-per-user","text":"Configuration Select \"Datatables\" and \"jobs\" index Metric (Metrics): Aggregation: Count Split Rows (Buckets): Aggregation: Term Field: user.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: instance_type_used.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: price_ondemand.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: job_name.keyword Order By: metric: Count","title":"Detailed information per user"},{"location":"analytics/build-kibana-dashboards/#find-the-price-for-a-given-simulation","text":"Each job comes with price_ondemand and price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Find the price for a given simulation"},{"location":"analytics/monitor-cluster-activity/","text":"Dashboard URL \u00b6 Open your AWS console and navigate to CloudFormation. Select your parent Stack, click Output, and retrieve \"WebUserInterface\" Create Indexes \u00b6 Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data Index Information \u00b6 Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time ____ Examples \u00b6 Cluster Node \u00b6 Job Metadata \u00b6 Troubleshooting access permission \u00b6 Access to ElasticSearch is restricted to the IP you have specified during the installation. If your IP change for any reason, you won't be able to access the analytics dashboard and will get the following error message: { \"Message\" : \"User: anonymous is not authorized to perform: es:ESHttpGet\" } To solve this issue, log in to AWS Console and go to ElasticSearch Service dashboard. Select your ElasticSearch cluster and click \"Modify Access Policy\" Finally, simply add your new IP under the \"Condition\" block, then click Submit Please note it may take up to 5 minutes for your IP to be whitelisted Create your own dashboard \u00b6","title":"Monitor your cluster and job activity"},{"location":"analytics/monitor-cluster-activity/#dashboard-url","text":"Open your AWS console and navigate to CloudFormation. Select your parent Stack, click Output, and retrieve \"WebUserInterface\"","title":"Dashboard URL"},{"location":"analytics/monitor-cluster-activity/#create-indexes","text":"Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data","title":"Create Indexes"},{"location":"analytics/monitor-cluster-activity/#index-information","text":"Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time ____","title":"Index Information"},{"location":"analytics/monitor-cluster-activity/#examples","text":"","title":"Examples"},{"location":"analytics/monitor-cluster-activity/#cluster-node","text":"","title":"Cluster Node"},{"location":"analytics/monitor-cluster-activity/#job-metadata","text":"","title":"Job Metadata"},{"location":"analytics/monitor-cluster-activity/#troubleshooting-access-permission","text":"Access to ElasticSearch is restricted to the IP you have specified during the installation. If your IP change for any reason, you won't be able to access the analytics dashboard and will get the following error message: { \"Message\" : \"User: anonymous is not authorized to perform: es:ESHttpGet\" } To solve this issue, log in to AWS Console and go to ElasticSearch Service dashboard. Select your ElasticSearch cluster and click \"Modify Access Policy\" Finally, simply add your new IP under the \"Condition\" block, then click Submit Please note it may take up to 5 minutes for your IP to be whitelisted","title":"Troubleshooting access permission"},{"location":"analytics/monitor-cluster-activity/#create-your-own-dashboard","text":"","title":"Create your own dashboard"},{"location":"analytics/prevent-overspend-hpc-cost-on-aws-soca/","text":"Scale-Out Computing on AWS offers multiple ways to make sure you will stay within budget while running your HPC workloads on AWS Limit who can submit jobs \u00b6 Only allow specific individual users or/and LDAP groups to submit jobs. Refer to this page for examples and documentation Limit what type of EC2 instance can be provisioned \u00b6 Control what type of EC2 instances can be provisioned for any given queue. Refer to this page for examples and documentation Accelerated Computing Instances Unless required for your workloads, it's recommended to exclude \"p2\", \"p3\", \"g2\", \"g3\", \"p3dn\" or other GPU instances type. Create a budget \u00b6 Creating an AWS Budget will ensure jobs can't be submitted if the budget allocated to the team/queue/project has exceeded the authorized amount. Refer to this page for examples and documentation Review your HPC cost in a central dashboard \u00b6 Stay on top of your AWS costs in real time. Quickly visualize your overall usage and find answers to your most common questions: Who are my top users? How much money did we spend for Project A? How much storage did we use for Queue B? Where my money is going (storage, compute ...) Etc ... Refer to this page for examples and documentation Best practices \u00b6 Assuming you are on-boarding a new team, here are our recommend best practices: 1 - Create LDAP account for all users 2 - Create LDAP group for the team. Add all users to the group 3 - Create a new queue 4 - Limit the queue to the LDAP group you just created 5 - Limit the type of EC2 instances your users can provision 6 - If needed, configure restricted parameters 7 - Create a Budget to make sure the new team won't spend more than what's authorized","title":"Keep control of your HPC cost on AWS"},{"location":"analytics/prevent-overspend-hpc-cost-on-aws-soca/#limit-who-can-submit-jobs","text":"Only allow specific individual users or/and LDAP groups to submit jobs. Refer to this page for examples and documentation","title":"Limit who can submit jobs"},{"location":"analytics/prevent-overspend-hpc-cost-on-aws-soca/#limit-what-type-of-ec2-instance-can-be-provisioned","text":"Control what type of EC2 instances can be provisioned for any given queue. Refer to this page for examples and documentation Accelerated Computing Instances Unless required for your workloads, it's recommended to exclude \"p2\", \"p3\", \"g2\", \"g3\", \"p3dn\" or other GPU instances type.","title":"Limit what type of EC2 instance can be provisioned"},{"location":"analytics/prevent-overspend-hpc-cost-on-aws-soca/#create-a-budget","text":"Creating an AWS Budget will ensure jobs can't be submitted if the budget allocated to the team/queue/project has exceeded the authorized amount. Refer to this page for examples and documentation","title":"Create a budget"},{"location":"analytics/prevent-overspend-hpc-cost-on-aws-soca/#review-your-hpc-cost-in-a-central-dashboard","text":"Stay on top of your AWS costs in real time. Quickly visualize your overall usage and find answers to your most common questions: Who are my top users? How much money did we spend for Project A? How much storage did we use for Queue B? Where my money is going (storage, compute ...) Etc ... Refer to this page for examples and documentation","title":"Review your HPC cost in a central dashboard"},{"location":"analytics/prevent-overspend-hpc-cost-on-aws-soca/#best-practices","text":"Assuming you are on-boarding a new team, here are our recommend best practices: 1 - Create LDAP account for all users 2 - Create LDAP group for the team. Add all users to the group 3 - Create a new queue 4 - Limit the queue to the LDAP group you just created 5 - Limit the type of EC2 instances your users can provision 6 - If needed, configure restricted parameters 7 - Create a Budget to make sure the new team won't spend more than what's authorized","title":"Best practices"},{"location":"analytics/review-hpc-costs/","text":"AWS Cost Explorer \u00b6 Any EC2 resource launched by Scale-Out Computing on AWS comes with an extensive list of EC2 tags that can be used to get detailed information about your cluster usage. List includes (but not limited to): Project Name Job Owner Job Name Job Queue Job Id These are the default tags and you can add your own tags if needed. Step1: Enable Cost Allocation Tags \u00b6 Be patient It could take up to 24 hours for the tags to be active Click on your account name (1) then select \"My Billing Dashboard\" (2) Then click Cost Allocation tag Finally, search all \"Scale-Out Computing on AWS\" tags then click \"Activate\" Step 2: Enable Cost Explorer \u00b6 In your billing dashboard, select \"Cost Explorer\" (1) and click \"Enable Cost Explorer\" (2). Step 3: Query Cost Explorer \u00b6 Open your Cost Explorer tab and specify your filters. In this example I want to get the EC2 cost (1), group by day for my queue named \"cpus\" (2). To get more detailed information, select 'Group By' and apply additional filters. Here is an example if I want user level information for \"cpus\" queue Click \"Tag\" section under \"Group By\" horizontal label (1) and select \"soca:JobOwner\" tag. Your graph will automatically be updated with a cost breakdown by users for \"cpus\" queue","title":"Review your HPC costs"},{"location":"analytics/review-hpc-costs/#aws-cost-explorer","text":"Any EC2 resource launched by Scale-Out Computing on AWS comes with an extensive list of EC2 tags that can be used to get detailed information about your cluster usage. List includes (but not limited to): Project Name Job Owner Job Name Job Queue Job Id These are the default tags and you can add your own tags if needed.","title":"AWS Cost Explorer"},{"location":"analytics/review-hpc-costs/#step1-enable-cost-allocation-tags","text":"Be patient It could take up to 24 hours for the tags to be active Click on your account name (1) then select \"My Billing Dashboard\" (2) Then click Cost Allocation tag Finally, search all \"Scale-Out Computing on AWS\" tags then click \"Activate\"","title":"Step1: Enable Cost Allocation Tags"},{"location":"analytics/review-hpc-costs/#step-2-enable-cost-explorer","text":"In your billing dashboard, select \"Cost Explorer\" (1) and click \"Enable Cost Explorer\" (2).","title":"Step 2: Enable Cost Explorer"},{"location":"analytics/review-hpc-costs/#step-3-query-cost-explorer","text":"Open your Cost Explorer tab and specify your filters. In this example I want to get the EC2 cost (1), group by day for my queue named \"cpus\" (2). To get more detailed information, select 'Group By' and apply additional filters. Here is an example if I want user level information for \"cpus\" queue Click \"Tag\" section under \"Group By\" horizontal label (1) and select \"soca:JobOwner\" tag. Your graph will automatically be updated with a cost breakdown by users for \"cpus\" queue","title":"Step 3: Query Cost Explorer"},{"location":"analytics/set-up-budget-project/","text":"On this page, I will demonstrate how to configure a budget for a given project and reject job if you exceed the allocated budget For this example, I will create a budget named \"Project 1\" and prevent user to submit job if (1) they do not belong to the project and (2) if the budget has expired. First, read this link to understand how to monitor your cluster cost and budgets on AWS. Configure the scheduler hook \u00b6 To enable this feature, you will first need to verify the project assigned to each job during submission time. The script managing this can be found on your Scale-Out Computing on AWS cluster at /apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_hooks/queuejob/check_project_budget.py First, edit this file and manually enter your AWS account id: # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_manager/settings/project_cost_manager.txt' user_must_belong_to_project = True # Change if you don't want to restrict project to a list of users allow_job_no_project = False # Change if you do not want to enforce project at job submission allow_user_multiple_projects = True # Change if you want to restrict a user to one project Then enable the hook by running the following commands as root (on the scheduler host): user@host: qmgr -c \"create hook check_project_budget event=queuejob\" user@host: qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_hooks/queuejob/check_project_budget.py\" Test it \u00b6 Submit a job when budget is valid \u00b6 Go to AWS Billing , click Budget on the left sidebar and create a new budget Select \"Cost Budget\". Name your budget \"Project 1\" and configure the Period/Budget based on your requirements. For my example I will allocate a $100 per month recurring budget for my project called \"Project 1\" (use Tag: soca:JobProject ) Set up a email notification when your budget exceed 80% then click \"Confirm Budget\" As you can see, I have still money available for this project (budgeted $100 but only used $15). Let's try to submit a job user@host$ qsub -- /bin/echo Hello qsub: Error. You tried to submit job without project. Specify project using -P parameter This does not work because the job was submitted without project defined. If you still want to let your users do that, edit allow_job_no_project = False on the hook file. Let's try the same request but specify -P \"Project 1\" during submission: user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: User mickael is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt This time, the hook complains thant my user \"mickael\" is not mapped the the project. This is because (1) the budget does not exist on the HPC cluster or (2) my user has not been approved to use this project. Edit /apps/soca/cluster_manager/settings/project_cost_manager.txt and configure your budget for this user: # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https: //soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [ Project 1 ] mickael Important The config section (\"Project 1\") MUST match the name of the budget your created on AWS Budget (it's case sensitive) Save this file and try to submit a job, this time the job should go to the queue user@host$ qsub -P \"Project 1\" -- /bin/echo Hello 5 .ip-10-0-1-223 Submit a job when budget is invalid \u00b6 Now let's go back to your AWS budget, and let's simulate we are over-budget Now try to submit a job for \"Project 1\", your request should be rejected user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: Error. Budget for Project 1 exceed allocated threshold. Update it on AWS Budget bash The hook query the AWS Budget in real-time. So if your users are blocked because of budget restriction, you can at any time edit the value on AWS Budget and unblock them (assuming you still have some money left in your pocket :P ) As mentioned above, the project name on /apps/soca/cluster_manager/settings/project_cost_manager.txt and the name of your AWS Budget must match (case sensitive). If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist.","title":"Set up budget per project"},{"location":"analytics/set-up-budget-project/#configure-the-scheduler-hook","text":"To enable this feature, you will first need to verify the project assigned to each job during submission time. The script managing this can be found on your Scale-Out Computing on AWS cluster at /apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_hooks/queuejob/check_project_budget.py First, edit this file and manually enter your AWS account id: # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_manager/settings/project_cost_manager.txt' user_must_belong_to_project = True # Change if you don't want to restrict project to a list of users allow_job_no_project = False # Change if you do not want to enforce project at job submission allow_user_multiple_projects = True # Change if you want to restrict a user to one project Then enable the hook by running the following commands as root (on the scheduler host): user@host: qmgr -c \"create hook check_project_budget event=queuejob\" user@host: qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_hooks/queuejob/check_project_budget.py\"","title":"Configure the scheduler hook"},{"location":"analytics/set-up-budget-project/#test-it","text":"","title":"Test it"},{"location":"analytics/set-up-budget-project/#submit-a-job-when-budget-is-valid","text":"Go to AWS Billing , click Budget on the left sidebar and create a new budget Select \"Cost Budget\". Name your budget \"Project 1\" and configure the Period/Budget based on your requirements. For my example I will allocate a $100 per month recurring budget for my project called \"Project 1\" (use Tag: soca:JobProject ) Set up a email notification when your budget exceed 80% then click \"Confirm Budget\" As you can see, I have still money available for this project (budgeted $100 but only used $15). Let's try to submit a job user@host$ qsub -- /bin/echo Hello qsub: Error. You tried to submit job without project. Specify project using -P parameter This does not work because the job was submitted without project defined. If you still want to let your users do that, edit allow_job_no_project = False on the hook file. Let's try the same request but specify -P \"Project 1\" during submission: user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: User mickael is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt This time, the hook complains thant my user \"mickael\" is not mapped the the project. This is because (1) the budget does not exist on the HPC cluster or (2) my user has not been approved to use this project. Edit /apps/soca/cluster_manager/settings/project_cost_manager.txt and configure your budget for this user: # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https: //soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [ Project 1 ] mickael Important The config section (\"Project 1\") MUST match the name of the budget your created on AWS Budget (it's case sensitive) Save this file and try to submit a job, this time the job should go to the queue user@host$ qsub -P \"Project 1\" -- /bin/echo Hello 5 .ip-10-0-1-223","title":"Submit a job when budget is valid"},{"location":"analytics/set-up-budget-project/#submit-a-job-when-budget-is-invalid","text":"Now let's go back to your AWS budget, and let's simulate we are over-budget Now try to submit a job for \"Project 1\", your request should be rejected user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: Error. Budget for Project 1 exceed allocated threshold. Update it on AWS Budget bash The hook query the AWS Budget in real-time. So if your users are blocked because of budget restriction, you can at any time edit the value on AWS Budget and unblock them (assuming you still have some money left in your pocket :P ) As mentioned above, the project name on /apps/soca/cluster_manager/settings/project_cost_manager.txt and the name of your AWS Budget must match (case sensitive). If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist.","title":"Submit a job when budget is invalid"},{"location":"security/backup-restore-your-cluster/","text":"By default, Scale-Out Computing on AWS automatically backup your EC2 scheduler and EFS filesystems every day and keep the backups for 30 days using AWS Backup. During the installation, Scale-Out Computing on AWS creates a new backup vault and one backup plan. You can edit them if needed. What is AWS Backups? \u00b6 AWS Backup is a fully managed backup service that makes it easy to centralize and automate the back up of data across AWS services in the cloud. Using AWS Backup, you can centrally configure backup policies and monitor backup activity for AWS resources, such as Amazon EBS volumes, Amazon RDS databases, Amazon DynamoDB tables, Amazon EFS file systems. Backup Vault \u00b6 A \"Backup Vault\" is where all your backups are stored. Your vault is automatically encrypted using your Key Management Service (KMS) key and reference to your SOCA cluster ID ( soca-mycluster in this example) Backup Plan \u00b6 A \"Backup Plan\" is where you define all your backup strategy such as backup frequency, data retention or resource assignments. Backup rules (red section) \u00b6 By default, Scale-Out Computing on AWS creates one backup rule with the following parameters: Backup will start every day between 5AM and 6AM UTC (blue section) Backup will expire after 1 month (orange section) Backup is stored on the encrypted vault created by SOCA (purple section) If needed, you can edit this rule (or create a new one) to match your company backup strategy. Resources Assignments (green section) \u00b6 By default, Scale-Out Computing on AWS backup all data using the soca : BackupPlan tag. Value of this tag must match your cluster ID ( soca-mycluster in this example). Supported Resources AWS Backup only support the following resources: EC2 instances EBS disks EFS filesystems RDS (not used by SOCA) DynamoDB (not used by SOCA) How to add/remove resources to the backup plan \u00b6 Backup resources are managed by soca : BackupPlan tag. The value of this tag must match the value of your Backup Plan (by default it should match the name of your cluster). Apply this tag to any EC2 instance, EBS volumes or EFS filesystem you want to backup. How to restore a backup? \u00b6 On the left sidebar, click \"Protected Resources\" then choose the resource you want to restore This will open a new window with additional information about this resource (either EFS or EC2). Select the latest entry you want to restore from the \"Backups\" section then click \"Restore\" This will open a regular EC2 launch instance or EFS wizard. Specify the parameters (VPC, Subnet, Security Group, IAM role ...) you want to use and click \"Restore Backup\" Restore Role By default, you can only apply SchedulerIAMRole or ComputeNodeIAMRole to the EC2 resource you are restoring. If you want to restore one EC2 instance with a different role, you must edit iam : PassRole policy of your SOCA-Backup role. Make sure to use the SOCA-Backup IAM role created by SOCA during initial installation. If you want to use the default role created by AWS Backup, make sure to add iam : PassRole permission. How to delete a backup ? \u00b6 Select your vault, choose which recovery point you want to remove under the \"Backups\" section then click \"Delete\". Check the status of the backup jobs \u00b6 On the left sidebar, check \"Jobs\" to verify if your backup jobs are running correctly What happen if you delete your Cloudformation stack? \u00b6 Your backup vault won't be deleted if you have active backups in it. In case of accidental termination of your primary cloudformation template, you will still be able to recover your data by restoring the EFS and/or EC2. To delete your AWS Backup entry, you first need to manually remove all backups present in your vault.","title":"Backup your entire cluster automatically"},{"location":"security/backup-restore-your-cluster/#what-is-aws-backups","text":"AWS Backup is a fully managed backup service that makes it easy to centralize and automate the back up of data across AWS services in the cloud. Using AWS Backup, you can centrally configure backup policies and monitor backup activity for AWS resources, such as Amazon EBS volumes, Amazon RDS databases, Amazon DynamoDB tables, Amazon EFS file systems.","title":"What is AWS Backups?"},{"location":"security/backup-restore-your-cluster/#backup-vault","text":"A \"Backup Vault\" is where all your backups are stored. Your vault is automatically encrypted using your Key Management Service (KMS) key and reference to your SOCA cluster ID ( soca-mycluster in this example)","title":"Backup Vault"},{"location":"security/backup-restore-your-cluster/#backup-plan","text":"A \"Backup Plan\" is where you define all your backup strategy such as backup frequency, data retention or resource assignments.","title":"Backup Plan"},{"location":"security/backup-restore-your-cluster/#backup-rules-red-section","text":"By default, Scale-Out Computing on AWS creates one backup rule with the following parameters: Backup will start every day between 5AM and 6AM UTC (blue section) Backup will expire after 1 month (orange section) Backup is stored on the encrypted vault created by SOCA (purple section) If needed, you can edit this rule (or create a new one) to match your company backup strategy.","title":"Backup rules (red section)"},{"location":"security/backup-restore-your-cluster/#resources-assignments-green-section","text":"By default, Scale-Out Computing on AWS backup all data using the soca : BackupPlan tag. Value of this tag must match your cluster ID ( soca-mycluster in this example). Supported Resources AWS Backup only support the following resources: EC2 instances EBS disks EFS filesystems RDS (not used by SOCA) DynamoDB (not used by SOCA)","title":"Resources Assignments (green section)"},{"location":"security/backup-restore-your-cluster/#how-to-addremove-resources-to-the-backup-plan","text":"Backup resources are managed by soca : BackupPlan tag. The value of this tag must match the value of your Backup Plan (by default it should match the name of your cluster). Apply this tag to any EC2 instance, EBS volumes or EFS filesystem you want to backup.","title":"How to add/remove resources to the backup plan"},{"location":"security/backup-restore-your-cluster/#how-to-restore-a-backup","text":"On the left sidebar, click \"Protected Resources\" then choose the resource you want to restore This will open a new window with additional information about this resource (either EFS or EC2). Select the latest entry you want to restore from the \"Backups\" section then click \"Restore\" This will open a regular EC2 launch instance or EFS wizard. Specify the parameters (VPC, Subnet, Security Group, IAM role ...) you want to use and click \"Restore Backup\" Restore Role By default, you can only apply SchedulerIAMRole or ComputeNodeIAMRole to the EC2 resource you are restoring. If you want to restore one EC2 instance with a different role, you must edit iam : PassRole policy of your SOCA-Backup role. Make sure to use the SOCA-Backup IAM role created by SOCA during initial installation. If you want to use the default role created by AWS Backup, make sure to add iam : PassRole permission.","title":"How to restore a backup?"},{"location":"security/backup-restore-your-cluster/#how-to-delete-a-backup","text":"Select your vault, choose which recovery point you want to remove under the \"Backups\" section then click \"Delete\".","title":"How to delete a backup ?"},{"location":"security/backup-restore-your-cluster/#check-the-status-of-the-backup-jobs","text":"On the left sidebar, check \"Jobs\" to verify if your backup jobs are running correctly","title":"Check the status of the backup jobs"},{"location":"security/backup-restore-your-cluster/#what-happen-if-you-delete-your-cloudformation-stack","text":"Your backup vault won't be deleted if you have active backups in it. In case of accidental termination of your primary cloudformation template, you will still be able to recover your data by restoring the EFS and/or EC2. To delete your AWS Backup entry, you first need to manually remove all backups present in your vault.","title":"What happen if you delete your Cloudformation stack?"},{"location":"security/integrate-cognito-sso/","text":"On this page, we will see how you can automatically authenticate your users to Scale-Out Computing on AWS using without having them to enter their password. What is Cognito / Oauth2 \u00b6 With Amazon Cognito , your users can sign-in through social identity providers such as Google, Facebook, and Amazon, and through enterprise identity providers such as Microsoft Active Directory using SAML. Amazon Cognito User Pools provide a secure user directory that scales to hundreds of millions of users. As a fully managed service, User Pools are easy to set up without any worries about server infrastructure. User Pools provide user profiles and authentication tokens for users who sign up directly and for federated users who sign in with social and enterprise identity providers. Additionally, read this link if you are not already familiar with Oauth2 workflow. How it works ? \u00b6 1: Mary has an account on her corporate LDAP or Active Directory. This account has an username (e.g mary), an email (e.g mary@company.com ) and other parameters (cost center, location ... ). She uses her account to log in to her corporate network. 2: Mary wants to access the web UI of SOCA (we assume she already has an active account on SOCA. If not, refer to this page to learn how to manage user account on SOCA ) 2.1: She can access the application by entering her SOCA LDAP username/password 2.2: She can be automatically logged in using Amazon Cognito 3: Assuming SSO is enabled, SOCA will forward the access request Cognito which will use Mary's Corporate LDAP as a Federated identity to determine if she is a valid user. This is the authentication part. 4: Mary's Corporate LDAP will check her account (e.g based on Kerberos ticket) and return a SAML token. This is the authorization part. 5: Based on the authorization results, SOCA will automatically logs Mary in or reject her request What if Mary does not have an account on SOCA? Assuming a user has a valid credential on corporate LDAP/AD but not on SOCA, Cognito will redirect the user to the default SOCA login portal. How do you determine if a user has an account on SOCA? To verify if a corporate user is active and can log in, SOCA checks whether or not this user has an account. By default, we determine the user account name is the first part of an email address. For example, if the email returned by your corporate LDAP for a given user is myuser@company.com , we will assume this user the SOCA account is myuser . If this mapping does not apply to your company, you can change it by editing /apps/soca/<CLUSTER_ID>/cluster_web_ui/generic/auth.py . Create your Cognito User Pool \u00b6 Info This setup is different for each customer, refer fo the official AWS Documentaton for Cognito if needed. First, you need to configure your Cognito User Pool with your existing SAML provider . You can leave most settings by default as we won't be using any user password, custom login UI etc.. This step is optional if you already have a User Pool configured. Integrate your AD/LDAP with Cognito \u00b6 Once your User Pool is created, go to Federation > Identity Provider and choose whatever IDP you want to use (most likely SAML if you are looking to integrate your corporate LDAP) SAML integration vary based on your own corporate settings, reach out to your local IT if needed. Here are some extra links if you need more documentation related to SAML: SAML Authentication Flow Adding SAML Identity Providers to a User Pool Create a SAML Provider on Cognito Warning The only required parameters for Scale-Out Computing on AWS is the email attribute. Create a Cognito Client \u00b6 On Cognito interface, click User Pools > Federated Identities then General Settings > App Clients and finally click Add Another App Client . Note your client name, client id and client secret and leave all other parameters by default. Configure your Cognito Client \u00b6 Now visit App Integration > App client setting and configure your application with the Identity Provider you just created ( Enabled Identity Provider ). You also want to specify the callback url(s) for all domains you are planning to use. The default callback URL you must specify is https://<your_soca_elb_name>/oauth but you can add any other URL if you have multiple environments (e.g: dev/test) Important Info about Callback Update the callback URLs with your custom domain if you are using one. If you don't do that, you will get a Cognito error with \"Redirect URI mismatch\" Configure SOCA \u00b6 Edit /apps/soca/<CLUSTER_ID>/cluster_web_ui/generic/parameters.cfg and update the [ cognito ] section as shown below: [ cognito ] ## Cognito enable_sso = \"true\" # Set this flag to \"true\" cognito_oauth_authorize_endpoint = \"https://<YOUR_COGNITO_POOL>.auth.<YOUR_REGION>.amazoncognito.com/oauth2/authorize\" cognito_oauth_token_endpoint = \"https://<YOUR_COGNITO_POOL>.auth.<YOUR_REGION>.amazoncognito.com/oauth2/token\" cognito_jws_keys_endpoint = \"https://cognito-idp.<YOUR_REGION>.amazonaws.com/<YOUR_REGION>_<YOUR_POOL_ID>/.well-known/jwks.json\" cognito_app_secret = \"<YOUR_APP_SECRET>\" cognito_app_id = \"<YOUR_APP_ID>\" cognito_root_url = \"<YOUR_WEB_URL>\" cognito_callback_url = \"<YOUR_CALLBACK_URL>\" Important Make sure to use double quotes for all variables (eg. enable_sso=\"true\" and not enable_sso='true') Restart the Web UI \u00b6 Simply restart the Web UI by running: /apps/soca/<CLUSTER_ID>/cluster_web_ui/socawebui.sh stop /apps/soca/<CLUSTER_ID>/cluster_web_ui/socawebui.sh start Now try to access https://<YOUR_SOCA_DNS>/ , you should be automatically logged in. Note that we keep /login as fallback authentication mechanism to LDAP, so make sure your users access https://<YOUR_SOCA_DNS>/ and not https://<YOUR_SOCA_DNS>/login if they want to be automatically logged in with SSO","title":"Enable Oauth2 authentication with Cognito"},{"location":"security/integrate-cognito-sso/#what-is-cognito-oauth2","text":"With Amazon Cognito , your users can sign-in through social identity providers such as Google, Facebook, and Amazon, and through enterprise identity providers such as Microsoft Active Directory using SAML. Amazon Cognito User Pools provide a secure user directory that scales to hundreds of millions of users. As a fully managed service, User Pools are easy to set up without any worries about server infrastructure. User Pools provide user profiles and authentication tokens for users who sign up directly and for federated users who sign in with social and enterprise identity providers. Additionally, read this link if you are not already familiar with Oauth2 workflow.","title":"What is Cognito / Oauth2"},{"location":"security/integrate-cognito-sso/#how-it-works","text":"1: Mary has an account on her corporate LDAP or Active Directory. This account has an username (e.g mary), an email (e.g mary@company.com ) and other parameters (cost center, location ... ). She uses her account to log in to her corporate network. 2: Mary wants to access the web UI of SOCA (we assume she already has an active account on SOCA. If not, refer to this page to learn how to manage user account on SOCA ) 2.1: She can access the application by entering her SOCA LDAP username/password 2.2: She can be automatically logged in using Amazon Cognito 3: Assuming SSO is enabled, SOCA will forward the access request Cognito which will use Mary's Corporate LDAP as a Federated identity to determine if she is a valid user. This is the authentication part. 4: Mary's Corporate LDAP will check her account (e.g based on Kerberos ticket) and return a SAML token. This is the authorization part. 5: Based on the authorization results, SOCA will automatically logs Mary in or reject her request What if Mary does not have an account on SOCA? Assuming a user has a valid credential on corporate LDAP/AD but not on SOCA, Cognito will redirect the user to the default SOCA login portal. How do you determine if a user has an account on SOCA? To verify if a corporate user is active and can log in, SOCA checks whether or not this user has an account. By default, we determine the user account name is the first part of an email address. For example, if the email returned by your corporate LDAP for a given user is myuser@company.com , we will assume this user the SOCA account is myuser . If this mapping does not apply to your company, you can change it by editing /apps/soca/<CLUSTER_ID>/cluster_web_ui/generic/auth.py .","title":"How it works ?"},{"location":"security/integrate-cognito-sso/#create-your-cognito-user-pool","text":"Info This setup is different for each customer, refer fo the official AWS Documentaton for Cognito if needed. First, you need to configure your Cognito User Pool with your existing SAML provider . You can leave most settings by default as we won't be using any user password, custom login UI etc.. This step is optional if you already have a User Pool configured.","title":"Create your Cognito User Pool"},{"location":"security/integrate-cognito-sso/#integrate-your-adldap-with-cognito","text":"Once your User Pool is created, go to Federation > Identity Provider and choose whatever IDP you want to use (most likely SAML if you are looking to integrate your corporate LDAP) SAML integration vary based on your own corporate settings, reach out to your local IT if needed. Here are some extra links if you need more documentation related to SAML: SAML Authentication Flow Adding SAML Identity Providers to a User Pool Create a SAML Provider on Cognito Warning The only required parameters for Scale-Out Computing on AWS is the email attribute.","title":"Integrate your AD/LDAP with Cognito"},{"location":"security/integrate-cognito-sso/#create-a-cognito-client","text":"On Cognito interface, click User Pools > Federated Identities then General Settings > App Clients and finally click Add Another App Client . Note your client name, client id and client secret and leave all other parameters by default.","title":"Create a Cognito Client"},{"location":"security/integrate-cognito-sso/#configure-your-cognito-client","text":"Now visit App Integration > App client setting and configure your application with the Identity Provider you just created ( Enabled Identity Provider ). You also want to specify the callback url(s) for all domains you are planning to use. The default callback URL you must specify is https://<your_soca_elb_name>/oauth but you can add any other URL if you have multiple environments (e.g: dev/test) Important Info about Callback Update the callback URLs with your custom domain if you are using one. If you don't do that, you will get a Cognito error with \"Redirect URI mismatch\"","title":"Configure your Cognito Client"},{"location":"security/integrate-cognito-sso/#configure-soca","text":"Edit /apps/soca/<CLUSTER_ID>/cluster_web_ui/generic/parameters.cfg and update the [ cognito ] section as shown below: [ cognito ] ## Cognito enable_sso = \"true\" # Set this flag to \"true\" cognito_oauth_authorize_endpoint = \"https://<YOUR_COGNITO_POOL>.auth.<YOUR_REGION>.amazoncognito.com/oauth2/authorize\" cognito_oauth_token_endpoint = \"https://<YOUR_COGNITO_POOL>.auth.<YOUR_REGION>.amazoncognito.com/oauth2/token\" cognito_jws_keys_endpoint = \"https://cognito-idp.<YOUR_REGION>.amazonaws.com/<YOUR_REGION>_<YOUR_POOL_ID>/.well-known/jwks.json\" cognito_app_secret = \"<YOUR_APP_SECRET>\" cognito_app_id = \"<YOUR_APP_ID>\" cognito_root_url = \"<YOUR_WEB_URL>\" cognito_callback_url = \"<YOUR_CALLBACK_URL>\" Important Make sure to use double quotes for all variables (eg. enable_sso=\"true\" and not enable_sso='true')","title":"Configure SOCA"},{"location":"security/integrate-cognito-sso/#restart-the-web-ui","text":"Simply restart the Web UI by running: /apps/soca/<CLUSTER_ID>/cluster_web_ui/socawebui.sh stop /apps/soca/<CLUSTER_ID>/cluster_web_ui/socawebui.sh start Now try to access https://<YOUR_SOCA_DNS>/ , you should be automatically logged in. Note that we keep /login as fallback authentication mechanism to LDAP, so make sure your users access https://<YOUR_SOCA_DNS>/ and not https://<YOUR_SOCA_DNS>/login if they want to be automatically logged in with SSO","title":"Restart the Web UI"},{"location":"security/manage-queue-acls/","text":"You can manage ACLs for each queue by configuring both allowed_users or excluded_users . These parameters can be configured as: List of allowed/excluded users: allowed_users: [\"user1\", \"user2\"] List of LDAP groups: allowed_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"] List of username and LDAP groups: allowed_users: [\"user1\", \"cn=mynewgroup,ou=Group,dc=soca,dc=local\", \"user2\"] Restrict queue for some users \u00b6 Considering /apps/soca/<CLUSTER_ID>cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" ] allowed_users : [] # empty list = all users can submit job excluded_users : [] # empty list = no restriction , [ \"*\" ] = only allowed_users can submit job ... test : queues : [ \"high\" , \"low\" ] allowed_users : [] excluded_users : [ \"user1\" ] In this example, user1 can submit a job to \"normal\" queue but not on \"high\" or \"low\" queues. # Job submission does not work on \"high\" queue because user1 is on the excluded_users list pattern qsub -q high -- /bin/sleep 60 qsub: user1 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml # Job submission is ok on \"normal\" queue qsub -q normal -- /bin/sleep 60 19.ip-30-0-2-29 allowed_users overrides excluded_users Job will go through if a user is present is both allowed_users and excluded_users lists. Restrict the queue for everyone except allowed_users \u00b6 excluded_users: [\"*\"] will prevent anyone to use the queue except for the list of allowed_users . In the example below, user1 is the only user authorized to submit job. queue_type : compute : queues : [ \"normal\" ] allowed_users : [ \"user1\" ] excluded_users : [ \"*\" ] Manage ACLs using LDAP groups \u00b6 SOCA will consider allowed_users or excluded_users as LDAP group if you did not specify them as list. Create a text file mynewgroup.ldif and add the following content (note we are adding our user1 as a group member) dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 Create the group using ldapadd command ~ ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif adding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\" Run ldapsearch command to confirm your group has been created correctly and your user1 is part of it ~ ldapsearch -x -b cn=mynewgroup,ou=Group,dc=soca,dc=local -LLL dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 Let's configure our queue to reject all users: allowed_users: [] excluded_users: [\"*\"] Confirm user1 can't submit any job: qsub -q high -- /bin/sleep 60 qsub: user1 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/cluster_manager/settings/queue_mapping.yml Edit your allowed_users and specify your LDAP group: allowed_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"] excluded_users: [\"*\"] Verify user1 can submit job: qsub -q high -- /bin/sleep 60 22.ip-30-0-2-29 Let's now assume you have a user2 . Confirm this user can't submit job qsub -q high -- /bin/sleep 60 qsub: user2 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/cluster_manager/settings/queue_mapping.yml Create a new ldif file (add_new_user.ldif) and add user2 to your group dn : cn = mynewgroup , ou = Group , dc = soca , dc = local changetype : modify add : memberUid memberUid : user2 Execute the command ~ ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f add_new_user.ldif modifying entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local Confirm both users are part of the group: ldapsearch -x -b cn=mynewgroup,ou=Group,dc=soca,dc=local -LLL dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 memberUid: user2 Finally, confirm user2 is now authorized to submit job: qsub -q high -- /bin/sleep 60 23.ip-30-0-2-29 On the other side, you can also prevent users from a LDAP group to use the queue by specifying the ldap group as \"excluded_users\" allowed_users: [] excluded_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"] Check the logs \u00b6 Scheduler hooks are located on /var/spool/pbs/server_logs/ Code \u00b6 The hook file can be found under /apps/soca/cluster_hooks/<CLUSTER_ID>/queuejob/check_queue_acls.py on your Scale-Out Computing on AWS cluster) Disable the hook \u00b6 You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_acls event=queuejob\"","title":"Manage Access Lists at queue level"},{"location":"security/manage-queue-acls/#restrict-queue-for-some-users","text":"Considering /apps/soca/<CLUSTER_ID>cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" ] allowed_users : [] # empty list = all users can submit job excluded_users : [] # empty list = no restriction , [ \"*\" ] = only allowed_users can submit job ... test : queues : [ \"high\" , \"low\" ] allowed_users : [] excluded_users : [ \"user1\" ] In this example, user1 can submit a job to \"normal\" queue but not on \"high\" or \"low\" queues. # Job submission does not work on \"high\" queue because user1 is on the excluded_users list pattern qsub -q high -- /bin/sleep 60 qsub: user1 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml # Job submission is ok on \"normal\" queue qsub -q normal -- /bin/sleep 60 19.ip-30-0-2-29 allowed_users overrides excluded_users Job will go through if a user is present is both allowed_users and excluded_users lists.","title":"Restrict queue for some users"},{"location":"security/manage-queue-acls/#restrict-the-queue-for-everyone-except-allowed_users","text":"excluded_users: [\"*\"] will prevent anyone to use the queue except for the list of allowed_users . In the example below, user1 is the only user authorized to submit job. queue_type : compute : queues : [ \"normal\" ] allowed_users : [ \"user1\" ] excluded_users : [ \"*\" ]","title":"Restrict the queue for everyone except allowed_users"},{"location":"security/manage-queue-acls/#manage-acls-using-ldap-groups","text":"SOCA will consider allowed_users or excluded_users as LDAP group if you did not specify them as list. Create a text file mynewgroup.ldif and add the following content (note we are adding our user1 as a group member) dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 Create the group using ldapadd command ~ ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif adding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\" Run ldapsearch command to confirm your group has been created correctly and your user1 is part of it ~ ldapsearch -x -b cn=mynewgroup,ou=Group,dc=soca,dc=local -LLL dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 Let's configure our queue to reject all users: allowed_users: [] excluded_users: [\"*\"] Confirm user1 can't submit any job: qsub -q high -- /bin/sleep 60 qsub: user1 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/cluster_manager/settings/queue_mapping.yml Edit your allowed_users and specify your LDAP group: allowed_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"] excluded_users: [\"*\"] Verify user1 can submit job: qsub -q high -- /bin/sleep 60 22.ip-30-0-2-29 Let's now assume you have a user2 . Confirm this user can't submit job qsub -q high -- /bin/sleep 60 qsub: user2 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/cluster_manager/settings/queue_mapping.yml Create a new ldif file (add_new_user.ldif) and add user2 to your group dn : cn = mynewgroup , ou = Group , dc = soca , dc = local changetype : modify add : memberUid memberUid : user2 Execute the command ~ ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f add_new_user.ldif modifying entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local Confirm both users are part of the group: ldapsearch -x -b cn=mynewgroup,ou=Group,dc=soca,dc=local -LLL dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 memberUid: user2 Finally, confirm user2 is now authorized to submit job: qsub -q high -- /bin/sleep 60 23.ip-30-0-2-29 On the other side, you can also prevent users from a LDAP group to use the queue by specifying the ldap group as \"excluded_users\" allowed_users: [] excluded_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"]","title":"Manage ACLs using LDAP groups"},{"location":"security/manage-queue-acls/#check-the-logs","text":"Scheduler hooks are located on /var/spool/pbs/server_logs/","title":"Check the logs"},{"location":"security/manage-queue-acls/#code","text":"The hook file can be found under /apps/soca/cluster_hooks/<CLUSTER_ID>/queuejob/check_queue_acls.py on your Scale-Out Computing on AWS cluster)","title":"Code"},{"location":"security/manage-queue-acls/#disable-the-hook","text":"You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_acls event=queuejob\"","title":"Disable the hook"},{"location":"security/manage-queue-instance-types/","text":"You can manage the EC2 instance types allowed for each queue by configuring both allowed_instance_types and excluded_instance_types . This allows you to restrict which instance types or family of instances are allowed to be used for jobs on a per queue basis by either white listing instance types or blocking them. Default settings By default, users can provision any type of instance. There are no restrictions configured out of the box. These parameters can be configured as: List of allowed EC2 instance types for a queue: allowed_instance_types: [\"c5.4xlarge\", \"r5.2xlarge\"] List of excluded EC2 instance types for a queue: excluded_instance_types: [\"f1.16xlarge\", \"i3.2xlarge\"] Allow EC2 instance types by specific type or by instance family: allowed_instance_types: [\"c5\", \"r5.2xlarge\"] Exclude EC2 instance types by specific type or by instance family: excluded_instance_types: [\"f1.16xlarge\", \"i3\"] Instance family specification uses the exact name of the instance family. If you add c5 to the allowed instance list c5 instances will be allowed. c5n instances will be blocked unless c5n is added to the allowed_instance_types to allow c5n instances to run. Allow only compute optimized EC2 instances \u00b6 Considering /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" ] allowed_instance_types : [ \"c5\" , \"c5n\" ] excluded_instance_types : [] ... test : queues : [ \"test\" ] allowed_instance_types : [ \"c5.4xlarge\" ] excluded_instance_types : [] In this example, only EC2 instance types in the c5 and c5n families can be used for jobs submitted to the normal queue. For the test queue only c5.4xlarge instance type will be allowed. # Job submission to queue \"normal\" using instance type i3.2xlarge is blocked qsub -q normal -l instance_type=i3.2xlarge -- /bin/echo test qsub: i3.2xlarge is not a valid instance type for the job queue normal. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml # Job submission to queue \"normal\" using instance from c5 family is allowed. qsub -q normal -l instance_type=c5.2xlarge -- /bin/echo test 15.ip-110-0-12-28 # Job submission to \"test\" queue only allowed if using c5.4xlarge instance type qsub -q test -l instance_type=c5.2xlarge -- /bin/echo test qsub: c5.2xlarge is not a valid instance type for the job queue test. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml qsub -q test -l instance_type=c5.4xlarge -- /bin/echo test 16.ip-110-0-12-28 Instance types in excluded_instance_types will be blocked even if they appear in allowed_instance_types If an instance type or family appears in both the excluded_instance_types list as well as allowed_instance_types for a queue, the excluded_instance_types setting takes priority. Block users from using specific EC2 instance types \u00b6 Considering /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" ] allowed_instance_types : [] excluded_instance_types : [ \"f1\" , \"g4.16xlarge\" , \"g3\" ] ... test : queues : [ \"test\" ] allowed_instance_types : [] excluded_instance_types : [ \"f1\" , \"g4dn\" ] In this example the normal queue will not allow instances in the f1 and g3 family as well as g4.16xlarge instanct types. The test queue will not allow instances in the f1 and g4 family. # Job submission to queue \"normal\" using instance type in f1 family is blocked qsub -q normal -l instance_type=f1.2xlarge -- /bin/echo test qsub: f1.2xlarge is not a valid instance type for the job queue normal. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml # Job submission to queue \"test\" using instance type in g4dn is blocked qsub -q normal -l instance_type=g4dn.xlarge -- /bin/echo test qsub: g4dn.xlarge is not a valid instance type for the job queue test. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml Check the logs \u00b6 Scheduler hooks are located on /var/spool/pbs/server_logs/ Code \u00b6 The hook file can be found under /apps/soca/cluster_hooks/<CLUSTER_ID>/queuejob/check_queue_instance_types.py on your Scale-Out Computing on AWS cluster) Disable the hook \u00b6 You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_instance_types event=queuejob\"","title":"Retrict provisioning of specific instance type"},{"location":"security/manage-queue-instance-types/#allow-only-compute-optimized-ec2-instances","text":"Considering /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" ] allowed_instance_types : [ \"c5\" , \"c5n\" ] excluded_instance_types : [] ... test : queues : [ \"test\" ] allowed_instance_types : [ \"c5.4xlarge\" ] excluded_instance_types : [] In this example, only EC2 instance types in the c5 and c5n families can be used for jobs submitted to the normal queue. For the test queue only c5.4xlarge instance type will be allowed. # Job submission to queue \"normal\" using instance type i3.2xlarge is blocked qsub -q normal -l instance_type=i3.2xlarge -- /bin/echo test qsub: i3.2xlarge is not a valid instance type for the job queue normal. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml # Job submission to queue \"normal\" using instance from c5 family is allowed. qsub -q normal -l instance_type=c5.2xlarge -- /bin/echo test 15.ip-110-0-12-28 # Job submission to \"test\" queue only allowed if using c5.4xlarge instance type qsub -q test -l instance_type=c5.2xlarge -- /bin/echo test qsub: c5.2xlarge is not a valid instance type for the job queue test. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml qsub -q test -l instance_type=c5.4xlarge -- /bin/echo test 16.ip-110-0-12-28 Instance types in excluded_instance_types will be blocked even if they appear in allowed_instance_types If an instance type or family appears in both the excluded_instance_types list as well as allowed_instance_types for a queue, the excluded_instance_types setting takes priority.","title":"Allow only compute optimized EC2 instances"},{"location":"security/manage-queue-instance-types/#block-users-from-using-specific-ec2-instance-types","text":"Considering /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" ] allowed_instance_types : [] excluded_instance_types : [ \"f1\" , \"g4.16xlarge\" , \"g3\" ] ... test : queues : [ \"test\" ] allowed_instance_types : [] excluded_instance_types : [ \"f1\" , \"g4dn\" ] In this example the normal queue will not allow instances in the f1 and g3 family as well as g4.16xlarge instanct types. The test queue will not allow instances in the f1 and g4 family. # Job submission to queue \"normal\" using instance type in f1 family is blocked qsub -q normal -l instance_type=f1.2xlarge -- /bin/echo test qsub: f1.2xlarge is not a valid instance type for the job queue normal. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml # Job submission to queue \"test\" using instance type in g4dn is blocked qsub -q normal -l instance_type=g4dn.xlarge -- /bin/echo test qsub: g4dn.xlarge is not a valid instance type for the job queue test. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml","title":"Block users from using specific EC2 instance types"},{"location":"security/manage-queue-instance-types/#check-the-logs","text":"Scheduler hooks are located on /var/spool/pbs/server_logs/","title":"Check the logs"},{"location":"security/manage-queue-instance-types/#code","text":"The hook file can be found under /apps/soca/cluster_hooks/<CLUSTER_ID>/queuejob/check_queue_instance_types.py on your Scale-Out Computing on AWS cluster)","title":"Code"},{"location":"security/manage-queue-instance-types/#disable-the-hook","text":"You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_instance_types event=queuejob\"","title":"Disable the hook"},{"location":"security/manage-queue-restricted-parameters/","text":"When submitting a job, users can override the default parameters configured for the queue ( click here to see a list of all parameters supported by SOCA ). For security, compliance or cost reasons, you may want prevent users to override these default parameters by configuring restricted_parameters on your queue_mapping.yml Prevent user to choose a different instance type \u00b6 Considering /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" , \"low\" ] instance_type : \"c5.large\" restricted_parameters : [ \"instance_type\" ] ... In this example, a job will be rejected if a user try to specify the instance_type parameter when using the normal or low queues. In this particular case, any job sent to the normal or low queue will be forced to use c5.large instance, which is the default instance type configured by HPC admins. qsub -q normal -l instance_type=m5.24xlarge -- /bin/echo test qsub: instance_type is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml Need to whitelist more than one instance type/family? Read the documentation if you want to limit users to a list of multiple instance types Prevent user to provision additional storage \u00b6 Considering /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" , \"low\" ] scratch_size : \"200\" restricted_parameters : [ \"scratch_size\" ] ... In this example, a job will be rejected if a user try to specify the scratch_size parameter when using the normal or low queues. In this particular case, any job sent to the normal or low queue will be forced to use a 200 GB EBS disk as /scratch partition. Users are no longer able to provision more storage than what's allocated to them. qsub -q normal -l scratch_size=550 -- /bin/echo test qsub: scratch_size is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml Combine multiple restrictions \u00b6 Considering /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" , \"low\" ] scratch_size : \"200\" restricted_parameters :[ \"instance_type\" , \"fsx_lustre_bucket\" , \"scratch_size\" ] ... In this example, a job will be rejected if a user try to change either instance_type , fsx_lustre_bucket or scratch_size parameters. qsub -q normal -l fsx_lustre_bucket=mybucket -- /bin/echo test qsub: fsx_lustre_bucket is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml Check the logs \u00b6 Scheduler hooks are located on /var/spool/pbs/server_logs/ Code \u00b6 The hook file can be found under /apps/soca/cluster_hooks/<CLUSTER_ID>/queuejob/check_queue_restricted_parameters.py on your Scale-Out Computing on AWS cluster) Disable the hook \u00b6 You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_restricted_parameters event=queuejob\"","title":"Prevent user to change specific parameters"},{"location":"security/manage-queue-restricted-parameters/#prevent-user-to-choose-a-different-instance-type","text":"Considering /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" , \"low\" ] instance_type : \"c5.large\" restricted_parameters : [ \"instance_type\" ] ... In this example, a job will be rejected if a user try to specify the instance_type parameter when using the normal or low queues. In this particular case, any job sent to the normal or low queue will be forced to use c5.large instance, which is the default instance type configured by HPC admins. qsub -q normal -l instance_type=m5.24xlarge -- /bin/echo test qsub: instance_type is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml Need to whitelist more than one instance type/family? Read the documentation if you want to limit users to a list of multiple instance types","title":"Prevent user to choose a different instance type"},{"location":"security/manage-queue-restricted-parameters/#prevent-user-to-provision-additional-storage","text":"Considering /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" , \"low\" ] scratch_size : \"200\" restricted_parameters : [ \"scratch_size\" ] ... In this example, a job will be rejected if a user try to specify the scratch_size parameter when using the normal or low queues. In this particular case, any job sent to the normal or low queue will be forced to use a 200 GB EBS disk as /scratch partition. Users are no longer able to provision more storage than what's allocated to them. qsub -q normal -l scratch_size=550 -- /bin/echo test qsub: scratch_size is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml","title":"Prevent user to provision additional storage"},{"location":"security/manage-queue-restricted-parameters/#combine-multiple-restrictions","text":"Considering /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml queue_type : compute : queues : [ \"normal\" , \"low\" ] scratch_size : \"200\" restricted_parameters :[ \"instance_type\" , \"fsx_lustre_bucket\" , \"scratch_size\" ] ... In this example, a job will be rejected if a user try to change either instance_type , fsx_lustre_bucket or scratch_size parameters. qsub -q normal -l fsx_lustre_bucket=mybucket -- /bin/echo test qsub: fsx_lustre_bucket is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml","title":"Combine multiple restrictions"},{"location":"security/manage-queue-restricted-parameters/#check-the-logs","text":"Scheduler hooks are located on /var/spool/pbs/server_logs/","title":"Check the logs"},{"location":"security/manage-queue-restricted-parameters/#code","text":"The hook file can be found under /apps/soca/cluster_hooks/<CLUSTER_ID>/queuejob/check_queue_restricted_parameters.py on your Scale-Out Computing on AWS cluster)","title":"Code"},{"location":"security/manage-queue-restricted-parameters/#disable-the-hook","text":"You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_restricted_parameters event=queuejob\"","title":"Disable the hook"},{"location":"security/update-soca-dns-ssl-certificate/","text":"By default, Scale-Out Computing on AWS will use a non-friendly DNS name and create a unique certificate to enable access through your HTTPS endpoint. Because it's a self-signed certificate, browsers won't recognized it and you will get a security warning on your first connection. In this page, we will see how you can update Scale-Out Computing on AWS to match your company domain name. Create a new DNS record for Scale-Out Computing on AWS \u00b6 For this example, let's assume I want to use https://demo.soca.dev . First locate the DNS associated to your ALB endpoint using the AWS console. Create a new CNAME record which point to your ALB endpoint. Once done, validate your DNS is working properly using the nslookup command. user@host: nslookup demo.soca.dev Non-authoritative answer: demo.soca.dev canonical name = soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com. Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .40.2.185 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 54 .68.240.4 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .27.180.89 Upload your SSL certificate to ACM \u00b6 Now that your friendly DNS is running, you will need to update the default ALB certificate to match your new domain. This assume you have a valid SSL certificate signed by a valid Certificate Authority (Symantec, Digicert ...) To upload your certificate, visit the AWS Certificate Manager (ACM) bash and click \"Import a Certificate\". Enter your private key, certificate and certificate chain (optional), then click Import. Once the import is complete, note your certificate identifier. Update your ALB with the new certificate \u00b6 Navigate to your Scale-Out Computing on AWS Load Balancer and choose \"Listeners\" tab. Select your HTTPS listener and click 'Edit' button. Change the default certificate to point to your new certificate and save your change. Update your default domain for DCV \u00b6 Now that you have updated your domain, you must also update DCV to point to the new DNS. Open your Secret Manager bash and select your Scale-Out Computing on AWS cluster configuration. Click \"Retrieve Secret Value\" and then \"Edit\". Find the entry \"LoadBalancerName\" and update the value with your new DNS name (demo.soca.dev in my case) then click Save Validate everything \u00b6 Now that you have your friendly DNS and SSL certificate configured, it's time to test. Visit your new DNS ( https://demo.soca.dev in my case) and make sure you can access Scale-Out Computing on AWS correctly. Make sure your browser is detecting your new SSL certificate correctly. Finally, create a new DCV session and verify the endpoint is using your new DNS name","title":"Change your DNS name and SSL certificate"},{"location":"security/update-soca-dns-ssl-certificate/#create-a-new-dns-record-for-scale-out-computing-on-aws","text":"For this example, let's assume I want to use https://demo.soca.dev . First locate the DNS associated to your ALB endpoint using the AWS console. Create a new CNAME record which point to your ALB endpoint. Once done, validate your DNS is working properly using the nslookup command. user@host: nslookup demo.soca.dev Non-authoritative answer: demo.soca.dev canonical name = soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com. Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .40.2.185 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 54 .68.240.4 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .27.180.89","title":"Create a new DNS record for Scale-Out Computing on AWS"},{"location":"security/update-soca-dns-ssl-certificate/#upload-your-ssl-certificate-to-acm","text":"Now that your friendly DNS is running, you will need to update the default ALB certificate to match your new domain. This assume you have a valid SSL certificate signed by a valid Certificate Authority (Symantec, Digicert ...) To upload your certificate, visit the AWS Certificate Manager (ACM) bash and click \"Import a Certificate\". Enter your private key, certificate and certificate chain (optional), then click Import. Once the import is complete, note your certificate identifier.","title":"Upload your SSL certificate to ACM"},{"location":"security/update-soca-dns-ssl-certificate/#update-your-alb-with-the-new-certificate","text":"Navigate to your Scale-Out Computing on AWS Load Balancer and choose \"Listeners\" tab. Select your HTTPS listener and click 'Edit' button. Change the default certificate to point to your new certificate and save your change.","title":"Update your ALB with the new certificate"},{"location":"security/update-soca-dns-ssl-certificate/#update-your-default-domain-for-dcv","text":"Now that you have updated your domain, you must also update DCV to point to the new DNS. Open your Secret Manager bash and select your Scale-Out Computing on AWS cluster configuration. Click \"Retrieve Secret Value\" and then \"Edit\". Find the entry \"LoadBalancerName\" and update the value with your new DNS name (demo.soca.dev in my case) then click Save","title":"Update your default domain for DCV"},{"location":"security/update-soca-dns-ssl-certificate/#validate-everything","text":"Now that you have your friendly DNS and SSL certificate configured, it's time to test. Visit your new DNS ( https://demo.soca.dev in my case) and make sure you can access Scale-Out Computing on AWS correctly. Make sure your browser is detecting your new SSL certificate correctly. Finally, create a new DCV session and verify the endpoint is using your new DNS name","title":"Validate everything"},{"location":"storage/backend-storage-options/","text":"Scale-Out Computing on AWS gives you the flexibility to customize your storage backend based on your requirements You can customize the root partition size You can provision a local scratch partition You can deploy standard SSD (gp2) or IO Optimized SSD (io1) volumes Scale-Out Computing on AWS automatically leverages instance store disk(s) as scratch partition when applicable In term of performance: Instance Store > EBS SSD IO > EBS SSD Standard > EFS Refer to this link to learn more about EBS volumes. EFS (shared) partitions \u00b6 /data partition \u00b6 /data is an Elastic File System partition mounted on all hosts. This contains the home directory of your LDAP users ($HOME = /data/home/<USERNAME> ). This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead) /apps partition \u00b6 /apps is an Elastic File System partition mounted on all hosts. This partition is designed to host all your CFD/FEA/EDA/Mathematical applications. This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead) FSx \u00b6 Scale-Out Computing on AWS supports FSx natively. Click here to learn how to use FSx as backend storage for your jobs . Instance (local) partitions \u00b6 Below are the storage options you can configure at an instance level for your jobs. If needed, add/remove/modify the storage logic by editing ComputeNode.sh script to match your requirements. Root partition \u00b6 By default Scale-Out Computing on AWS provision a 10GB EBS disk for the root partition. This may be an issue if you are using a custom AMI configured with a bigger root disk size or if you simply want to allocate additional storage for the root partition. To expand the size of the volume, submit a simulation using -l root_size=<SIZE_IN_GB> parameter. user@host:qsub -l root_size = 25 -- /bin/sleep 600 Result: Root partition is now 25GB user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259 :0 0 25G 0 disk \u251c\u2500nvme0n1p1 259 :1 0 25G 0 part / \u2514\u2500nvme0n1p128 259 :2 0 1M 0 part user@host: df -h / Filesystem Size Used Avail Use% Mounted on /dev/nvme0n1p1 25G 2 .2G 23G 9 % / Scratch Partition \u00b6 Info It's recommended to provision /scratch directory whenever your simulation is I/O intensive. /scratch is a local partition and will be deleted when you job complete. Make sure to copy the job output back to your $HOME /scratch is automatically created when Instance supports local ephemeral storage Request a /scratch partition with SSD disk \u00b6 During job submission, specify -l scratch_size=<SIZE_IN_GB> to provision a new EBS disk ( /dev/sdj ) mounted as /scratch user@host: qsub -l scratch_size = 150 -- /bin/sleep 600 Result: a 150 GB /scratch partition is available on all nodes user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 150G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 148G 61M 140G 1 % /scratch To verify the type of your EBS disk, simply go to your AWS bash > EC2 > Volumes and verify your EBS type is \"gp2\" (SSD). Refer to this link for more information about the various EBS types available. Request a /scratch partition with IO optimized disk \u00b6 To request an optimized SSD disk, use -l scratch_iops=<IOPS> along with -l scratch_size=<SIZE_IN_GB> . Refer to this link to get more details about burstable/IO EBS disks. user@host: qsub -l scratch_iops = 6000 -l scratch_size = 200 -- /bin/sleep 600 Looking at the EBS bash, the disk type is now \"io1\" and the number of IOPS match the value specified at job submission. Instance store partition \u00b6 Free storage is always good You are not charged for instance storage (included in the price of the instance) Some instances come with default instance storage . An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer and is removed as soon as the node is deleted. Scale-Out Computing on AWS automatically detects instance store disk and will use them as /scratch unless you specify -l scratch_size parameter for your job . In this case, Scale-Out Computing on AWS honors the user request and ignore the instance store volume(s). When node has 1 instance store volume \u00b6 For this example, I will use a \"c5d.9xlarge\" instance which is coming with a 900GB instance store disk. user@host: qsub -l instance_type = c5d.9xlarge -- /bin/sleep 600 Result: Default /scratch partition has been provisioned automatically using local instance storage user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 825G 77M 783G 1 % /scratch When node has more than 1 instance store volumes \u00b6 In this special case, ComputeNode.sh script will create a raid0 partition using all instance store volumes available. For this example, I will use a \"m5dn.12xlarge\" instance which is shipped with a 2 * 900GB instance store disks (total 1.8Tb). user@host: qsub -l instance_type = m5dn.12xlarge -- /bin/sleep 600 Result: /scratch is a 1.7TB raid0 partition (using 2 instance store volumes) user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme2n1 259 :1 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme0n1 259 :2 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :3 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :4 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/md127 1 .7T 77M 1 .6T 1 % /scratch Combine custom scratch and root size \u00b6 You can combine parameters as needed. For example, qsub -l root_size=150 -l scratch_size=200 -l nodes=2 will provision 2 nodes with 150GB / and 200GB SSD /scratch Change the storage parameters at queue level \u00b6 Edit /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml to configure default storage settings at a queue level: queue_type : compute : # /root will be 30 GB and /scratch will be a standard 100GB SSD queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"c5.large\" scratch_size : \"100\" root_size : \"30\" # .. Refer to the doc for more supported parameters memory : # /scratch will be a SSD with provisioned IO queues : [ \"queue4\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"r5.large\" scratch_size : \"300\" scratch_iops : \"5000\" instancestore : # /scratch will use the default instance store queues : [ \"queue5\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"m5dn.12large\" root_size : \"300\"","title":"Understand backend storage"},{"location":"storage/backend-storage-options/#efs-shared-partitions","text":"","title":"EFS (shared) partitions"},{"location":"storage/backend-storage-options/#data-partition","text":"/data is an Elastic File System partition mounted on all hosts. This contains the home directory of your LDAP users ($HOME = /data/home/<USERNAME> ). This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead)","title":"/data partition"},{"location":"storage/backend-storage-options/#apps-partition","text":"/apps is an Elastic File System partition mounted on all hosts. This partition is designed to host all your CFD/FEA/EDA/Mathematical applications. This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead)","title":"/apps partition"},{"location":"storage/backend-storage-options/#fsx","text":"Scale-Out Computing on AWS supports FSx natively. Click here to learn how to use FSx as backend storage for your jobs .","title":"FSx"},{"location":"storage/backend-storage-options/#instance-local-partitions","text":"Below are the storage options you can configure at an instance level for your jobs. If needed, add/remove/modify the storage logic by editing ComputeNode.sh script to match your requirements.","title":"Instance (local) partitions"},{"location":"storage/backend-storage-options/#root-partition","text":"By default Scale-Out Computing on AWS provision a 10GB EBS disk for the root partition. This may be an issue if you are using a custom AMI configured with a bigger root disk size or if you simply want to allocate additional storage for the root partition. To expand the size of the volume, submit a simulation using -l root_size=<SIZE_IN_GB> parameter. user@host:qsub -l root_size = 25 -- /bin/sleep 600 Result: Root partition is now 25GB user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259 :0 0 25G 0 disk \u251c\u2500nvme0n1p1 259 :1 0 25G 0 part / \u2514\u2500nvme0n1p128 259 :2 0 1M 0 part user@host: df -h / Filesystem Size Used Avail Use% Mounted on /dev/nvme0n1p1 25G 2 .2G 23G 9 % /","title":"Root partition"},{"location":"storage/backend-storage-options/#scratch-partition","text":"Info It's recommended to provision /scratch directory whenever your simulation is I/O intensive. /scratch is a local partition and will be deleted when you job complete. Make sure to copy the job output back to your $HOME /scratch is automatically created when Instance supports local ephemeral storage","title":"Scratch Partition"},{"location":"storage/backend-storage-options/#request-a-scratch-partition-with-ssd-disk","text":"During job submission, specify -l scratch_size=<SIZE_IN_GB> to provision a new EBS disk ( /dev/sdj ) mounted as /scratch user@host: qsub -l scratch_size = 150 -- /bin/sleep 600 Result: a 150 GB /scratch partition is available on all nodes user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 150G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 148G 61M 140G 1 % /scratch To verify the type of your EBS disk, simply go to your AWS bash > EC2 > Volumes and verify your EBS type is \"gp2\" (SSD). Refer to this link for more information about the various EBS types available.","title":"Request a /scratch partition with SSD disk"},{"location":"storage/backend-storage-options/#request-a-scratch-partition-with-io-optimized-disk","text":"To request an optimized SSD disk, use -l scratch_iops=<IOPS> along with -l scratch_size=<SIZE_IN_GB> . Refer to this link to get more details about burstable/IO EBS disks. user@host: qsub -l scratch_iops = 6000 -l scratch_size = 200 -- /bin/sleep 600 Looking at the EBS bash, the disk type is now \"io1\" and the number of IOPS match the value specified at job submission.","title":"Request a /scratch partition with IO optimized disk"},{"location":"storage/backend-storage-options/#instance-store-partition","text":"Free storage is always good You are not charged for instance storage (included in the price of the instance) Some instances come with default instance storage . An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer and is removed as soon as the node is deleted. Scale-Out Computing on AWS automatically detects instance store disk and will use them as /scratch unless you specify -l scratch_size parameter for your job . In this case, Scale-Out Computing on AWS honors the user request and ignore the instance store volume(s).","title":"Instance store partition"},{"location":"storage/backend-storage-options/#when-node-has-1-instance-store-volume","text":"For this example, I will use a \"c5d.9xlarge\" instance which is coming with a 900GB instance store disk. user@host: qsub -l instance_type = c5d.9xlarge -- /bin/sleep 600 Result: Default /scratch partition has been provisioned automatically using local instance storage user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 825G 77M 783G 1 % /scratch","title":"When node has 1 instance store volume"},{"location":"storage/backend-storage-options/#when-node-has-more-than-1-instance-store-volumes","text":"In this special case, ComputeNode.sh script will create a raid0 partition using all instance store volumes available. For this example, I will use a \"m5dn.12xlarge\" instance which is shipped with a 2 * 900GB instance store disks (total 1.8Tb). user@host: qsub -l instance_type = m5dn.12xlarge -- /bin/sleep 600 Result: /scratch is a 1.7TB raid0 partition (using 2 instance store volumes) user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme2n1 259 :1 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme0n1 259 :2 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :3 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :4 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/md127 1 .7T 77M 1 .6T 1 % /scratch","title":"When node has more than 1 instance store volumes"},{"location":"storage/backend-storage-options/#combine-custom-scratch-and-root-size","text":"You can combine parameters as needed. For example, qsub -l root_size=150 -l scratch_size=200 -l nodes=2 will provision 2 nodes with 150GB / and 200GB SSD /scratch","title":"Combine custom scratch and root size"},{"location":"storage/backend-storage-options/#change-the-storage-parameters-at-queue-level","text":"Edit /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml to configure default storage settings at a queue level: queue_type : compute : # /root will be 30 GB and /scratch will be a standard 100GB SSD queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"c5.large\" scratch_size : \"100\" root_size : \"30\" # .. Refer to the doc for more supported parameters memory : # /scratch will be a SSD with provisioned IO queues : [ \"queue4\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"r5.large\" scratch_size : \"300\" scratch_iops : \"5000\" instancestore : # /scratch will use the default instance store queues : [ \"queue5\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"m5dn.12large\" root_size : \"300\"","title":"Change the storage parameters at queue level"},{"location":"storage/launch-job-with-fsx/","text":"What is FSx \u00b6 Amazon FSx provides you with the native compatibility of third-party file systems with feature sets for workloads such as high-performance computing (HPC), machine learning and electronic design automation (EDA). You don\u2019t have to worry about managing file servers and storage, as Amazon FSx automates the time-consuming administration tasks such as hardware provisioning, software configuration, patching, and backups. Amazon FSx provides FSx for Lustre for compute-intensive workloads. Please note the following when using FSx on Scale-Out Computing on AWS FSx is supported natively (Linux clients, security groups and backend configuration is automatically managed by Scale-Out Computing on AWS) You can launch an ephemeral FSx filesystem for your job You can connect to an existing FSx filesystem You can dynamically adjust the storage capacity of your FSx filesystem Exported files (if any) from FSx to S3 will be stored under s3://<YOUR_BUCKET_NAME>/<CLUSTER_ID>-fsxoutput/job-<JOB_ID>/ by default (you can change it if needed) Scale-Out automatically determines the actions to be taken based on the fsx_lustre value you specified during job submission If value is yes/true/on , a standard FSx for Lustre will be provisioned If value starts with s3:// or is a string, SOCA will try to mount the S3 bucket automatically as part of the FSx deployment If value starts with fs-xxx , SOCA will try to mount an existing FSx automatically How to provision an ephemeral FSx \u00b6 To provision an FSx for Lustre without S3 backend, simply specify -l fsx_lustre=True at job submission. If -l fsx_lustre_capacity is not set, the default storage provisioned will be 1.2 TB. The FSx will be mounted under \"/fsx\" by default, you can change this value by referring at the section at the end of this doc. How to provision an ephemeral FSx with S3 backend \u00b6 Pre-requisite \u00b6 S3 Backend This section is only required if you are planning to use S3 as a data backend for FSx You need to give Scale-Out Computing on AWS the permission to map the S3 bucket you want to mount on FSx. To do that, add a new inline policy to the scheduler IAM role . The Scheduler IAM role can be found on the IAM bash and is named <SOCA_AWS_STACK_NAME>-Security-<UUID>-SchedulerIAMRole-<UUID> . To create an inline policy, select your IAM role, click \"Add Inline Policy\": Select \"JSON\" tab Finally copy/paste the JSON policy listed below (make sure to adjust to your bucket name), click \"Review\" and \"Create Policy\". { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowAccessFSxtoS3\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*\" , \"Resource\" : [ \"arn:aws:s3:::<YOUR_BUCKET_NAME>\" , \"arn:aws:s3:::<YOUR_BUCKET_NAME>/*\" ] } ] } To validate your policy is effective, access the scheduler host and run the following commmand: ## Example when IAM policy is not correct user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> An error occurred ( AccessDenied ) when calling the ListObjectsV2 operation: Access Denied ## Example when IAM policy is valid (output will list content of your bucket) user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> 2019 -11-02 04 :26:27 2209 dataset1.txt 2019 -11-02 04 :26:39 10285 dataset2.csv Warning This permission will give scheduler host access to your S3 bucket, therefore you want to limit access to this host to approved users only. DCV sessions or other compute nodes will not have access to the S3 bucket. Setup \u00b6 For this example, let's say I have my dataset available on S3 and I want to access them for my simulation. Submit a job using -l fsx_lustre=s3://<YOUR_BUCKET_NAME> . The bucket will then be mounted on all nodes provisioned for the job under /fsx mountpoint. user@host: qsub -l fsx_lustre = s3://<YOUR_BUCKET_NAME> -- /bin/sleep 600 This command will provision a new 1200 GB (smallest capacity available) FSx filesystem for your job: Your job will automatically start as soon as both your FSx filesystem and compute nodes are available. Your filesystem will be available on all nodes allocated to your job under /fsx user@host: df -h /fsx Filesystem Size Used Avail Use% Mounted on 200 .0.170.60@tcp:/fsx 1 .1T 4 .4M 1 .1T 1 % /fsx ## Verify the content of your bucket is accessible user@host: ls -ltr /fsx total 1 -rwxr-xr-x 1 root root 2209 Nov 2 04 :26 dataset1.txt -rwxr-xr-x 1 root root 10285 Nov 2 04 :26 dataset2.csv You can change the ImportPath / ExportPath by using the following syntax: -l fsx_lustre=<BUCKET>+<EXPORT_PATH>+<IMPORT_PATH> . If <IMPORT_PATH> is not set, value defaults to the bucket root level. The default <EXPORT_PATH> is <BUCKET>/<CLUSTER_ID>-fsxoutput/<JOBID> Your FSx filesystem will automatically be terminated when your job complete. Refer to this link to learn how to interact with FSx data repositories. How to connect to a permanent/existing FSx \u00b6 If you already have a running FSx, you can mount it using -l fsx_lustre variable. user@host: qsub -l fsx_lustre = <MY_FSX_DNS> -- /bin/sleep 60 To retrieve your FSx DNS, select your filesystem and select \"Network & Security\" Warning Make sure your FSx is running on the same VPC as Scale-Out Computing on AWS Make sure your FSx security group allow traffic from/to Scale-Out Computing on AWS ComputeNodes SG If you specify both \"fsx_lustre\" and \"fsx_lustre\", only \"fsx_lustre\" will be mounted. Change FSx capacity \u00b6 Use -l fsx_lustre_size=<SIZE_IN_GB> to specify the size of your FSx filesystem. Please note the following informations: - If not specified, Scale-Out Computing on AWS deploy the smallest possible capacity (1200GB) - Valid sizes (in GB) are 1200, 2400, 3600 and increments of 3600 user@host: qsub -l fsx_lustre_size = 3600 -l fsx_lustre = s3://<YOUR_S3_BUCKET> -- /bin/sleep 600 This command will mount a 3.6TB FSx filesystem on all nodes provisioned for your simulation. How to change the mountpoint \u00b6 By default Scale-Out Computing on AWS mounts fsx on /fsx . If you need to change this value, edit scripts/ComputeNode.sh update the value of FSX_MOUNTPOINT . ... if [[ $SOCA_AWS_fsx_lustre ! = 'false' ]] ; then echo \"FSx request detected, installing FSX Lustre client ... \" FSX_MOUNTPOINT = \"/fsx\" ## <-- Update mountpoint here mkdir -p $FSX_MOUNTPOINT ... Learn about the other storage options on Scale-Out Computing on AWS \u00b6 Click here to learn about the other storage options offered by Scale-Out Computing on AWS. Troubleshooting and most common errors \u00b6 Like any other parameter, FSx options can be debugged using /apps/soca/<CLUSTER_ID>/cluster_manager/logs/<QUEUE_NAME>.log [ Error while trying to create ASG: Scale-Out Computing on AWS does not have access to this bucket. Update IAM policy as described on the documentation ] Resolution : Scale-Out Computing on AWS does not have access to this S3 bucket. Update your IAM role with the policy listed above [ Error while trying to create ASG: fsx_lustre_size must be: 1200 , 2400 , 3600 , 7200 , 10800 ] Resolution : fsx_lustre_size must be 1200, 2400, 3600 and increments of 3600","title":"Launch a job with FSx for Lustre"},{"location":"storage/launch-job-with-fsx/#what-is-fsx","text":"Amazon FSx provides you with the native compatibility of third-party file systems with feature sets for workloads such as high-performance computing (HPC), machine learning and electronic design automation (EDA). You don\u2019t have to worry about managing file servers and storage, as Amazon FSx automates the time-consuming administration tasks such as hardware provisioning, software configuration, patching, and backups. Amazon FSx provides FSx for Lustre for compute-intensive workloads. Please note the following when using FSx on Scale-Out Computing on AWS FSx is supported natively (Linux clients, security groups and backend configuration is automatically managed by Scale-Out Computing on AWS) You can launch an ephemeral FSx filesystem for your job You can connect to an existing FSx filesystem You can dynamically adjust the storage capacity of your FSx filesystem Exported files (if any) from FSx to S3 will be stored under s3://<YOUR_BUCKET_NAME>/<CLUSTER_ID>-fsxoutput/job-<JOB_ID>/ by default (you can change it if needed) Scale-Out automatically determines the actions to be taken based on the fsx_lustre value you specified during job submission If value is yes/true/on , a standard FSx for Lustre will be provisioned If value starts with s3:// or is a string, SOCA will try to mount the S3 bucket automatically as part of the FSx deployment If value starts with fs-xxx , SOCA will try to mount an existing FSx automatically","title":"What is FSx"},{"location":"storage/launch-job-with-fsx/#how-to-provision-an-ephemeral-fsx","text":"To provision an FSx for Lustre without S3 backend, simply specify -l fsx_lustre=True at job submission. If -l fsx_lustre_capacity is not set, the default storage provisioned will be 1.2 TB. The FSx will be mounted under \"/fsx\" by default, you can change this value by referring at the section at the end of this doc.","title":"How to provision an ephemeral FSx"},{"location":"storage/launch-job-with-fsx/#how-to-provision-an-ephemeral-fsx-with-s3-backend","text":"","title":"How to provision an ephemeral FSx with S3 backend"},{"location":"storage/launch-job-with-fsx/#pre-requisite","text":"S3 Backend This section is only required if you are planning to use S3 as a data backend for FSx You need to give Scale-Out Computing on AWS the permission to map the S3 bucket you want to mount on FSx. To do that, add a new inline policy to the scheduler IAM role . The Scheduler IAM role can be found on the IAM bash and is named <SOCA_AWS_STACK_NAME>-Security-<UUID>-SchedulerIAMRole-<UUID> . To create an inline policy, select your IAM role, click \"Add Inline Policy\": Select \"JSON\" tab Finally copy/paste the JSON policy listed below (make sure to adjust to your bucket name), click \"Review\" and \"Create Policy\". { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowAccessFSxtoS3\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*\" , \"Resource\" : [ \"arn:aws:s3:::<YOUR_BUCKET_NAME>\" , \"arn:aws:s3:::<YOUR_BUCKET_NAME>/*\" ] } ] } To validate your policy is effective, access the scheduler host and run the following commmand: ## Example when IAM policy is not correct user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> An error occurred ( AccessDenied ) when calling the ListObjectsV2 operation: Access Denied ## Example when IAM policy is valid (output will list content of your bucket) user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> 2019 -11-02 04 :26:27 2209 dataset1.txt 2019 -11-02 04 :26:39 10285 dataset2.csv Warning This permission will give scheduler host access to your S3 bucket, therefore you want to limit access to this host to approved users only. DCV sessions or other compute nodes will not have access to the S3 bucket.","title":"Pre-requisite"},{"location":"storage/launch-job-with-fsx/#setup","text":"For this example, let's say I have my dataset available on S3 and I want to access them for my simulation. Submit a job using -l fsx_lustre=s3://<YOUR_BUCKET_NAME> . The bucket will then be mounted on all nodes provisioned for the job under /fsx mountpoint. user@host: qsub -l fsx_lustre = s3://<YOUR_BUCKET_NAME> -- /bin/sleep 600 This command will provision a new 1200 GB (smallest capacity available) FSx filesystem for your job: Your job will automatically start as soon as both your FSx filesystem and compute nodes are available. Your filesystem will be available on all nodes allocated to your job under /fsx user@host: df -h /fsx Filesystem Size Used Avail Use% Mounted on 200 .0.170.60@tcp:/fsx 1 .1T 4 .4M 1 .1T 1 % /fsx ## Verify the content of your bucket is accessible user@host: ls -ltr /fsx total 1 -rwxr-xr-x 1 root root 2209 Nov 2 04 :26 dataset1.txt -rwxr-xr-x 1 root root 10285 Nov 2 04 :26 dataset2.csv You can change the ImportPath / ExportPath by using the following syntax: -l fsx_lustre=<BUCKET>+<EXPORT_PATH>+<IMPORT_PATH> . If <IMPORT_PATH> is not set, value defaults to the bucket root level. The default <EXPORT_PATH> is <BUCKET>/<CLUSTER_ID>-fsxoutput/<JOBID> Your FSx filesystem will automatically be terminated when your job complete. Refer to this link to learn how to interact with FSx data repositories.","title":"Setup"},{"location":"storage/launch-job-with-fsx/#how-to-connect-to-a-permanentexisting-fsx","text":"If you already have a running FSx, you can mount it using -l fsx_lustre variable. user@host: qsub -l fsx_lustre = <MY_FSX_DNS> -- /bin/sleep 60 To retrieve your FSx DNS, select your filesystem and select \"Network & Security\" Warning Make sure your FSx is running on the same VPC as Scale-Out Computing on AWS Make sure your FSx security group allow traffic from/to Scale-Out Computing on AWS ComputeNodes SG If you specify both \"fsx_lustre\" and \"fsx_lustre\", only \"fsx_lustre\" will be mounted.","title":"How to connect to a permanent/existing FSx"},{"location":"storage/launch-job-with-fsx/#change-fsx-capacity","text":"Use -l fsx_lustre_size=<SIZE_IN_GB> to specify the size of your FSx filesystem. Please note the following informations: - If not specified, Scale-Out Computing on AWS deploy the smallest possible capacity (1200GB) - Valid sizes (in GB) are 1200, 2400, 3600 and increments of 3600 user@host: qsub -l fsx_lustre_size = 3600 -l fsx_lustre = s3://<YOUR_S3_BUCKET> -- /bin/sleep 600 This command will mount a 3.6TB FSx filesystem on all nodes provisioned for your simulation.","title":"Change FSx capacity"},{"location":"storage/launch-job-with-fsx/#how-to-change-the-mountpoint","text":"By default Scale-Out Computing on AWS mounts fsx on /fsx . If you need to change this value, edit scripts/ComputeNode.sh update the value of FSX_MOUNTPOINT . ... if [[ $SOCA_AWS_fsx_lustre ! = 'false' ]] ; then echo \"FSx request detected, installing FSX Lustre client ... \" FSX_MOUNTPOINT = \"/fsx\" ## <-- Update mountpoint here mkdir -p $FSX_MOUNTPOINT ...","title":"How to change the mountpoint"},{"location":"storage/launch-job-with-fsx/#learn-about-the-other-storage-options-on-scale-out-computing-on-aws","text":"Click here to learn about the other storage options offered by Scale-Out Computing on AWS.","title":"Learn about the other storage options on Scale-Out Computing on AWS"},{"location":"storage/launch-job-with-fsx/#troubleshooting-and-most-common-errors","text":"Like any other parameter, FSx options can be debugged using /apps/soca/<CLUSTER_ID>/cluster_manager/logs/<QUEUE_NAME>.log [ Error while trying to create ASG: Scale-Out Computing on AWS does not have access to this bucket. Update IAM policy as described on the documentation ] Resolution : Scale-Out Computing on AWS does not have access to this S3 bucket. Update your IAM role with the policy listed above [ Error while trying to create ASG: fsx_lustre_size must be: 1200 , 2400 , 3600 , 7200 , 10800 ] Resolution : fsx_lustre_size must be 1200, 2400, 3600 and increments of 3600","title":"Troubleshooting and most common errors"},{"location":"tutorials/create-your-own-queue/","text":"Things to know before you start By default, Scale-Out Computing on AWS creates 4 queues: high, normal (default), low and alwayson. Queue with automatic instance provisioning \u00b6 Create the queue \u00b6 On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Set default compute node - See below for more information Qmgr:set queue <queue_name> default_chunk.compute_node = tbd # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit What is compute_node=tbd On Scale-Out Computing on AWS, unless you configure queue with AlwaysOn instances, nodes will be provisioned based on queue status. When you submit a job, Scale-Out Computing on AWS will automatically provision capacity for this job and compute_node is the scheduler making sure only one job can run on this instance. compute_node=tbd is the default value, making sure any new jobs won't run on existing (if any) nodes Configure automatic host provisioning \u00b6 If you want to enable automatic host provisioning, edit this file: /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml Option1: I want to use the same settings as an existing queue \u00b6 In this case, simply update the array with your new queue queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] # <- Add your queue to the array instance_ami : \"ami-1234567\" instance_type : \"c5.large\" ... Option2: I want to configure specific settings \u00b6 In this case, you will first need to create a new section on the YAML file (see example with memory) queue_type : compute : queues : [ \"queue1\" ] instance_ami : \"ami-1234567\" instance_type : \"c5.large\" scratch_size : \"100\" memory : # <- Add new section queues : [ \"queue2\" ] instance_ami : \"ami-9876543\" instance_type : \"r5.24xlarge\" scratch_size : \"600\" Finally, add a new crontab on the scheduler machine (as root). Use -c to path to the YAML file and -t to the YAML section you just created */3 * * * * source /etc/environment ; /apps/soca/<CLUSTER_ID>/python/latest/bin/python3 /apps/soca/<CLUSTER_ID>/cluster_manager/dispatcher.py -c /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml -t memory Automatic Host provisioning logs \u00b6 All logs queues are stored under /apps/soca/<CLUSTER_ID>/cluster_manager/logs/<queue_name> Queue with AlwaysOn instances \u00b6 Important Scale-Out Computing on AWS automatically created one AlwaysOn queue for you called \"alwayson\" during the first installation In this mode, instances will never be stopped programmatically. You are responsible to terminate the capacity manually by deleting the associated CloudFormation stack Create the queue \u00b6 On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit Start provisioning some capacity \u00b6 Run python3 apps/soca/cluster_manager/add_nodes.py and enable --keep_forever True flag # Launch 1 c5.large always on python3 /apps/soca/<CLUSTER_ID>/cluster_manager/add_nodes.py --instance_type c5.large \\ --desired_capacity 1 \\ --queue <queue_name> \\ --job_name instancealwayson \\ --job_owner mcrozes \\ --keep_forever True IMPORTANT: You specified --keep-forever flag. This instance will be running 24 /7 until you MANUALLY terminate the Cloudformation Stack If you need help with this script, run python3 add_nodes.py -h Delete AlwaysOn capacity \u00b6 Simply go to your CloudFormation console, locate the stack following the naming convention: soca- cluster-name -keepforever- queue_name -uniqueid and terminate it.","title":"Create your own queue"},{"location":"tutorials/create-your-own-queue/#queue-with-automatic-instance-provisioning","text":"","title":"Queue with automatic instance provisioning"},{"location":"tutorials/create-your-own-queue/#create-the-queue","text":"On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Set default compute node - See below for more information Qmgr:set queue <queue_name> default_chunk.compute_node = tbd # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit What is compute_node=tbd On Scale-Out Computing on AWS, unless you configure queue with AlwaysOn instances, nodes will be provisioned based on queue status. When you submit a job, Scale-Out Computing on AWS will automatically provision capacity for this job and compute_node is the scheduler making sure only one job can run on this instance. compute_node=tbd is the default value, making sure any new jobs won't run on existing (if any) nodes","title":"Create the queue"},{"location":"tutorials/create-your-own-queue/#configure-automatic-host-provisioning","text":"If you want to enable automatic host provisioning, edit this file: /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml","title":"Configure automatic host provisioning"},{"location":"tutorials/create-your-own-queue/#option1-i-want-to-use-the-same-settings-as-an-existing-queue","text":"In this case, simply update the array with your new queue queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] # <- Add your queue to the array instance_ami : \"ami-1234567\" instance_type : \"c5.large\" ...","title":"Option1: I want to use the same settings as an existing queue"},{"location":"tutorials/create-your-own-queue/#option2-i-want-to-configure-specific-settings","text":"In this case, you will first need to create a new section on the YAML file (see example with memory) queue_type : compute : queues : [ \"queue1\" ] instance_ami : \"ami-1234567\" instance_type : \"c5.large\" scratch_size : \"100\" memory : # <- Add new section queues : [ \"queue2\" ] instance_ami : \"ami-9876543\" instance_type : \"r5.24xlarge\" scratch_size : \"600\" Finally, add a new crontab on the scheduler machine (as root). Use -c to path to the YAML file and -t to the YAML section you just created */3 * * * * source /etc/environment ; /apps/soca/<CLUSTER_ID>/python/latest/bin/python3 /apps/soca/<CLUSTER_ID>/cluster_manager/dispatcher.py -c /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml -t memory","title":"Option2: I want to configure specific settings"},{"location":"tutorials/create-your-own-queue/#automatic-host-provisioning-logs","text":"All logs queues are stored under /apps/soca/<CLUSTER_ID>/cluster_manager/logs/<queue_name>","title":"Automatic Host provisioning logs"},{"location":"tutorials/create-your-own-queue/#queue-with-alwayson-instances","text":"Important Scale-Out Computing on AWS automatically created one AlwaysOn queue for you called \"alwayson\" during the first installation In this mode, instances will never be stopped programmatically. You are responsible to terminate the capacity manually by deleting the associated CloudFormation stack","title":"Queue with AlwaysOn instances"},{"location":"tutorials/create-your-own-queue/#create-the-queue_1","text":"On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit","title":"Create the queue"},{"location":"tutorials/create-your-own-queue/#start-provisioning-some-capacity","text":"Run python3 apps/soca/cluster_manager/add_nodes.py and enable --keep_forever True flag # Launch 1 c5.large always on python3 /apps/soca/<CLUSTER_ID>/cluster_manager/add_nodes.py --instance_type c5.large \\ --desired_capacity 1 \\ --queue <queue_name> \\ --job_name instancealwayson \\ --job_owner mcrozes \\ --keep_forever True IMPORTANT: You specified --keep-forever flag. This instance will be running 24 /7 until you MANUALLY terminate the Cloudformation Stack If you need help with this script, run python3 add_nodes.py -h","title":"Start provisioning some capacity"},{"location":"tutorials/create-your-own-queue/#delete-alwayson-capacity","text":"Simply go to your CloudFormation console, locate the stack following the naming convention: soca- cluster-name -keepforever- queue_name -uniqueid and terminate it.","title":"Delete AlwaysOn capacity"},{"location":"tutorials/integration-ec2-job-parameters/","text":"Scale-Out Computing on AWS made job submission on EC2 very easy and is fully integrated with EC2. Below is a list of parameters you can specify when you request your simulation to ensure the hardware provisioned will exactly match your simulation requirements. Info If you don't specify them, your job will use the default values configured for your queue (see /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml ) You can use the web-based simulator to generate your qsub command very easily. Compute \u00b6 base_os \u00b6 Description: Reference to the base OS of the AMI you are using Allowed Values: amazonlinux2 centos7 rhel7 Default: If not specified, value default to the OS of the install AMI Examples: -l base_os=centos7 : Instances provisioned will be deployed against CentOS manifest ht_support \u00b6 Disabled by default Description: Enable support for hyper-threading Allowed Value: yes true no false (case insensitive) Examples: -l ht_support=True : Enable hyper-threading for all instances -l ht_support=False : Disable hyper-threading for all instances (default) instance_ami \u00b6 Description: Reference to a custom AMI you want to use Default: If not specified, value default to the AMI specified during installation Examples: -l instance_ami=ami-abcde123 : Capacity provisioned for the job will use the specific AMI Info If your are planning to use an AMI which is not using the same OS as the scheduler, you will need to specify base_os parameter instance_type \u00b6 Description: The type of instance to provision for the simulation Examples: -l instance_type=c5.large : Provision a c5.large for the simulation -l instance_type=c5.large+m5.large : Provision c5.large and m5.large (if needed) for the simulation. Info You can specify multiple instances type using \"+\" sign. When using more than 1 instance type, AWS will prioritize the capacity based on the order (eg: launch c5.large first and switch to m5.large if AWS can't provision c5.large anymore) nodes \u00b6 Description:The number of EC2 instance to provision Examples: -l nodes=5 : Provision 5 EC2 instances spot_allocation_count \u00b6 Description: Specify the number of SPOT instances to launch when provisioning both OD (On Demand) and SPOT instances Allowed Value: Integer Examples: -l nodes=10 -l spot_price=auto -l spot_allocation_count=8 : Provision 10 instances, 2 OD and 8 SPOT with max spot price capped to OD price -l nodes=10 -l spot_price=1.4 -l spot_allocation_count=5 : Provision 10 instances, 5 OD and 5 SPOT with max spot price set to $1.4 -l nodes=10 -l spot_price=auto : Only provision SPOT instances -l nodes=10 : Only provision OD instances Note This parameter is ignored if spot_price is not specified spot_allocation_count must be lower that the total number of nodes you are requesting (eg: you can not do -l nodes=5 -l spot_allocation_count=15 ) spot_allocation_strategy \u00b6 Description: Choose allocation strategy when using multiple SPOT instances type Allowed Value: capacity-optimized or lowest-cost Default Value: lowest-cost Examples: -l spot_allocation_strategy=capacity-optimized : AWS will provision compute nodes based on capacity availabilities spot_price \u00b6 Description: Enable support for SPOT instances Allowed Value: any float value or auto Examples: -l spot_price=auto : Max price will be capped to the On-Demand price -l spot_price=1.4 : Max price you are willing to pay for this instance will be $1.4 an hour. Note spot_price is capped to On-Demand price (e.g: Assuming you are provisioning a t3.medium, AWS will default spot price to 0.418 (OD price) even though you specified -l spot_price=15 ) subnet_id \u00b6 Description: Reference to a subnet ID to use Default: If not specified, value default to one of the three private subnets created during installation Examples: -l subnet_id=sub-123 : Will provision capacity on sub-123 subnet -l subnet_id=sub-123+sub-456+sub-789 : + separated list of private subnets. Specifying more than 1 subnet is useful when requesting large number of instances Note If you specify more than 1 subnet and have placement_group set to True, SOCA will automatically provision capacity and placement group on the first subnet from the list Note Capacity provisioning is limited to private subnets. Storage \u00b6 EBS \u00b6 keep_ebs \u00b6 Disabled by default Description: Retain or not the EBS disks once the simulation is complete Allowed Value: yes true false no (case insensitive) Default Value: False Example: -l keep_ebs=False : (Default) All EBS disks associated to the job will be deleted -l keep_ebs=True : Retain EBS disks after the simulation has terminated (mostly for debugging/troubleshooting procedures) root_size \u00b6 Description: Define the size of the local root volume Unit: GB Example: -l root_size=300 : Provision a 300 GB SSD disk for / (either sda1 or xvda1 ) scratch_size \u00b6 Description: Define the size of the local root volume Unity: GB Example: -l scratch_size=500 : Provision a 500 GB SSD disk for /scratch Info scratch disk is automatically mounted on all nodes associated to the simulation under /scratch instance_store \u00b6 Info SOCA automatically mount instance storage when available. For instances having more than 1 volume, SOCA will create a raid device In all cases, instance store volumes will be mounted on /scratch scratch_iops \u00b6 Description: Define the number of provisioned IOPS to allocate for your /scratch device Unity: IOPS Example: -l scratch_iops=3000 : Your EBS disks provisioned for /scratch will have 3000 dedicated IOPS Info It is recommended to set the IOPs to 3x storage capacity of your EBS disk FSx for Lustre \u00b6 fsx_lustre \u00b6 With no S3 backend \u00b6 Example: -l fsx_lustre=True : Create a new FSx for Lustre and mount it accross all nodes Info FSx partitions are mounted as /fsx . This can be changed if needed If fsx_lustre_size is not specified, default to 1200 GB With S3 backend \u00b6 Example: -l fsx_lustre=my-bucket-name or -l fsx_lustre=s3://my-bucket-name : Create a new FSx for Lustre and mount it across all nodes Info FSx partitions are mounted as /fsx . This can be changed if needed You need to give IAM permission first If not specified, SOCA automatically prefix your bucket name with s3:// If fsx_lustre_size is not specified, default to 1200 GB You can configure custom ImportPath and ExportPath Mount existing FSx \u00b6 Description: Mount an existing FSx to all compute nodes if fsx_lustre points to a FSx filesystem ID Example: -l fsx_lustre=fs-xxxx Info FSx partitions are mounted as /fsx . This can be changed if needed Make sure your FSx for Luster configuration is correct (use SOCA VPC and correct IAM roles) fsx_lustre_size \u00b6 Description: Create an ephemeral FSx for your job and mount the S3 bucket specified Unit: GB Example: -l fsx_lustre_size=3600 : Provision a 3.6TB EFS disk Info If fsx_lustre_size is not specified, default to 1200 GB (smallest size supported) Network \u00b6 efa_support \u00b6 Description: Enable EFA support Allowed Value: yes, true, True Example: -l efa_support=True : Deploy an EFA device on all the nodes Info You must use an EFA compatible instance, otherwise your job will stay in the queue ht_support \u00b6 Disabled by default Description: Enable support for hyper-threading Allowed Value: yes true (case insensitive) Example: -l ht_support=True : Enable hyper-threading for all instances placement_group \u00b6 Enabled by default Description: Disable placement group Allowed Value: yes true (case insensitive) Example: -l placement_group=True : Instances will not use placement groups Info Placement group is enabled by default as long as the number of nodes provisioned is greated than 1 Others \u00b6 anonymous_metrics \u00b6 Default to the value specified during SOCA installation Description: Send anonymous operational metrics to AWS Allowed Value: yes true (case insensitive) Example: -l anonymous_metrics=True How to use custom parameters \u00b6 Example Here is an example about how to use a custom AMI at job or queue level. This example is applicable to all other parameters (simply change the parameter name to the one you one to use). For a single job \u00b6 Use -l instance_ami parameter if you want to only change the AMI for a single job $ qsub -l instance_ami = ami-082b... -- /bin/echo Hello Priority Job resources have the highest priorities. Your job will always use the AMI specified at submission time even if it's different thant the one configure at queue level. For an entire queue \u00b6 Edit /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml and update the default instance_ami parameter if you want all jobs in this queue to use your new AMI: queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"<YOUR_AMI_ID>\" # <- Add your new AMI instance_type : ... root_size : ... scratch_size : ... efa : ... .... View Examples \u00b6","title":"Job customization for EC2"},{"location":"tutorials/integration-ec2-job-parameters/#compute","text":"","title":"Compute"},{"location":"tutorials/integration-ec2-job-parameters/#base_os","text":"Description: Reference to the base OS of the AMI you are using Allowed Values: amazonlinux2 centos7 rhel7 Default: If not specified, value default to the OS of the install AMI Examples: -l base_os=centos7 : Instances provisioned will be deployed against CentOS manifest","title":"base_os"},{"location":"tutorials/integration-ec2-job-parameters/#ht_support","text":"Disabled by default Description: Enable support for hyper-threading Allowed Value: yes true no false (case insensitive) Examples: -l ht_support=True : Enable hyper-threading for all instances -l ht_support=False : Disable hyper-threading for all instances (default)","title":"ht_support"},{"location":"tutorials/integration-ec2-job-parameters/#instance_ami","text":"Description: Reference to a custom AMI you want to use Default: If not specified, value default to the AMI specified during installation Examples: -l instance_ami=ami-abcde123 : Capacity provisioned for the job will use the specific AMI Info If your are planning to use an AMI which is not using the same OS as the scheduler, you will need to specify base_os parameter","title":"instance_ami"},{"location":"tutorials/integration-ec2-job-parameters/#instance_type","text":"Description: The type of instance to provision for the simulation Examples: -l instance_type=c5.large : Provision a c5.large for the simulation -l instance_type=c5.large+m5.large : Provision c5.large and m5.large (if needed) for the simulation. Info You can specify multiple instances type using \"+\" sign. When using more than 1 instance type, AWS will prioritize the capacity based on the order (eg: launch c5.large first and switch to m5.large if AWS can't provision c5.large anymore)","title":"instance_type"},{"location":"tutorials/integration-ec2-job-parameters/#nodes","text":"Description:The number of EC2 instance to provision Examples: -l nodes=5 : Provision 5 EC2 instances","title":"nodes"},{"location":"tutorials/integration-ec2-job-parameters/#spot_allocation_count","text":"Description: Specify the number of SPOT instances to launch when provisioning both OD (On Demand) and SPOT instances Allowed Value: Integer Examples: -l nodes=10 -l spot_price=auto -l spot_allocation_count=8 : Provision 10 instances, 2 OD and 8 SPOT with max spot price capped to OD price -l nodes=10 -l spot_price=1.4 -l spot_allocation_count=5 : Provision 10 instances, 5 OD and 5 SPOT with max spot price set to $1.4 -l nodes=10 -l spot_price=auto : Only provision SPOT instances -l nodes=10 : Only provision OD instances Note This parameter is ignored if spot_price is not specified spot_allocation_count must be lower that the total number of nodes you are requesting (eg: you can not do -l nodes=5 -l spot_allocation_count=15 )","title":"spot_allocation_count"},{"location":"tutorials/integration-ec2-job-parameters/#spot_allocation_strategy","text":"Description: Choose allocation strategy when using multiple SPOT instances type Allowed Value: capacity-optimized or lowest-cost Default Value: lowest-cost Examples: -l spot_allocation_strategy=capacity-optimized : AWS will provision compute nodes based on capacity availabilities","title":"spot_allocation_strategy"},{"location":"tutorials/integration-ec2-job-parameters/#spot_price","text":"Description: Enable support for SPOT instances Allowed Value: any float value or auto Examples: -l spot_price=auto : Max price will be capped to the On-Demand price -l spot_price=1.4 : Max price you are willing to pay for this instance will be $1.4 an hour. Note spot_price is capped to On-Demand price (e.g: Assuming you are provisioning a t3.medium, AWS will default spot price to 0.418 (OD price) even though you specified -l spot_price=15 )","title":"spot_price"},{"location":"tutorials/integration-ec2-job-parameters/#subnet_id","text":"Description: Reference to a subnet ID to use Default: If not specified, value default to one of the three private subnets created during installation Examples: -l subnet_id=sub-123 : Will provision capacity on sub-123 subnet -l subnet_id=sub-123+sub-456+sub-789 : + separated list of private subnets. Specifying more than 1 subnet is useful when requesting large number of instances Note If you specify more than 1 subnet and have placement_group set to True, SOCA will automatically provision capacity and placement group on the first subnet from the list Note Capacity provisioning is limited to private subnets.","title":"subnet_id"},{"location":"tutorials/integration-ec2-job-parameters/#storage","text":"","title":"Storage"},{"location":"tutorials/integration-ec2-job-parameters/#ebs","text":"","title":"EBS"},{"location":"tutorials/integration-ec2-job-parameters/#keep_ebs","text":"Disabled by default Description: Retain or not the EBS disks once the simulation is complete Allowed Value: yes true false no (case insensitive) Default Value: False Example: -l keep_ebs=False : (Default) All EBS disks associated to the job will be deleted -l keep_ebs=True : Retain EBS disks after the simulation has terminated (mostly for debugging/troubleshooting procedures)","title":"keep_ebs"},{"location":"tutorials/integration-ec2-job-parameters/#root_size","text":"Description: Define the size of the local root volume Unit: GB Example: -l root_size=300 : Provision a 300 GB SSD disk for / (either sda1 or xvda1 )","title":"root_size"},{"location":"tutorials/integration-ec2-job-parameters/#scratch_size","text":"Description: Define the size of the local root volume Unity: GB Example: -l scratch_size=500 : Provision a 500 GB SSD disk for /scratch Info scratch disk is automatically mounted on all nodes associated to the simulation under /scratch","title":"scratch_size"},{"location":"tutorials/integration-ec2-job-parameters/#instance_store","text":"Info SOCA automatically mount instance storage when available. For instances having more than 1 volume, SOCA will create a raid device In all cases, instance store volumes will be mounted on /scratch","title":"instance_store"},{"location":"tutorials/integration-ec2-job-parameters/#scratch_iops","text":"Description: Define the number of provisioned IOPS to allocate for your /scratch device Unity: IOPS Example: -l scratch_iops=3000 : Your EBS disks provisioned for /scratch will have 3000 dedicated IOPS Info It is recommended to set the IOPs to 3x storage capacity of your EBS disk","title":"scratch_iops"},{"location":"tutorials/integration-ec2-job-parameters/#fsx-for-lustre","text":"","title":"FSx for Lustre"},{"location":"tutorials/integration-ec2-job-parameters/#fsx_lustre","text":"","title":"fsx_lustre"},{"location":"tutorials/integration-ec2-job-parameters/#with-no-s3-backend","text":"Example: -l fsx_lustre=True : Create a new FSx for Lustre and mount it accross all nodes Info FSx partitions are mounted as /fsx . This can be changed if needed If fsx_lustre_size is not specified, default to 1200 GB","title":"With no S3 backend"},{"location":"tutorials/integration-ec2-job-parameters/#with-s3-backend","text":"Example: -l fsx_lustre=my-bucket-name or -l fsx_lustre=s3://my-bucket-name : Create a new FSx for Lustre and mount it across all nodes Info FSx partitions are mounted as /fsx . This can be changed if needed You need to give IAM permission first If not specified, SOCA automatically prefix your bucket name with s3:// If fsx_lustre_size is not specified, default to 1200 GB You can configure custom ImportPath and ExportPath","title":"With S3 backend"},{"location":"tutorials/integration-ec2-job-parameters/#mount-existing-fsx","text":"Description: Mount an existing FSx to all compute nodes if fsx_lustre points to a FSx filesystem ID Example: -l fsx_lustre=fs-xxxx Info FSx partitions are mounted as /fsx . This can be changed if needed Make sure your FSx for Luster configuration is correct (use SOCA VPC and correct IAM roles)","title":"Mount existing FSx"},{"location":"tutorials/integration-ec2-job-parameters/#fsx_lustre_size","text":"Description: Create an ephemeral FSx for your job and mount the S3 bucket specified Unit: GB Example: -l fsx_lustre_size=3600 : Provision a 3.6TB EFS disk Info If fsx_lustre_size is not specified, default to 1200 GB (smallest size supported)","title":"fsx_lustre_size"},{"location":"tutorials/integration-ec2-job-parameters/#network","text":"","title":"Network"},{"location":"tutorials/integration-ec2-job-parameters/#efa_support","text":"Description: Enable EFA support Allowed Value: yes, true, True Example: -l efa_support=True : Deploy an EFA device on all the nodes Info You must use an EFA compatible instance, otherwise your job will stay in the queue","title":"efa_support"},{"location":"tutorials/integration-ec2-job-parameters/#ht_support_1","text":"Disabled by default Description: Enable support for hyper-threading Allowed Value: yes true (case insensitive) Example: -l ht_support=True : Enable hyper-threading for all instances","title":"ht_support"},{"location":"tutorials/integration-ec2-job-parameters/#placement_group","text":"Enabled by default Description: Disable placement group Allowed Value: yes true (case insensitive) Example: -l placement_group=True : Instances will not use placement groups Info Placement group is enabled by default as long as the number of nodes provisioned is greated than 1","title":"placement_group"},{"location":"tutorials/integration-ec2-job-parameters/#others","text":"","title":"Others"},{"location":"tutorials/integration-ec2-job-parameters/#anonymous_metrics","text":"Default to the value specified during SOCA installation Description: Send anonymous operational metrics to AWS Allowed Value: yes true (case insensitive) Example: -l anonymous_metrics=True","title":"anonymous_metrics"},{"location":"tutorials/integration-ec2-job-parameters/#how-to-use-custom-parameters","text":"Example Here is an example about how to use a custom AMI at job or queue level. This example is applicable to all other parameters (simply change the parameter name to the one you one to use).","title":"How to use custom parameters"},{"location":"tutorials/integration-ec2-job-parameters/#for-a-single-job","text":"Use -l instance_ami parameter if you want to only change the AMI for a single job $ qsub -l instance_ami = ami-082b... -- /bin/echo Hello Priority Job resources have the highest priorities. Your job will always use the AMI specified at submission time even if it's different thant the one configure at queue level.","title":"For a single job"},{"location":"tutorials/integration-ec2-job-parameters/#for-an-entire-queue","text":"Edit /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml and update the default instance_ami parameter if you want all jobs in this queue to use your new AMI: queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"<YOUR_AMI_ID>\" # <- Add your new AMI instance_type : ... root_size : ... scratch_size : ... efa : ... ....","title":"For an entire queue"},{"location":"tutorials/integration-ec2-job-parameters/#view-examples","text":"","title":"View Examples"},{"location":"tutorials/job-licenses-flexlm/","text":"In this page, we will see how Scale-Out Computing on AWS manages job and capacity provisioning based on license availabilities. Example configuration Test settings used for all examples: License Server Hostname: licenses.soca.dev License Server port: 5000 License Daemon port: 5001 Feature to check: Audio_System_Toolbox Scale-Out Computing on AWS cluster name: rctest Firewall Configuration \u00b6 Depending your configuration, you may need to edit the security groups to allow traffic to/from your license servers. FlexLM server installed on Scheduler host No further actions are required if you have installed your FlexLM server on the scheduler host as Scale-Out Computing on AWS automatically whitelist all traffic between the scheduler and the compute nodes. Warning FlexLM configure two ports for each application (DAEMON and SERVER ports). You need to whitelist both of them. Allow traffic from your license server IP to Scale-Out Computing on AWS Assuming my license server IP is 10.0.15.18 , simply go to the EC2 console, locate your Scheduler and ComputeNode security groups (filter by your cluster name) associated to your Scale-Out Computing on AWS cluster and whitelist both SERVER and DAEMON ports: Allow traffic from Scale-Out Computing on AWS to your license server Since FlexLM use client/server protocol, you will need to authorize traffic coming from Scale-Out Computing on AWS to your license servers for both SERVER and DAEMON ports. You will need to whitelist the IP for your scheduler as well as the NAT Gateway used by the compute nodes. Your Scheduler Public IP is listed on CloudFormation, to retrieve your NAT Gateway IP, visit VPC console, select NAT Gateway and find the NAT Gateway IP associated to your Scale-Out Computing on AWS cluster. Upload your lmutil \u00b6 lmutil binary is not included with Scale-Out Computing on AWS. You are required to upload it manually and update /apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py with the location of your file. Note You do not need to install FlexLM server manager. Only lmutil binary is required. lmutil and RHEL based distro FlexLM may requires 32 bits lib depending your system. If launching lmutil returns an ELF version mismatch, simply install yum install redhat-lsb (or equivalent) How to retrieve number of licenses available \u00b6 Scale-Out Computing on AWS includes a script ( /apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py ) which output the number of FlexLM available for a given feature. This script takes the following arguments: -s: The license server hostname -p: The port used by your flexlm deamon -f: The feature name (case sensitive) (Optional) -m: Reserve licenses number for non HPC usage Let say you have 30 Audio_System_Toolbox licenses and 4 are currently in use. The command below will list how many licenses are currently available to use for your jobs: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 26 Now let's say you want to reserve 15 licenses for non HPC/Scale-Out Computing on AWS usage: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox -m 15 11 Info license_check.py is simply a lmutil wrapper. You can get the same output by running the command below and adding some regex validations. You can edit the script to match your own requirements if needed lmutil lmstat -a -c 5000 @licenses-soca.dev | grep \"Users of Audio_System_Toolbox:\" Integration with Scale-Out Computing on AWS \u00b6 IMPORTANT The name of the resource must be *_lic_* . We recommend using <application>_lic_<feature_name> Update your /apps/soca/<CLUSTER_ID>/cluster_manager/settings/licenses_mapping.yml and create a new resource. This file must follow the YAML syntax. # There is no requirements for section names, but I recommend having 1 section = 1 application matlab : matlab_lic_audiosystemtoolbox : \"/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" # Example for other daemons/features comsol : comsol_lic_acoustic : \"/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f ACOUSTICS\" comsol_lic_cadimport : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f CADIMPORT\" synopsys : synopsys_lic_testbenchruntime : \"/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VT_TestbenchRuntime\" synopsys_lic_vcsruntime : \"/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VCSRuntime_Net\" synopsys_lic_vipambaaxisvt : \"/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VIP-AMBA-AXI-SVT\" This parameter will let Scale-Out Computing on AWS knows your license mapping and capacity will only be provisioned if enough licenses are available based on job's requirements. Since you are about to create a new custom resource, additional configuration is required at the scheduler level. On the scheduler host, edit /var/spool/pbs/sched_priv/sched_config and add a new server_dyn_res server_dyn_res: \"matlab_lic_audiosystemtoolbox !/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" On the same file, add your new resource under resources section. This section will not allow a job to run if the amount of assigned resources exceeds the available amount. resources: \"matlab_lic_audiosystemtoolbox, ncpus, mem, arch, host, vnode, aoe, eoe, compute_node\" Finally, edit /var/spool/pbs/server_priv/resourcedef and add your new resource with type=long ... ht_support type = string base_os type = string fsx_lustre_bucket type = string fsx_lustre_size type = string fsx_lustre_dns type = string matlab_lic_audiosystemtoolbox type = long Once done, restart the scheduler using service pbs restart Test \u00b6 For this example, let's assume we do have 3 \"Audio_System_Toolbox\" licenses available /apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 3 Let's try to submit a job which require 5 licenses qsub -l matlab_lic_audiosystemtoolbox = 5 -- /bin/sleep 600 31 .ip-20-0-2-69 Let's check the log files under /apps/soca/<CLUSTER_ID>/cluster_manager/log/<QUEUE_NAME> . Scale-Out Computing on AWS will ignore this job due to the lack of licenses available [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 3}] [157] [INFO] [Next User is mickael] [157] [INFO] [Next Job for user is ['31']] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_31 as we we dont have enough: matlab_lic_audiosystemtoolbox] If you have multiple jobs in the queue, the license counter is dynamically updated each time the dispatcher script is running (every 3 minutes): [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 10}] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 5}] [157] [INFO] [Checking if we have enough resources available to run job_32] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 0}] [157] [INFO] [Checking if we have enough resources available to run job_33] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_33 as we we dont have enough: matlab_lic_audiosystemtoolbox] Scale-Out Computing on AWS ensures licenses provisioned for given jobs are in use before provisioning capacity for new jobs Let say you have 10 licenses available and you submit job1 and job2 which both have a requirement of 5 licenses. Scale-Out Computing on AWS will determine licenses are available and will start provision the capacity. Shortly after you submit job3 which require another 5 licenses. The first 2 jobs may not have started yet, meaning you still have 10 licenses available (even though the 10 licenses will be used by job1 and job2 as soon as they start). In that case we skip job3 until both job1 and job2 and in running state. Invalid Resource You can not submit if you are using an invalid resource (aka: resource not recognized by the scheduler). If you are getting this error, refer to section Integration with Scale-Out Computing on AWS ) qsub -l matlab_lic_fakeresource = 3 -- /bin/sleep 60 qsub: Unknown resource Resource_List.matlab_lic_fakeresource","title":"Job with FlexLM licenses requirements"},{"location":"tutorials/job-licenses-flexlm/#firewall-configuration","text":"Depending your configuration, you may need to edit the security groups to allow traffic to/from your license servers. FlexLM server installed on Scheduler host No further actions are required if you have installed your FlexLM server on the scheduler host as Scale-Out Computing on AWS automatically whitelist all traffic between the scheduler and the compute nodes. Warning FlexLM configure two ports for each application (DAEMON and SERVER ports). You need to whitelist both of them. Allow traffic from your license server IP to Scale-Out Computing on AWS Assuming my license server IP is 10.0.15.18 , simply go to the EC2 console, locate your Scheduler and ComputeNode security groups (filter by your cluster name) associated to your Scale-Out Computing on AWS cluster and whitelist both SERVER and DAEMON ports: Allow traffic from Scale-Out Computing on AWS to your license server Since FlexLM use client/server protocol, you will need to authorize traffic coming from Scale-Out Computing on AWS to your license servers for both SERVER and DAEMON ports. You will need to whitelist the IP for your scheduler as well as the NAT Gateway used by the compute nodes. Your Scheduler Public IP is listed on CloudFormation, to retrieve your NAT Gateway IP, visit VPC console, select NAT Gateway and find the NAT Gateway IP associated to your Scale-Out Computing on AWS cluster.","title":"Firewall Configuration"},{"location":"tutorials/job-licenses-flexlm/#upload-your-lmutil","text":"lmutil binary is not included with Scale-Out Computing on AWS. You are required to upload it manually and update /apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py with the location of your file. Note You do not need to install FlexLM server manager. Only lmutil binary is required. lmutil and RHEL based distro FlexLM may requires 32 bits lib depending your system. If launching lmutil returns an ELF version mismatch, simply install yum install redhat-lsb (or equivalent)","title":"Upload your lmutil"},{"location":"tutorials/job-licenses-flexlm/#how-to-retrieve-number-of-licenses-available","text":"Scale-Out Computing on AWS includes a script ( /apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py ) which output the number of FlexLM available for a given feature. This script takes the following arguments: -s: The license server hostname -p: The port used by your flexlm deamon -f: The feature name (case sensitive) (Optional) -m: Reserve licenses number for non HPC usage Let say you have 30 Audio_System_Toolbox licenses and 4 are currently in use. The command below will list how many licenses are currently available to use for your jobs: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 26 Now let's say you want to reserve 15 licenses for non HPC/Scale-Out Computing on AWS usage: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox -m 15 11 Info license_check.py is simply a lmutil wrapper. You can get the same output by running the command below and adding some regex validations. You can edit the script to match your own requirements if needed lmutil lmstat -a -c 5000 @licenses-soca.dev | grep \"Users of Audio_System_Toolbox:\"","title":"How to retrieve number of licenses available"},{"location":"tutorials/job-licenses-flexlm/#integration-with-scale-out-computing-on-aws","text":"IMPORTANT The name of the resource must be *_lic_* . We recommend using <application>_lic_<feature_name> Update your /apps/soca/<CLUSTER_ID>/cluster_manager/settings/licenses_mapping.yml and create a new resource. This file must follow the YAML syntax. # There is no requirements for section names, but I recommend having 1 section = 1 application matlab : matlab_lic_audiosystemtoolbox : \"/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" # Example for other daemons/features comsol : comsol_lic_acoustic : \"/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f ACOUSTICS\" comsol_lic_cadimport : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f CADIMPORT\" synopsys : synopsys_lic_testbenchruntime : \"/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VT_TestbenchRuntime\" synopsys_lic_vcsruntime : \"/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VCSRuntime_Net\" synopsys_lic_vipambaaxisvt : \"/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VIP-AMBA-AXI-SVT\" This parameter will let Scale-Out Computing on AWS knows your license mapping and capacity will only be provisioned if enough licenses are available based on job's requirements. Since you are about to create a new custom resource, additional configuration is required at the scheduler level. On the scheduler host, edit /var/spool/pbs/sched_priv/sched_config and add a new server_dyn_res server_dyn_res: \"matlab_lic_audiosystemtoolbox !/apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" On the same file, add your new resource under resources section. This section will not allow a job to run if the amount of assigned resources exceeds the available amount. resources: \"matlab_lic_audiosystemtoolbox, ncpus, mem, arch, host, vnode, aoe, eoe, compute_node\" Finally, edit /var/spool/pbs/server_priv/resourcedef and add your new resource with type=long ... ht_support type = string base_os type = string fsx_lustre_bucket type = string fsx_lustre_size type = string fsx_lustre_dns type = string matlab_lic_audiosystemtoolbox type = long Once done, restart the scheduler using service pbs restart","title":"Integration with Scale-Out Computing on AWS"},{"location":"tutorials/job-licenses-flexlm/#test","text":"For this example, let's assume we do have 3 \"Audio_System_Toolbox\" licenses available /apps/soca/<CLUSTER_ID>/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 3 Let's try to submit a job which require 5 licenses qsub -l matlab_lic_audiosystemtoolbox = 5 -- /bin/sleep 600 31 .ip-20-0-2-69 Let's check the log files under /apps/soca/<CLUSTER_ID>/cluster_manager/log/<QUEUE_NAME> . Scale-Out Computing on AWS will ignore this job due to the lack of licenses available [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 3}] [157] [INFO] [Next User is mickael] [157] [INFO] [Next Job for user is ['31']] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_31 as we we dont have enough: matlab_lic_audiosystemtoolbox] If you have multiple jobs in the queue, the license counter is dynamically updated each time the dispatcher script is running (every 3 minutes): [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 10}] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 5}] [157] [INFO] [Checking if we have enough resources available to run job_32] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 0}] [157] [INFO] [Checking if we have enough resources available to run job_33] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_33 as we we dont have enough: matlab_lic_audiosystemtoolbox] Scale-Out Computing on AWS ensures licenses provisioned for given jobs are in use before provisioning capacity for new jobs Let say you have 10 licenses available and you submit job1 and job2 which both have a requirement of 5 licenses. Scale-Out Computing on AWS will determine licenses are available and will start provision the capacity. Shortly after you submit job3 which require another 5 licenses. The first 2 jobs may not have started yet, meaning you still have 10 licenses available (even though the 10 licenses will be used by job1 and job2 as soon as they start). In that case we skip job3 until both job1 and job2 and in running state. Invalid Resource You can not submit if you are using an invalid resource (aka: resource not recognized by the scheduler). If you are getting this error, refer to section Integration with Scale-Out Computing on AWS ) qsub -l matlab_lic_fakeresource = 3 -- /bin/sleep 60 qsub: Unknown resource Resource_List.matlab_lic_fakeresource","title":"Test"},{"location":"tutorials/job-start-stop-email-notification/","text":"In this page, I will show you how to configure email notification when your job start/stop. For this example, I will use Simple Email Service (SES) , but you can use any SMTP provider. Info Note1: By default, the Scale-Out Computing on AWS admin user you created during the installation does not have any associated email address. If you want to use this account you must edit LDAP and add the \"mail\" attribute. Note2: All qmgr command must be executed on the scheduler host Configure SES sender domain \u00b6 Open the SES console and verify your domain (or specific email addresses). For this example I will verify my entire domain (soca.dev) and enable DKIM support to prevent email spoofing. Click 'Verify this domain', you will get list of DNS records to update for verification. Once done, wait a couple of hours and you will receive a confirmation when your DNS are validated. Configure Recipients addresses \u00b6 By default SES limits you to send email to unique recipients which you will need verify manually If you want to be able to send email to any addresses, you need to request production access . Notification code \u00b6 Create a hook file (note: this file can be found under /apps/soca/<CLUSTER_ID>/cluster_hooks/job_notification.py on your Scale-Out Computing on AWS cluster) Edit the following section to match your SES settings ses_sender_email = '<SES_SENDER_EMAIL_ADDRESS_HERE>' ses_region = '<YOUR_SES_REGION_HERE>' Create the hooks \u00b6 Once your script is created, configure your scheduler hooks by running the following commands: user@host: qmgr -c \"create hook notify_job_start event=runjob\" user@host: qmgr -c \"create hook notify_job_complete event=execjob_end\" user@host: qmgr -c \"import hook notify_job_start application/x-python default /apps/soca/<CLUSTER_ID>/cluster_hooks/job_notifications.py\" user@host: qmgr -c \"import hook notify_job_complete application/x-python default /apps/soca/<CLUSTER_ID>/cluster_hooks/job_notifications.py\" Note: If you make any change to the python file, you must re-run the import hook command Test \u00b6 Let's submit a test job which will last 5 minutes qsub -N mytestjob -- /bin/sleep 300 Now let's verify if I received the alerts correctly. Job start: 5 minutes later: Add/Update email \u00b6 Run ldapsearch -x uid=<USER> command to verify if your user has a valid mail attribute and if this attribute is pointing to the correct email address. The example below shows a user without email attribute. user@host: ldapsearch -x uid = mickael ## mickael, People, soca.local dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael To add/update an email address, create a new ldif file (eg: update_email.ldif) and add the following content dn : uid = mickael , ou = People , dc = soca , dc = local changetype : modify add : mail mail : mickael @ soca . dev Then execute the ldapadd as root user@host: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f update_email.ldif modifying entry \"uid=mickael,ou=People,dc=soca,dc=local\" Finally re-run the ldapsearch command and validate your user now has mail attribute user@host: ldapsearch -x uid = mickael dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael mail: mickael@soca.dev Check the logs \u00b6 Scheduler hooks are located: /var/spool/pbs/server_logs/ for notify_job_start on the Scheduler /var/spool/pbs/mom_logs/ for notify_job_complete on the Execution Host(s)","title":"Automatic emails when your job start/stop"},{"location":"tutorials/job-start-stop-email-notification/#configure-ses-sender-domain","text":"Open the SES console and verify your domain (or specific email addresses). For this example I will verify my entire domain (soca.dev) and enable DKIM support to prevent email spoofing. Click 'Verify this domain', you will get list of DNS records to update for verification. Once done, wait a couple of hours and you will receive a confirmation when your DNS are validated.","title":"Configure SES sender domain"},{"location":"tutorials/job-start-stop-email-notification/#configure-recipients-addresses","text":"By default SES limits you to send email to unique recipients which you will need verify manually If you want to be able to send email to any addresses, you need to request production access .","title":"Configure Recipients addresses"},{"location":"tutorials/job-start-stop-email-notification/#notification-code","text":"Create a hook file (note: this file can be found under /apps/soca/<CLUSTER_ID>/cluster_hooks/job_notification.py on your Scale-Out Computing on AWS cluster) Edit the following section to match your SES settings ses_sender_email = '<SES_SENDER_EMAIL_ADDRESS_HERE>' ses_region = '<YOUR_SES_REGION_HERE>'","title":"Notification code"},{"location":"tutorials/job-start-stop-email-notification/#create-the-hooks","text":"Once your script is created, configure your scheduler hooks by running the following commands: user@host: qmgr -c \"create hook notify_job_start event=runjob\" user@host: qmgr -c \"create hook notify_job_complete event=execjob_end\" user@host: qmgr -c \"import hook notify_job_start application/x-python default /apps/soca/<CLUSTER_ID>/cluster_hooks/job_notifications.py\" user@host: qmgr -c \"import hook notify_job_complete application/x-python default /apps/soca/<CLUSTER_ID>/cluster_hooks/job_notifications.py\" Note: If you make any change to the python file, you must re-run the import hook command","title":"Create the hooks"},{"location":"tutorials/job-start-stop-email-notification/#test","text":"Let's submit a test job which will last 5 minutes qsub -N mytestjob -- /bin/sleep 300 Now let's verify if I received the alerts correctly. Job start: 5 minutes later:","title":"Test"},{"location":"tutorials/job-start-stop-email-notification/#addupdate-email","text":"Run ldapsearch -x uid=<USER> command to verify if your user has a valid mail attribute and if this attribute is pointing to the correct email address. The example below shows a user without email attribute. user@host: ldapsearch -x uid = mickael ## mickael, People, soca.local dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael To add/update an email address, create a new ldif file (eg: update_email.ldif) and add the following content dn : uid = mickael , ou = People , dc = soca , dc = local changetype : modify add : mail mail : mickael @ soca . dev Then execute the ldapadd as root user@host: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f update_email.ldif modifying entry \"uid=mickael,ou=People,dc=soca,dc=local\" Finally re-run the ldapsearch command and validate your user now has mail attribute user@host: ldapsearch -x uid = mickael dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael mail: mickael@soca.dev","title":"Add/Update email"},{"location":"tutorials/job-start-stop-email-notification/#check-the-logs","text":"Scheduler hooks are located: /var/spool/pbs/server_logs/ for notify_job_start on the Scheduler /var/spool/pbs/mom_logs/ for notify_job_complete on the Execution Host(s)","title":"Check the logs"},{"location":"tutorials/launch-always-on-instances/","text":"Why AlwaysOn instances? \u00b6 By default, Scale-Out Computing on AWS provision on-demand capacity when there are jobs in the queue. This mean any job submitted will wait in the queue 5 to 8 minutes until EC2 capacity is ready. If you want to avoid this penalty, you can provision \"AlwaysOn instance\". Please note you will be charged until you manually terminate it. How launch an AlwaysOn instance \u00b6 On your scheduler host, sudo as root and run source /etc/environment to load Scale-Out Computing on AWS shell and then execute /apps/soca/<CLUSTER_ID>/cluster_manager/add_nodes.py [ root@ip-40-0-22-232 ~ ] # python3 /apps/soca/<CLUSTER_ID>/cluster_manager/add_nodes.py -h usage: add_nodes.py [ -h ] --instance_type [ INSTANCE_TYPE ] --desired_capacity [ DESIRED_CAPACITY ] --queue [ QUEUE ] [ --instance_ami [ INSTANCE_AMI ]] [ --subnet_id SUBNET_ID ] [ --job_id [ JOB_ID ]] --job_name [ JOB_NAME ] --job_owner [ JOB_OWNER ] [ --job_project [ JOB_PROJECT ]] [ --scratch_size [ SCRATCH_SIZE ]] [ --placement_group PLACEMENT_GROUP ] [ --tags [ TAGS ]] [ --keep_forever ] [ --base_os BASE_OS ] [ --efa ] [ --spot_price [ SPOT_PRICE ]] optional arguments: -h, --help show this help message and exit --instance_type [ INSTANCE_TYPE ] Instance type you want to deploy --desired_capacity [ DESIRED_CAPACITY ] Number of EC2 instances to deploy --queue [ QUEUE ] Queue to map the capacity --instance_ami [ INSTANCE_AMI ] AMI to use --subnet_id SUBNET_ID Launch capacity in a special subnet --job_id [ JOB_ID ] Job ID for which the capacity is being provisioned --job_name [ JOB_NAME ] Job Name for which the capacity is being provisioned --job_owner [ JOB_OWNER ] Job Owner for which the capacity is being provisioned --job_project [ JOB_PROJECT ] Job Owner for which the capacity is being provisioned --scratch_size [ SCRATCH_SIZE ] Size of /scratch in GB --placement_group PLACEMENT_GROUP Enable or disable placement group --tags [ TAGS ] Tags, format must be { 'Key' : 'Value' } --keep_forever Wheter or not capacity will stay forever --base_os BASE_OS Specify custom Base OK --efa Support for EFA --spot_price [ SPOT_PRICE ] Spot Price To enable \"AlwaysOn\" instance, make sure to use --keep_forever tag and use alwayson queue. If you do not want to use alwayson instance, make sure the queue you have created has been configured correctly to support AlwaysOn ( see instructions ) See example below (note: you can use additional parameters if needed) python3 /apps/soca/<CLUSTER_ID>/cluster_manager/add_nodes.py --instance_type = c5.large \\ --desired_capacity = 1 \\ --keep_forever \\ --job_owner mickael --job_name always_on_capacity --queue alwayson When the capacity is available, simply run a job and specify alwayson as queue name How terminate an AlwaysOn instance \u00b6 Simply go to your CloudFormation console, locate the stack following the naming convention: soca-<cluster_name>-keepforever-<queue_name>-uniqueid and terminate it.","title":"Launch AlwaysOn nodes"},{"location":"tutorials/launch-always-on-instances/#why-alwayson-instances","text":"By default, Scale-Out Computing on AWS provision on-demand capacity when there are jobs in the queue. This mean any job submitted will wait in the queue 5 to 8 minutes until EC2 capacity is ready. If you want to avoid this penalty, you can provision \"AlwaysOn instance\". Please note you will be charged until you manually terminate it.","title":"Why AlwaysOn instances?"},{"location":"tutorials/launch-always-on-instances/#how-launch-an-alwayson-instance","text":"On your scheduler host, sudo as root and run source /etc/environment to load Scale-Out Computing on AWS shell and then execute /apps/soca/<CLUSTER_ID>/cluster_manager/add_nodes.py [ root@ip-40-0-22-232 ~ ] # python3 /apps/soca/<CLUSTER_ID>/cluster_manager/add_nodes.py -h usage: add_nodes.py [ -h ] --instance_type [ INSTANCE_TYPE ] --desired_capacity [ DESIRED_CAPACITY ] --queue [ QUEUE ] [ --instance_ami [ INSTANCE_AMI ]] [ --subnet_id SUBNET_ID ] [ --job_id [ JOB_ID ]] --job_name [ JOB_NAME ] --job_owner [ JOB_OWNER ] [ --job_project [ JOB_PROJECT ]] [ --scratch_size [ SCRATCH_SIZE ]] [ --placement_group PLACEMENT_GROUP ] [ --tags [ TAGS ]] [ --keep_forever ] [ --base_os BASE_OS ] [ --efa ] [ --spot_price [ SPOT_PRICE ]] optional arguments: -h, --help show this help message and exit --instance_type [ INSTANCE_TYPE ] Instance type you want to deploy --desired_capacity [ DESIRED_CAPACITY ] Number of EC2 instances to deploy --queue [ QUEUE ] Queue to map the capacity --instance_ami [ INSTANCE_AMI ] AMI to use --subnet_id SUBNET_ID Launch capacity in a special subnet --job_id [ JOB_ID ] Job ID for which the capacity is being provisioned --job_name [ JOB_NAME ] Job Name for which the capacity is being provisioned --job_owner [ JOB_OWNER ] Job Owner for which the capacity is being provisioned --job_project [ JOB_PROJECT ] Job Owner for which the capacity is being provisioned --scratch_size [ SCRATCH_SIZE ] Size of /scratch in GB --placement_group PLACEMENT_GROUP Enable or disable placement group --tags [ TAGS ] Tags, format must be { 'Key' : 'Value' } --keep_forever Wheter or not capacity will stay forever --base_os BASE_OS Specify custom Base OK --efa Support for EFA --spot_price [ SPOT_PRICE ] Spot Price To enable \"AlwaysOn\" instance, make sure to use --keep_forever tag and use alwayson queue. If you do not want to use alwayson instance, make sure the queue you have created has been configured correctly to support AlwaysOn ( see instructions ) See example below (note: you can use additional parameters if needed) python3 /apps/soca/<CLUSTER_ID>/cluster_manager/add_nodes.py --instance_type = c5.large \\ --desired_capacity = 1 \\ --keep_forever \\ --job_owner mickael --job_name always_on_capacity --queue alwayson When the capacity is available, simply run a job and specify alwayson as queue name","title":"How launch an AlwaysOn instance"},{"location":"tutorials/launch-always-on-instances/#how-terminate-an-alwayson-instance","text":"Simply go to your CloudFormation console, locate the stack following the naming convention: soca-<cluster_name>-keepforever-<queue_name>-uniqueid and terminate it.","title":"How terminate an AlwaysOn instance"},{"location":"tutorials/launch-your-first-job/","text":"Submit your job \u00b6 Things to know before you start Jobs start on average 5 minutes after submission (this value may differ depending on the number and type of compute resource you need to be provisioned). You can reduce this cold-start by pre-configuring your AMI Nodes are ephemeral and tie to a given job id. If needed, you can launch 'AlwaysOn' instances that will be running 24/7. If your simulation requires a lot of disk I/O, it's recommended to use high performance SSD-NVMe disks (using /scratch location) and not default $HOME path Use the web-based simulator to generate your qsub/script command. To get started, create a simple text file and name it \"job_submit.que\". See below for a simple template (you will be required to edit whatever is between **) # !/bin/bash # # BEGIN PBS SETTINGS: Note PBS lines MUST start with # # PBS -N **your_job_name** # PBS -V -j oe -o **your_job_name**.qlog # PBS -P **your_project** # PBS -q **your_queue** # PBS -l nodes = **number_of_nodes_for_this_job** # # END PBS SETTINGS # # BEGIN ACTUAL CODE ** your code goes here ** # # END ACTUAL CODE Run your job \u00b6 Run qsub job_submit.que to submit your job to the queue. user@host:~$ qsub job_submit.que 3323.ip-10-10-10-28 If your qsub command succeed, you will receive an id for your job (3323 in this example). To get more information about this job, run qstat -f 3323 (or qstat -f 3323 -x is the job is already terminated). Your job will start as soon as resources are available (usually within 5 minutes after job submission) Delete a job from the queue \u00b6 Run qdel <job_id> to remove a job from the queue. If the job was running, associated capacity will be terminated within 4 minutes. Custom AWS scheduler resources (optional) \u00b6 Here is a list of scheduler resources specially designed for workloads running on AWS. The line starting with -l (lowercase L) is meant to define scheduler resources which will be used by this job. Syntax is as follow: In a script: #PBS -l parameter_name=parameter_value,parameter_name_2=parameter_value_2 Using qsub: qsub -l parameter_name=parameter_value -l parameter_name_2=parameter_value_2 myscript.sh Info If you don't specify them, your job will use the default values configured for your queue (see /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml ) Specify an EC2 Instance Type (optional) \u00b6 Scale-Out Computing on AWS supports all type of EC2 instance. If you don't specify it, job will use a default type which may not be optimal (eg: simulation is memory intensive but default EC2 is compute optimized) If you are not familiar with EC2 instances, take some time to review https://aws.amazon.com/ec2/instance-types/ If you want to force utilization of a specific instance type (and not use the default one), simply change the line and modify instance_type value #PBS -l [existing_parameters...],instance_type=**instance_type_value** Specify a license restriction (optional) \u00b6 License Mapping Please refer to /apps/soca/ /cluster_manager/settings/licenses_mapping.yml for a list of licenses you can restrict. Contact your Administrator if your license is not available yet. If your job needs to check-out a specific license to run, you want to make sure enough licenses are available before provisioning capacity for the job. To do so, you can add a new resource which will be your license name and the number of license you need. Example: Your job will only start if we have at least 2 Synopsys VCSRuntime_Net licenses available. #PBS -l [existing_parameters...],synopsys_lic_vcsruntimenet=2 Manage your application logs \u00b6 PBS will automatically generate a .qlog file once the job is complete as shown below. #PBS -V -j oe -o **your_job_name**.qlog If you need more verbose log, we recommend you using STDERR/STDOUT redirection on your code My job is queued. What next? (AWS orchestration) \u00b6 First, let's make sure your jobs have been sent to the queue. You can run default qstat or use aligoqstat which is a custom wrapper developed for Scale-Out Computing on AWS. Web Based CLI As soon as jobs are sent to the queue, our in-house dispatcher script which will decide if the job can start based on hardware availabilities, priorities or license requirements. Run qstat -f **job_id** | grep Resource . Web Based CLI If you see stack_id or compute_node resource (under select), that means all requirements are met and capacity is being provisioned (aka: CloudFormation stack is created and capacity is being provisioned). Look at your EC2 console. This is what you will see (syntax is **cluster_id**-compute-node-**job_id** ): Instances are being provisioned successfully, now let's make sure they are correctly being added to the scheduler by running pbsnodes -a Note: PBS is updated as soon as the host are being added to EC2. You will need to wait a couple of minutes before the state change from \"down\" to \"free\" as Scale-Out Computing on AWS has to configure each node (install libraries, scheduler ...) user @host : ~ $ pbsnodes - a #Host Ready ip - 90 - 0 - 118 - 49 Mom = ip - 90 - 0 - 118 - 49. us - west - 2. compute . internal ntype = PBS state = free pcpus = 16 jobs = 1. ip - 90 - 0 - 24 - 214 / 0 resources_available . arch = linux resources_available . availability_zone = us - west - 2 a resources_available . compute_node = job1 resources_available . host = ip - 90 - 0 - 118 - 49 resources_available . instance_type = c5 .4 xlarge resources_available . mem = 31890060 kb resources_available . ncpus = 16 resources_available . subnet_id = subnet - 055 c0dcdd6ddbb020 resources_available . vnode = ip - 90 - 0 - 118 - 49 resources_assigned . accelerator_memory = 0 kb resources_assigned . hbmem = 0 kb resources_assigned . mem = 0 kb resources_assigned . naccelerators = 0 resources_assigned . ncpus = 1 resources_assigned . vmem = 0 kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23 : 30 : 05 2019 # Host not ready yet ip - 90 - 0 - 188 - 37 Mom = ip - 90 - 0 - 188 - 37. us - west - 2. compute . internal ntype = PBS state = state - unknown , down resources_available . availability_zone = us - west - 2 c resources_available . compute_node = job2 resources_available . host = ip - 90 - 0 - 188 - 37 resources_available . instance_type = r5 . xlarge resources_available . subnet_id = subnet - 0 d046c8668ccfdcb0 resources_available . vnode = ip - 90 - 0 - 188 - 37 resources_assigned . accelerator_memory = 0 kb resources_assigned . hbmem = 0 kb resources_assigned . mem = 0 kb resources_assigned . naccelerators = 0 resources_assigned . ncpus = 0 resources_assigned . vmem = 0 kb queue = normal comment = node down : communication closed resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23 : 28 : 05 2019 ` Simply wait a couple of minutes. Your jobs will start as soon as the PBS nodes are configured. The web ui will also reflect this change. Examples \u00b6 Job Submission Simulator Use the web-based simulator to generate your qsub/script command. How to set a parameter In a script: #PBS -l parameter_name=parameter_value,parameter_name_2=parameter_value_2 Using qsub: qsub -l parameter_name=parameter_value -l parameter_name_2=parameter_value_2 myscript.sh Refer to this page to get a list of all supported parameters For the rest of the examples below, I will run a simple script named \"script.sh\" with the following content: #!/bin/bash # Will output the hostname of the host where the script is executed # If using MPI (more than 1 node), you will get the hostname of all the hosts allocated for your job echo ` hostname ` Run a simple script on 1 node using default settings on 'normal' queue \u00b6 #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1 Run a simple script on 1 node using default settings on 'normal' queue \u00b6 #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1 Run a simple MPI script on 3 nodes using custom EC2 instance type \u00b6 This job will use a 3 c5.18xlarge instances #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log Run a simple script on 3 nodes using custom License Restriction \u00b6 This job will only start if we have at least 4 Comsol Acoustic licenses available #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge,comsol_lic_acoustic=4 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log Run a simple script on 5 nodes using custom AMI \u00b6 This job will use a user-specified AMI ID #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Run a simple script on 5 nodes using custom AMI using a different OS \u00b6 This job will use a user-specified AMI ID which use a operating system different than the scheduler #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde,base_os=centos7 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Run a simple script on 5 m5.24xlarge SPOT instances as long as instance price is lower than $2.5 per hour \u00b6 This job will use SPOT instances. Instances will be automatically terminated if BID price is higher than $2.5 / per hour per instance #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=2.5 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # m5.24xlarge is 48 cores so -np is 48 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh > my_output.log Run a simple script on 5 m5.24xlarge SPOT instances as long as instance price is lower than OD price \u00b6 #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=auto ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # m5.24xlarge is 48 cores so -np is 48 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh > my_output.log Submit a job with EFA \u00b6 Make sure to use an instance type supported by EFA https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=true ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Use 50 c5.xlarge for your job and fallback to m5.xlarge and r5.xlarge if capacity is not available \u00b6 AWS honors the instance order, so it will try to provision 50 c5.large first and fallback to m5.xlarge/r5.xlarge if needed (in case your account has instance limitation or AWS can't allocate more than X instance type on a given AZ/region). Ultimately, you may end up with the following configuration (but not limited to): 50 c5.xlarge 30 c5.xlarge, 20 m5.xlarge 20 c5.xlarge, 20 m5.xlarge, 10 r5.xlarge Or any other combination. The only certain know is that you will get 50 instances #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=50,instance_type=c5.xlarge+m5.xlarge+r5.xlarge ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Use multiple SPOT instance type \u00b6 #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.xlarge+m5.xlarge+r5.xlarge, spot_price=auto ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Provision 50 instances (10 On-Demand and 40 SPOT) \u00b6 #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=50,instance_type=c5.large,spot_allocation_count=40 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Multi-lines parameters \u00b6 Custom AMI running on a different distribution than the scheduler, with EFA enable, without placement group and within a specific subnet_id #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal ## Resources can be specified on multiple lines #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=yes #PBS -l placement_group=false,base_os=rhel7,ami_id=ami-12345,subnet_id=sub-abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Launch your first job"},{"location":"tutorials/launch-your-first-job/#submit-your-job","text":"Things to know before you start Jobs start on average 5 minutes after submission (this value may differ depending on the number and type of compute resource you need to be provisioned). You can reduce this cold-start by pre-configuring your AMI Nodes are ephemeral and tie to a given job id. If needed, you can launch 'AlwaysOn' instances that will be running 24/7. If your simulation requires a lot of disk I/O, it's recommended to use high performance SSD-NVMe disks (using /scratch location) and not default $HOME path Use the web-based simulator to generate your qsub/script command. To get started, create a simple text file and name it \"job_submit.que\". See below for a simple template (you will be required to edit whatever is between **) # !/bin/bash # # BEGIN PBS SETTINGS: Note PBS lines MUST start with # # PBS -N **your_job_name** # PBS -V -j oe -o **your_job_name**.qlog # PBS -P **your_project** # PBS -q **your_queue** # PBS -l nodes = **number_of_nodes_for_this_job** # # END PBS SETTINGS # # BEGIN ACTUAL CODE ** your code goes here ** # # END ACTUAL CODE","title":"Submit your job"},{"location":"tutorials/launch-your-first-job/#run-your-job","text":"Run qsub job_submit.que to submit your job to the queue. user@host:~$ qsub job_submit.que 3323.ip-10-10-10-28 If your qsub command succeed, you will receive an id for your job (3323 in this example). To get more information about this job, run qstat -f 3323 (or qstat -f 3323 -x is the job is already terminated). Your job will start as soon as resources are available (usually within 5 minutes after job submission)","title":"Run your job"},{"location":"tutorials/launch-your-first-job/#delete-a-job-from-the-queue","text":"Run qdel <job_id> to remove a job from the queue. If the job was running, associated capacity will be terminated within 4 minutes.","title":"Delete a job from the queue"},{"location":"tutorials/launch-your-first-job/#custom-aws-scheduler-resources-optional","text":"Here is a list of scheduler resources specially designed for workloads running on AWS. The line starting with -l (lowercase L) is meant to define scheduler resources which will be used by this job. Syntax is as follow: In a script: #PBS -l parameter_name=parameter_value,parameter_name_2=parameter_value_2 Using qsub: qsub -l parameter_name=parameter_value -l parameter_name_2=parameter_value_2 myscript.sh Info If you don't specify them, your job will use the default values configured for your queue (see /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml )","title":"Custom AWS scheduler resources (optional)"},{"location":"tutorials/launch-your-first-job/#specify-an-ec2-instance-type-optional","text":"Scale-Out Computing on AWS supports all type of EC2 instance. If you don't specify it, job will use a default type which may not be optimal (eg: simulation is memory intensive but default EC2 is compute optimized) If you are not familiar with EC2 instances, take some time to review https://aws.amazon.com/ec2/instance-types/ If you want to force utilization of a specific instance type (and not use the default one), simply change the line and modify instance_type value #PBS -l [existing_parameters...],instance_type=**instance_type_value**","title":"Specify an EC2 Instance Type (optional)"},{"location":"tutorials/launch-your-first-job/#specify-a-license-restriction-optional","text":"License Mapping Please refer to /apps/soca/ /cluster_manager/settings/licenses_mapping.yml for a list of licenses you can restrict. Contact your Administrator if your license is not available yet. If your job needs to check-out a specific license to run, you want to make sure enough licenses are available before provisioning capacity for the job. To do so, you can add a new resource which will be your license name and the number of license you need. Example: Your job will only start if we have at least 2 Synopsys VCSRuntime_Net licenses available. #PBS -l [existing_parameters...],synopsys_lic_vcsruntimenet=2","title":"Specify a license restriction (optional)"},{"location":"tutorials/launch-your-first-job/#manage-your-application-logs","text":"PBS will automatically generate a .qlog file once the job is complete as shown below. #PBS -V -j oe -o **your_job_name**.qlog If you need more verbose log, we recommend you using STDERR/STDOUT redirection on your code","title":"Manage your application logs"},{"location":"tutorials/launch-your-first-job/#my-job-is-queued-what-next-aws-orchestration","text":"First, let's make sure your jobs have been sent to the queue. You can run default qstat or use aligoqstat which is a custom wrapper developed for Scale-Out Computing on AWS. Web Based CLI As soon as jobs are sent to the queue, our in-house dispatcher script which will decide if the job can start based on hardware availabilities, priorities or license requirements. Run qstat -f **job_id** | grep Resource . Web Based CLI If you see stack_id or compute_node resource (under select), that means all requirements are met and capacity is being provisioned (aka: CloudFormation stack is created and capacity is being provisioned). Look at your EC2 console. This is what you will see (syntax is **cluster_id**-compute-node-**job_id** ): Instances are being provisioned successfully, now let's make sure they are correctly being added to the scheduler by running pbsnodes -a Note: PBS is updated as soon as the host are being added to EC2. You will need to wait a couple of minutes before the state change from \"down\" to \"free\" as Scale-Out Computing on AWS has to configure each node (install libraries, scheduler ...) user @host : ~ $ pbsnodes - a #Host Ready ip - 90 - 0 - 118 - 49 Mom = ip - 90 - 0 - 118 - 49. us - west - 2. compute . internal ntype = PBS state = free pcpus = 16 jobs = 1. ip - 90 - 0 - 24 - 214 / 0 resources_available . arch = linux resources_available . availability_zone = us - west - 2 a resources_available . compute_node = job1 resources_available . host = ip - 90 - 0 - 118 - 49 resources_available . instance_type = c5 .4 xlarge resources_available . mem = 31890060 kb resources_available . ncpus = 16 resources_available . subnet_id = subnet - 055 c0dcdd6ddbb020 resources_available . vnode = ip - 90 - 0 - 118 - 49 resources_assigned . accelerator_memory = 0 kb resources_assigned . hbmem = 0 kb resources_assigned . mem = 0 kb resources_assigned . naccelerators = 0 resources_assigned . ncpus = 1 resources_assigned . vmem = 0 kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23 : 30 : 05 2019 # Host not ready yet ip - 90 - 0 - 188 - 37 Mom = ip - 90 - 0 - 188 - 37. us - west - 2. compute . internal ntype = PBS state = state - unknown , down resources_available . availability_zone = us - west - 2 c resources_available . compute_node = job2 resources_available . host = ip - 90 - 0 - 188 - 37 resources_available . instance_type = r5 . xlarge resources_available . subnet_id = subnet - 0 d046c8668ccfdcb0 resources_available . vnode = ip - 90 - 0 - 188 - 37 resources_assigned . accelerator_memory = 0 kb resources_assigned . hbmem = 0 kb resources_assigned . mem = 0 kb resources_assigned . naccelerators = 0 resources_assigned . ncpus = 0 resources_assigned . vmem = 0 kb queue = normal comment = node down : communication closed resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23 : 28 : 05 2019 ` Simply wait a couple of minutes. Your jobs will start as soon as the PBS nodes are configured. The web ui will also reflect this change.","title":"My job is queued. What next? (AWS orchestration)"},{"location":"tutorials/launch-your-first-job/#examples","text":"Job Submission Simulator Use the web-based simulator to generate your qsub/script command. How to set a parameter In a script: #PBS -l parameter_name=parameter_value,parameter_name_2=parameter_value_2 Using qsub: qsub -l parameter_name=parameter_value -l parameter_name_2=parameter_value_2 myscript.sh Refer to this page to get a list of all supported parameters For the rest of the examples below, I will run a simple script named \"script.sh\" with the following content: #!/bin/bash # Will output the hostname of the host where the script is executed # If using MPI (more than 1 node), you will get the hostname of all the hosts allocated for your job echo ` hostname `","title":"Examples"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-1-node-using-default-settings-on-normal-queue","text":"#!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1","title":"Run a simple script on 1 node using default settings on 'normal' queue"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-1-node-using-default-settings-on-normal-queue_1","text":"#!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1","title":"Run a simple script on 1 node using default settings on 'normal' queue"},{"location":"tutorials/launch-your-first-job/#run-a-simple-mpi-script-on-3-nodes-using-custom-ec2-instance-type","text":"This job will use a 3 c5.18xlarge instances #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log","title":"Run a simple MPI script on 3 nodes using custom EC2 instance type"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-3-nodes-using-custom-license-restriction","text":"This job will only start if we have at least 4 Comsol Acoustic licenses available #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge,comsol_lic_acoustic=4 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log","title":"Run a simple script on 3 nodes using custom License Restriction"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-nodes-using-custom-ami","text":"This job will use a user-specified AMI ID #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Run a simple script on 5 nodes using custom AMI"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-nodes-using-custom-ami-using-a-different-os","text":"This job will use a user-specified AMI ID which use a operating system different than the scheduler #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde,base_os=centos7 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Run a simple script on 5 nodes using custom AMI using a different OS"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-m524xlarge-spot-instances-as-long-as-instance-price-is-lower-than-25-per-hour","text":"This job will use SPOT instances. Instances will be automatically terminated if BID price is higher than $2.5 / per hour per instance #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=2.5 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # m5.24xlarge is 48 cores so -np is 48 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh > my_output.log","title":"Run a simple script on 5 m5.24xlarge SPOT instances as long as instance price is lower than $2.5 per hour"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-m524xlarge-spot-instances-as-long-as-instance-price-is-lower-than-od-price","text":"#!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=auto ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # m5.24xlarge is 48 cores so -np is 48 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh > my_output.log","title":"Run a simple script on 5 m5.24xlarge SPOT instances as long as instance price is lower than OD price"},{"location":"tutorials/launch-your-first-job/#submit-a-job-with-efa","text":"Make sure to use an instance type supported by EFA https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=true ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Submit a job with EFA"},{"location":"tutorials/launch-your-first-job/#use-50-c5xlarge-for-your-job-and-fallback-to-m5xlarge-and-r5xlarge-if-capacity-is-not-available","text":"AWS honors the instance order, so it will try to provision 50 c5.large first and fallback to m5.xlarge/r5.xlarge if needed (in case your account has instance limitation or AWS can't allocate more than X instance type on a given AZ/region). Ultimately, you may end up with the following configuration (but not limited to): 50 c5.xlarge 30 c5.xlarge, 20 m5.xlarge 20 c5.xlarge, 20 m5.xlarge, 10 r5.xlarge Or any other combination. The only certain know is that you will get 50 instances #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=50,instance_type=c5.xlarge+m5.xlarge+r5.xlarge ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Use 50 c5.xlarge for your job and fallback to m5.xlarge and r5.xlarge if capacity is not available"},{"location":"tutorials/launch-your-first-job/#use-multiple-spot-instance-type","text":"#!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.xlarge+m5.xlarge+r5.xlarge, spot_price=auto ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Use multiple SPOT instance type"},{"location":"tutorials/launch-your-first-job/#provision-50-instances-10-on-demand-and-40-spot","text":"#!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=50,instance_type=c5.large,spot_allocation_count=40 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Provision 50 instances (10 On-Demand and 40 SPOT)"},{"location":"tutorials/launch-your-first-job/#multi-lines-parameters","text":"Custom AMI running on a different distribution than the scheduler, with EFA enable, without placement group and within a specific subnet_id #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal ## Resources can be specified on multiple lines #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=yes #PBS -l placement_group=false,base_os=rhel7,ami_id=ami-12345,subnet_id=sub-abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Multi-lines parameters"},{"location":"tutorials/manage-ldap-users/","text":"Using Web UI \u00b6 Log in to the Web UI with an admin account and locate \"Users\" section on the left sidebar Add users \u00b6 To create a new user, simply fill out the \"Create New User\" form. Select whether or not the user will be an admin by checking \"Enable Sudo Access\" checkbox You will see a success message if the user is created correctly What is a SUDO user? Users will SUDO permissions will be admin on the cluster and authorized to run any sudo command. Make sure to limit this ability to HPC/AWS/Linux admins and other power users. Delete users \u00b6 To delete a user, select the user you want to delete and check the checkbox You will see a success message if the user is deleted correctly Non-Admins users Users without \"Sudo\" are not authorized to manage LDAP accounts. Using command-line interface \u00b6 If you need to manage the permission programatically, access the scheduler host and execute /apps/soca/<CLUSTER_ID>/cluster_manager/ldap_manager.py python3 /apps/soca/<CLUSTER_ID>/cluster_manager/ldap_manager.py add-user -u newuser -p mynottoosecurepassword Created User: newuser id: 5002 Created group successfully Home directory created correctly Users created via CLI are visible to the web-ui and vice versa Other LDAP operations \u00b6 Scale-Out Computing on AWS uses OpenLDAP and you can interact with your directory using LDIF directly. Scale-Out Computing on AWS LDAP Schema People: OU=People,DC=soca,DC=local Groups: OU=Group,DC=soca,DC=local Sudoers: OU=Sudoers,DC=soca,DC=local (This OU manages sudo permission on the cluster) Admin LDAP account credentials Bind DN (-D): cn=admin,dc=soca,dc=local Password (-y) /root/OpenLdapAdminPassword.txt For example, if you want to create a new group, create a new LDIF file (mynewgroup.ldif) and add the following content: dn : cn = mynewgroup , ou = Group , dc = soca , dc = local objectClass : top objectClass : posixGroup cn : mynewgroup gidNumber : 6000 memberUid : mytestuser Run the following ldapadd command to add your new group: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif adding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\" Finally valid your group has been created correctly using ldapsearch # Validate with Ldapsearch ~ ldapsearch -x cn = mynewgroup #Extended LDIF # # LDAPv3 # base DC=soca,DC=local (default) with scope subtree # filter: cn=mynewgroup # requesting: ALL # # mynewgroup, Group, soca.local dn: cn = mynewgroup,ou = Group,dc = soca,dc = local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: mytestuser Example for LDIF modify operation dn : cn = mynewgroup , ou = Group , dc = soca , dc = local changetype : modify add : memberUid memberUid : anotheruser Example for LDIF delete operation dn : cn = mynewgroup , ou = Group , dc = soca , dc = local changetype : modify delete : memberUid memberUid :: anotheruser # you get the memberUid by running a simple ldapsearch first Give users permissions to submit job \u00b6 By default, users can submit job to any queue, however you can set up ACL at queue level if needed","title":"Centralized user management"},{"location":"tutorials/manage-ldap-users/#using-web-ui","text":"Log in to the Web UI with an admin account and locate \"Users\" section on the left sidebar","title":"Using Web UI"},{"location":"tutorials/manage-ldap-users/#add-users","text":"To create a new user, simply fill out the \"Create New User\" form. Select whether or not the user will be an admin by checking \"Enable Sudo Access\" checkbox You will see a success message if the user is created correctly What is a SUDO user? Users will SUDO permissions will be admin on the cluster and authorized to run any sudo command. Make sure to limit this ability to HPC/AWS/Linux admins and other power users.","title":"Add users"},{"location":"tutorials/manage-ldap-users/#delete-users","text":"To delete a user, select the user you want to delete and check the checkbox You will see a success message if the user is deleted correctly Non-Admins users Users without \"Sudo\" are not authorized to manage LDAP accounts.","title":"Delete users"},{"location":"tutorials/manage-ldap-users/#using-command-line-interface","text":"If you need to manage the permission programatically, access the scheduler host and execute /apps/soca/<CLUSTER_ID>/cluster_manager/ldap_manager.py python3 /apps/soca/<CLUSTER_ID>/cluster_manager/ldap_manager.py add-user -u newuser -p mynottoosecurepassword Created User: newuser id: 5002 Created group successfully Home directory created correctly Users created via CLI are visible to the web-ui and vice versa","title":"Using command-line interface"},{"location":"tutorials/manage-ldap-users/#other-ldap-operations","text":"Scale-Out Computing on AWS uses OpenLDAP and you can interact with your directory using LDIF directly. Scale-Out Computing on AWS LDAP Schema People: OU=People,DC=soca,DC=local Groups: OU=Group,DC=soca,DC=local Sudoers: OU=Sudoers,DC=soca,DC=local (This OU manages sudo permission on the cluster) Admin LDAP account credentials Bind DN (-D): cn=admin,dc=soca,dc=local Password (-y) /root/OpenLdapAdminPassword.txt For example, if you want to create a new group, create a new LDIF file (mynewgroup.ldif) and add the following content: dn : cn = mynewgroup , ou = Group , dc = soca , dc = local objectClass : top objectClass : posixGroup cn : mynewgroup gidNumber : 6000 memberUid : mytestuser Run the following ldapadd command to add your new group: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif adding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\" Finally valid your group has been created correctly using ldapsearch # Validate with Ldapsearch ~ ldapsearch -x cn = mynewgroup #Extended LDIF # # LDAPv3 # base DC=soca,DC=local (default) with scope subtree # filter: cn=mynewgroup # requesting: ALL # # mynewgroup, Group, soca.local dn: cn = mynewgroup,ou = Group,dc = soca,dc = local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: mytestuser Example for LDIF modify operation dn : cn = mynewgroup , ou = Group , dc = soca , dc = local changetype : modify add : memberUid memberUid : anotheruser Example for LDIF delete operation dn : cn = mynewgroup , ou = Group , dc = soca , dc = local changetype : modify delete : memberUid memberUid :: anotheruser # you get the memberUid by running a simple ldapsearch first","title":"Other LDAP operations"},{"location":"tutorials/manage-ldap-users/#give-users-permissions-to-submit-job","text":"By default, users can submit job to any queue, however you can set up ACL at queue level if needed","title":"Give users permissions to submit job"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/","text":"By default, SOCA provision a vanilla AMI and install all required packages in ~3 to 5 minutes. If this cold time is not acceptable for your workload, you can launch AlwaysOn instance or pre-bake your AMI with all required libraries. Step 1: Locate your base AMI \u00b6 Run cat /etc/environment | grep SOCA_INSTALL_AMI on your scheduler host $ ssh -i <key> ec2-user@<ip> Last login: Wed Oct 2 20 :06:47 2019 from <ip> _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-uiupdates > source /etc/environment to SOCA paths [ ec2-user@ip-30-0-1-28 ~ ] $ cat /etc/environment | grep SOCA_INSTALL_AMI export SOCA_INSTALL_AMI = ami-082b5a644766e0e6f [ ec2-user@ip-30-0-1-28 ~ ] $ Step 2: Launch a temporary EC2 instance \u00b6 Launch a new EC2 instance using the SOCA_INSTALL_AMI image Step 3: Pre-configure your AMI \u00b6 Important Step 3 is only required if you want to reduce the time required for your compute node to boot. You can skip this section if you just want to install your customization on your AMI and let SOCA handles PBS/Gnome/System packages installation. 3.1 Pre-Install system packages \u00b6 You can pre-install the packages listed on https://github.com/awslabs/scale-out-computing-on-aws/blob/master/source/scripts/config.cfg . You will need to run yum install for: SYSTEM_PKGS SCHEDULER_PKGS OPENLDAP_SERVER_PKGS SSSD_PKGS Easy Install Copy the content of the config.cfg on your filesystem (say /root/config.cfg ) Run source /root/config.cfg Run the following commands: yum install -y $(echo ${ SYSTEM_PKGS [ * ] } ) yum install -y $(echo ${ SCHEDULER_PKGS [ * ] } ) yum install -y $(echo ${ OPENLDAP_SERVER_PKGS [ * ] } ) yum install -y $(echo ${ SSSD_PKGS [ * ] } ) Here is an example of how you can install packages listed in an array in bash. 3.2: Pre-Install the scheduler \u00b6 To reduce the launch time of your EC2 instance, it's recommended to pre-install PBSPro. First, refer to https://github.com/awslabs/scale-out-computing-on-aws/blob/master/source/scripts/config.cfg and note all PBSPro related variables as you will need to use them below (see highlighted lines): # Sudo as Root sudo su - # Define PBSPro variable export PBSPRO_URL = <variable_from_config.txt> # ex https://github.com/PBSPro/pbspro/releases/download/v18.1.4/pbspro-18.1.4.tar.gz export PBSPRO_TGZ = <variable_from_config.txt> # ex pbspro-18.1.4.tar.gz export PBSPRO_VERSION = <variable_from_config.txt> # ex 18.1.4 # Run the following command to install PBS cd ~ wget $PBSPRO_URL tar zxvf $PBSPRO_TGZ cd pbspro- $PBSPRO_VERSION ./autogen.sh ./configure --prefix = /opt/pbs make -j6 make install -j6 /opt/pbs/libexec/pbs_postinstall chmod 4755 /opt/pbs/sbin/pbs_iff /opt/pbs/sbin/pbs_rcp Installation Path Make sure to install pbspro under /opt/pbs 3.3: (Optional) Pre-Install Gnome \u00b6 If you are using RHEL , run yum groupinstall \"Server with GUI\" -y If you are using Centos , run yum groupinstall \"GNOME Desktop\" -y If you are using Amazon Linux , run yum install -y $(echo ${ DCV_AMAZONLINUX_PKGS [ * ] } ) 3.4: Reboot your EC2 machine \u00b6 3.5: Make sure you do not have any libvirt of firewalld/iptables \u00b6 Post reboot, some distribution may automatically start libvirt or firewall. If that's the case you must delete them otherwise PBS won't be able to contact the master scheduler. To find if you have a running libvirt, run ifconfig and check if you have virbr0 interface such as: ifconfig ens5: flags = 4163 <UP,BROADCAST,RUNNING,MULTICAST> mtu 9001 inet 10 .10.2.19 netmask 255 .255.255.0 broadcast 10 .10.2.255 inet6 fe80::8b1:6aff:fe8a:5ad8 prefixlen 64 scopeid 0x20<link> ether 0a:b1:6a:8a:5a:d8 txqueuelen 1000 ( Ethernet ) RX packets 81 bytes 11842 ( 11 .5 KiB ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 92 bytes 12853 ( 12 .5 KiB ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags = 73 <UP,LOOPBACK,RUNNING> mtu 65536 inet 127 .0.0.1 netmask 255 .0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10<host> loop txqueuelen 1000 ( Local Loopback ) RX packets 8 bytes 601 ( 601 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8 bytes 601 ( 601 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 virbr0: flags = 4099 <UP,BROADCAST,MULTICAST> mtu 1500 inet 192 .168.122.1 netmask 255 .255.255.0 broadcast 192 .168.122.255 ether 52 :54:00:ea:5a:b9 txqueuelen 1000 ( Ethernet ) RX packets 0 bytes 0 ( 0 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 ( 0 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 If that's the case, disable libvirt by running /bin/systemctl disable libvirtd.service ip link set virbr0 down brctl delbr virbr0 Then, make sure you do not have iptables ( iptables -L ) running. If needed, disable firewalld by running /bin/systemctl disable firewalld Step 4: Create your AMI \u00b6 Once you are done, go back to EC2 console, locate your instance and click \"Actions > Image > Create Image\" Choose an AMI name and click 'Create Image'. Your AMI is now being created. Please note it may take a couple of minutes for the AMI to be ready. To check the status, go to EC2 Console and then click \"AMIs\" on the left sidebar Stop your temporary EC2 instance Once your AMI has been created, you can safely terminate the EC2 instance you just launched as you won't need it anymore. Step 5: Test your new AMI \u00b6 # Test 1: Submit a job with a vanilla AMI $ qsub -l instance_type = c5.9xlarge -- /bin/date # Test 2: Submit a job with a pre-configured AMI $ qsub -l instance_type = c5.9xlarge -l instance_ami = ami-0e05219e578020c64 -- /bin/date Results: Test1 (Vanilla): 3 minutes 45 seconds to provision EC2 capacity, register node on SOCA and start the job Test2 (Pre-Configured): 1 minute 44 seconds to provision EC2 capacity, register host on SOCA and start the job Step 6: Update default AMI (Optional) \u00b6 Single job \u00b6 As you are planning to use a custom AMI, you will be required to specify -l instance_ami=<IMAGE_ID> at job submission. It's recommended to go with the \"Entire Queue\" option below if you do not want to manually specify this resource each time you submit a job Entire queue \u00b6 Edit /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml and update the default AMI queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"<YOUR_AMI_ID>\" # <- Add your new AMI instance_type : ... Any jobs running in the queue configured on the queue_mapping will now use your pre-configured AMI by default. You do not need to specify -l instance_ami at job submission anymore. Entire cluster \u00b6 If you want to change the default AMI to use regardless of queue/job, open your Secret Manager console and select your Scale-Out Computing on AWS cluster configuration. Click \u201cRetrieve Secret Value\u201d and then \u201cEdit\u201d. Find the entry \u201cCustomAMI\u201d and update the value with your new AMI ID then click Save","title":"Import custom AMI to provision capacity faster"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-1-locate-your-base-ami","text":"Run cat /etc/environment | grep SOCA_INSTALL_AMI on your scheduler host $ ssh -i <key> ec2-user@<ip> Last login: Wed Oct 2 20 :06:47 2019 from <ip> _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-uiupdates > source /etc/environment to SOCA paths [ ec2-user@ip-30-0-1-28 ~ ] $ cat /etc/environment | grep SOCA_INSTALL_AMI export SOCA_INSTALL_AMI = ami-082b5a644766e0e6f [ ec2-user@ip-30-0-1-28 ~ ] $","title":"Step 1: Locate your base AMI"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-2-launch-a-temporary-ec2-instance","text":"Launch a new EC2 instance using the SOCA_INSTALL_AMI image","title":"Step 2: Launch a temporary EC2 instance"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-3-pre-configure-your-ami","text":"Important Step 3 is only required if you want to reduce the time required for your compute node to boot. You can skip this section if you just want to install your customization on your AMI and let SOCA handles PBS/Gnome/System packages installation.","title":"Step 3: Pre-configure your AMI"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#31-pre-install-system-packages","text":"You can pre-install the packages listed on https://github.com/awslabs/scale-out-computing-on-aws/blob/master/source/scripts/config.cfg . You will need to run yum install for: SYSTEM_PKGS SCHEDULER_PKGS OPENLDAP_SERVER_PKGS SSSD_PKGS Easy Install Copy the content of the config.cfg on your filesystem (say /root/config.cfg ) Run source /root/config.cfg Run the following commands: yum install -y $(echo ${ SYSTEM_PKGS [ * ] } ) yum install -y $(echo ${ SCHEDULER_PKGS [ * ] } ) yum install -y $(echo ${ OPENLDAP_SERVER_PKGS [ * ] } ) yum install -y $(echo ${ SSSD_PKGS [ * ] } ) Here is an example of how you can install packages listed in an array in bash.","title":"3.1 Pre-Install system packages"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#32-pre-install-the-scheduler","text":"To reduce the launch time of your EC2 instance, it's recommended to pre-install PBSPro. First, refer to https://github.com/awslabs/scale-out-computing-on-aws/blob/master/source/scripts/config.cfg and note all PBSPro related variables as you will need to use them below (see highlighted lines): # Sudo as Root sudo su - # Define PBSPro variable export PBSPRO_URL = <variable_from_config.txt> # ex https://github.com/PBSPro/pbspro/releases/download/v18.1.4/pbspro-18.1.4.tar.gz export PBSPRO_TGZ = <variable_from_config.txt> # ex pbspro-18.1.4.tar.gz export PBSPRO_VERSION = <variable_from_config.txt> # ex 18.1.4 # Run the following command to install PBS cd ~ wget $PBSPRO_URL tar zxvf $PBSPRO_TGZ cd pbspro- $PBSPRO_VERSION ./autogen.sh ./configure --prefix = /opt/pbs make -j6 make install -j6 /opt/pbs/libexec/pbs_postinstall chmod 4755 /opt/pbs/sbin/pbs_iff /opt/pbs/sbin/pbs_rcp Installation Path Make sure to install pbspro under /opt/pbs","title":"3.2: Pre-Install the scheduler"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#33-optional-pre-install-gnome","text":"If you are using RHEL , run yum groupinstall \"Server with GUI\" -y If you are using Centos , run yum groupinstall \"GNOME Desktop\" -y If you are using Amazon Linux , run yum install -y $(echo ${ DCV_AMAZONLINUX_PKGS [ * ] } )","title":"3.3: (Optional) Pre-Install Gnome"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#34-reboot-your-ec2-machine","text":"","title":"3.4: Reboot your EC2 machine"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#35-make-sure-you-do-not-have-any-libvirt-of-firewalldiptables","text":"Post reboot, some distribution may automatically start libvirt or firewall. If that's the case you must delete them otherwise PBS won't be able to contact the master scheduler. To find if you have a running libvirt, run ifconfig and check if you have virbr0 interface such as: ifconfig ens5: flags = 4163 <UP,BROADCAST,RUNNING,MULTICAST> mtu 9001 inet 10 .10.2.19 netmask 255 .255.255.0 broadcast 10 .10.2.255 inet6 fe80::8b1:6aff:fe8a:5ad8 prefixlen 64 scopeid 0x20<link> ether 0a:b1:6a:8a:5a:d8 txqueuelen 1000 ( Ethernet ) RX packets 81 bytes 11842 ( 11 .5 KiB ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 92 bytes 12853 ( 12 .5 KiB ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags = 73 <UP,LOOPBACK,RUNNING> mtu 65536 inet 127 .0.0.1 netmask 255 .0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10<host> loop txqueuelen 1000 ( Local Loopback ) RX packets 8 bytes 601 ( 601 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8 bytes 601 ( 601 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 virbr0: flags = 4099 <UP,BROADCAST,MULTICAST> mtu 1500 inet 192 .168.122.1 netmask 255 .255.255.0 broadcast 192 .168.122.255 ether 52 :54:00:ea:5a:b9 txqueuelen 1000 ( Ethernet ) RX packets 0 bytes 0 ( 0 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 ( 0 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 If that's the case, disable libvirt by running /bin/systemctl disable libvirtd.service ip link set virbr0 down brctl delbr virbr0 Then, make sure you do not have iptables ( iptables -L ) running. If needed, disable firewalld by running /bin/systemctl disable firewalld","title":"3.5: Make sure you do not have any libvirt of firewalld/iptables"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-4-create-your-ami","text":"Once you are done, go back to EC2 console, locate your instance and click \"Actions > Image > Create Image\" Choose an AMI name and click 'Create Image'. Your AMI is now being created. Please note it may take a couple of minutes for the AMI to be ready. To check the status, go to EC2 Console and then click \"AMIs\" on the left sidebar Stop your temporary EC2 instance Once your AMI has been created, you can safely terminate the EC2 instance you just launched as you won't need it anymore.","title":"Step 4: Create your AMI"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-5-test-your-new-ami","text":"# Test 1: Submit a job with a vanilla AMI $ qsub -l instance_type = c5.9xlarge -- /bin/date # Test 2: Submit a job with a pre-configured AMI $ qsub -l instance_type = c5.9xlarge -l instance_ami = ami-0e05219e578020c64 -- /bin/date Results: Test1 (Vanilla): 3 minutes 45 seconds to provision EC2 capacity, register node on SOCA and start the job Test2 (Pre-Configured): 1 minute 44 seconds to provision EC2 capacity, register host on SOCA and start the job","title":"Step 5: Test your new AMI"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-6-update-default-ami-optional","text":"","title":"Step 6: Update default AMI (Optional)"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#single-job","text":"As you are planning to use a custom AMI, you will be required to specify -l instance_ami=<IMAGE_ID> at job submission. It's recommended to go with the \"Entire Queue\" option below if you do not want to manually specify this resource each time you submit a job","title":"Single job"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#entire-queue","text":"Edit /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml and update the default AMI queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"<YOUR_AMI_ID>\" # <- Add your new AMI instance_type : ... Any jobs running in the queue configured on the queue_mapping will now use your pre-configured AMI by default. You do not need to specify -l instance_ami at job submission anymore.","title":"Entire queue"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#entire-cluster","text":"If you want to change the default AMI to use regardless of queue/job, open your Secret Manager console and select your Scale-Out Computing on AWS cluster configuration. Click \u201cRetrieve Secret Value\u201d and then \u201cEdit\u201d. Find the entry \u201cCustomAMI\u201d and update the value with your new AMI ID then click Save","title":"Entire cluster"},{"location":"tutorials/troubleshoot-job-queue/","text":"Jobs in dynamic queue \u00b6 First of all, unless you submit a job on the \"alwayson\" queue, it will usually take between 5 to 10 minutes before your job can start as Scale-Out Computing on AWS needs to provision your capacity. This can vary based on the type and number of EC2 instances you have requested for your job. Verify the log \u00b6 If your job is not starting, first verify the queue log under /apps/soca/<CLUSTER_ID>/cluster_manager/logs/<queue_name>.log If the log is not created or you don't see any update on it even though you submitted a job, try to run the dispatcher.py command manually. On the scheduler, list all crontabs as root crontab - and refer to \"Automatic Host Provisioning\" section: ## Automatic Host Provisioning */3 * * * * source /etc/environment ; /apps/soca/<CLUSTER_ID>/python/latest/bin/python3 /apps/soca/<CLUSTER_ID>cluster_manager/dispatcher.py -c /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml -t compute */3 * * * * source /etc/environment ; /apps/soca/<CLUSTER_ID>/python/latest/bin/python3 /apps/soca/<CLUSTER_ID>/cluster_manager/dispatcher.py -c /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml -t desktop */3 * * * * source /etc/environment ; /apps/soca/<CLUSTER_ID>/python/latest/bin/python3 /apps/soca/<CLUSTER_ID>/cluster_manager/dispatcher.py -c /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml -t test Run the command manually (ex source /etc/environment; /apps/soca/<CLUSTER_ID>/python/latest/bin/python3 /apps/soca/<CLUSTER_ID>cluster_manager/dispatcher.py -c /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml -t compute ) and look for any error. Common errors include malformed yaml files. Verify the job resource \u00b6 This guide assume you have created your queue correctly Run qstat -f <job_id> | grep -i resource and try to locate compute_node or stack_id resource. When your job is launched, these resources does not exist. The script dispatcher.py . running as a crontab and executed every 3 minutes will create these resources automatically. Example of job having all resources configured correctly # Job with Scale-Out Computing on AWS resources bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 :compute_node = job2 Resource_List.stack_id = soca-fpgaami-job-2 Please note these resources are created by dispatcher.py so allow a maximum of 3 minutes between job is submitted and resources are visibles on qstat output # Job without Scale-Out Computing on AWS resources created yet bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 If you see a compute_node different than tbd as well as stack_id , that means Scale-Out Computing on AWS triggered capacity provisioning by creating a new CloudFormation stack. If you go to your CloudFormation console, you should see a new stack being created using the following naming convention: soca-<cluster_name>-job-<job_id> If CloudFormation stack is NOT \"CREATE_COMPLETE\" \u00b6 Click on the stack name then check the \"Events\" tab and refer to any \"CREATE_FAILED\" errors In this example, the size of root device is too small and can be fixed by specify a bigger EBS disk using -l root_size=75 If CloudFormation stack is \"CREATE_COMPLETE\" \u00b6 First, make sure CloudFormation has created a new \"Launch Template\" for your job. Then navigate to AutoScaling console, select your AutoScaling group and click \"Activity\". You will see any EC2 errors related in this tab. Here is an example of capacity being provisioned correctly Here is an example of capacity provisioning errors: If capacity is being provisioned correctly, go back to Scale-Out Computing on AWS and run pbsnodes -a . Verify the capacity assigned to your job ID (refer to resources_available.compute_node ) is in state = free . pbsnodes -a ip-60-0-174-166 Mom = ip-60-0-174-166.us-west-2.compute.internal Port = 15002 pbs_version = 18.1.4 ntype = PBS state = free pcpus = 1 resources_available.arch = linux resources_available.availability_zone = us-west-2c resources_available.compute_node = job2 resources_available.host = ip-60-0-174-166 resources_available.instance_type = m5.large resources_available.mem = 7706180kb resources_available.ncpus = 1 resources_available.subnet_id = subnet-0af93e96ed9c4377d resources_available.vnode = ip-60-0-174-166 resources_assigned.accelerator_memory = 0kb resources_assigned.hbmem = 0kb resources_assigned.mem = 0kb resources_assigned.naccelerators = 0 resources_assigned.ncpus = 0 resources_assigned.vmem = 0kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sat Oct 12 17:37:28 2019 If host is not in state = free after 10 minutes, SSH to the host, sudo as root and check the log file located under /root as well as /var/log/message | grep cloud-init","title":"Debug why your jobs are not starting"},{"location":"tutorials/troubleshoot-job-queue/#jobs-in-dynamic-queue","text":"First of all, unless you submit a job on the \"alwayson\" queue, it will usually take between 5 to 10 minutes before your job can start as Scale-Out Computing on AWS needs to provision your capacity. This can vary based on the type and number of EC2 instances you have requested for your job.","title":"Jobs in dynamic queue"},{"location":"tutorials/troubleshoot-job-queue/#verify-the-log","text":"If your job is not starting, first verify the queue log under /apps/soca/<CLUSTER_ID>/cluster_manager/logs/<queue_name>.log If the log is not created or you don't see any update on it even though you submitted a job, try to run the dispatcher.py command manually. On the scheduler, list all crontabs as root crontab - and refer to \"Automatic Host Provisioning\" section: ## Automatic Host Provisioning */3 * * * * source /etc/environment ; /apps/soca/<CLUSTER_ID>/python/latest/bin/python3 /apps/soca/<CLUSTER_ID>cluster_manager/dispatcher.py -c /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml -t compute */3 * * * * source /etc/environment ; /apps/soca/<CLUSTER_ID>/python/latest/bin/python3 /apps/soca/<CLUSTER_ID>/cluster_manager/dispatcher.py -c /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml -t desktop */3 * * * * source /etc/environment ; /apps/soca/<CLUSTER_ID>/python/latest/bin/python3 /apps/soca/<CLUSTER_ID>/cluster_manager/dispatcher.py -c /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml -t test Run the command manually (ex source /etc/environment; /apps/soca/<CLUSTER_ID>/python/latest/bin/python3 /apps/soca/<CLUSTER_ID>cluster_manager/dispatcher.py -c /apps/soca/<CLUSTER_ID>/cluster_manager/settings/queue_mapping.yml -t compute ) and look for any error. Common errors include malformed yaml files.","title":"Verify the log"},{"location":"tutorials/troubleshoot-job-queue/#verify-the-job-resource","text":"This guide assume you have created your queue correctly Run qstat -f <job_id> | grep -i resource and try to locate compute_node or stack_id resource. When your job is launched, these resources does not exist. The script dispatcher.py . running as a crontab and executed every 3 minutes will create these resources automatically. Example of job having all resources configured correctly # Job with Scale-Out Computing on AWS resources bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 :compute_node = job2 Resource_List.stack_id = soca-fpgaami-job-2 Please note these resources are created by dispatcher.py so allow a maximum of 3 minutes between job is submitted and resources are visibles on qstat output # Job without Scale-Out Computing on AWS resources created yet bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 If you see a compute_node different than tbd as well as stack_id , that means Scale-Out Computing on AWS triggered capacity provisioning by creating a new CloudFormation stack. If you go to your CloudFormation console, you should see a new stack being created using the following naming convention: soca-<cluster_name>-job-<job_id>","title":"Verify the job resource"},{"location":"tutorials/troubleshoot-job-queue/#if-cloudformation-stack-is-not-create_complete","text":"Click on the stack name then check the \"Events\" tab and refer to any \"CREATE_FAILED\" errors In this example, the size of root device is too small and can be fixed by specify a bigger EBS disk using -l root_size=75","title":"If CloudFormation stack is NOT \"CREATE_COMPLETE\""},{"location":"tutorials/troubleshoot-job-queue/#if-cloudformation-stack-is-create_complete","text":"First, make sure CloudFormation has created a new \"Launch Template\" for your job. Then navigate to AutoScaling console, select your AutoScaling group and click \"Activity\". You will see any EC2 errors related in this tab. Here is an example of capacity being provisioned correctly Here is an example of capacity provisioning errors: If capacity is being provisioned correctly, go back to Scale-Out Computing on AWS and run pbsnodes -a . Verify the capacity assigned to your job ID (refer to resources_available.compute_node ) is in state = free . pbsnodes -a ip-60-0-174-166 Mom = ip-60-0-174-166.us-west-2.compute.internal Port = 15002 pbs_version = 18.1.4 ntype = PBS state = free pcpus = 1 resources_available.arch = linux resources_available.availability_zone = us-west-2c resources_available.compute_node = job2 resources_available.host = ip-60-0-174-166 resources_available.instance_type = m5.large resources_available.mem = 7706180kb resources_available.ncpus = 1 resources_available.subnet_id = subnet-0af93e96ed9c4377d resources_available.vnode = ip-60-0-174-166 resources_assigned.accelerator_memory = 0kb resources_assigned.hbmem = 0kb resources_assigned.mem = 0kb resources_assigned.naccelerators = 0 resources_assigned.ncpus = 0 resources_assigned.vmem = 0kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sat Oct 12 17:37:28 2019 If host is not in state = free after 10 minutes, SSH to the host, sudo as root and check the log file located under /root as well as /var/log/message | grep cloud-init","title":"If CloudFormation stack is \"CREATE_COMPLETE\""},{"location":"workshops/","text":"About \u00b6 Welcome to the AWS workshop and lab content portal for Scale-Out Computing on AWS! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of how to deploy and operate an elastic, multiuser environment for computationally intensive workflows on the AWS Cloud. The resources on this site include a collection of easy to follow instructions with examples, templates to help you get started and scripts automating tasks supporting the hands-on labs. Prior expertise with AWS and HPC workloads is helpful but not required to complete the labs.","title":"About"},{"location":"workshops/#about","text":"Welcome to the AWS workshop and lab content portal for Scale-Out Computing on AWS! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of how to deploy and operate an elastic, multiuser environment for computationally intensive workflows on the AWS Cloud. The resources on this site include a collection of easy to follow instructions with examples, templates to help you get started and scripts automating tasks supporting the hands-on labs. Prior expertise with AWS and HPC workloads is helpful but not required to complete the labs.","title":"About"},{"location":"workshops/DVCon/","text":"DVCon Workshop Overview \u00b6 Launch a turnkey scale-out compute environment in minutes on AWS \u00b6 The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line. Lab environment at a glance \u00b6 At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"DVCon Workshop Overview"},{"location":"workshops/DVCon/#dvcon-workshop-overview","text":"","title":"DVCon Workshop Overview"},{"location":"workshops/DVCon/#launch-a-turnkey-scale-out-compute-environment-in-minutes-on-aws","text":"The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line.","title":"Launch a turnkey scale-out compute environment in minutes on AWS"},{"location":"workshops/DVCon/#lab-environment-at-a-glance","text":"At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"Lab environment at a glance"},{"location":"workshops/DVCon/getting-started/","text":"Getting Started \u00b6 To begin, you'll log into a temporary AWS account that will be provided to you for this workshop. Accessing your AWS account \u00b6 At the beginning of the workshop, you will be given a 12-character access code . This access code grants you access to a temporary AWS account that you'll use for this workshop. Step 1. Log in \u00b6 Go to https://dashboard.eventengine.run , and enter the access code in the Team Hash field. Click Proceed . Step 2. Get Credentials \u00b6 On the Team Dashboard , click SSH Key to download the SSH Keypair PEM file. You'll use this file later to SSH into an EC2 instance. If your using a Mac, change permissions of the PEM file that you just downloaded. This is an SSH security requirement. chmod 600 /path/to/file.pem Next, click AWS Console to begin the login process to the AWS account. Step 3. Open AWS Console \u00b6 Click Open AWS Console . For this workshop, you will not need the Credentials or CLI Snippets Awesome! Now that you are logged into your temporary AWS account, we can start the labs. Click Next .","title":"Getting Started"},{"location":"workshops/DVCon/getting-started/#getting-started","text":"To begin, you'll log into a temporary AWS account that will be provided to you for this workshop.","title":"Getting Started"},{"location":"workshops/DVCon/getting-started/#accessing-your-aws-account","text":"At the beginning of the workshop, you will be given a 12-character access code . This access code grants you access to a temporary AWS account that you'll use for this workshop.","title":"Accessing your AWS account"},{"location":"workshops/DVCon/getting-started/#step-1-log-in","text":"Go to https://dashboard.eventengine.run , and enter the access code in the Team Hash field. Click Proceed .","title":"Step 1. Log in"},{"location":"workshops/DVCon/getting-started/#step-2-get-credentials","text":"On the Team Dashboard , click SSH Key to download the SSH Keypair PEM file. You'll use this file later to SSH into an EC2 instance. If your using a Mac, change permissions of the PEM file that you just downloaded. This is an SSH security requirement. chmod 600 /path/to/file.pem Next, click AWS Console to begin the login process to the AWS account.","title":"Step 2. Get Credentials"},{"location":"workshops/DVCon/getting-started/#step-3-open-aws-console","text":"Click Open AWS Console . For this workshop, you will not need the Credentials or CLI Snippets Awesome! Now that you are logged into your temporary AWS account, we can start the labs. Click Next .","title":"Step 3. Open AWS Console"},{"location":"workshops/DVCon/modules/01-deploy-env/","text":"Lab 1: Deploy Environment \u00b6 We want you to have hands-on experience deploying your own cluster, and in this module we will have you walk through the process of launching a cluster in your temporary AWS account. This cluster will be provisioned in the background. For the subsequent modules you'll access a pre-built cluster where EDA tools and licenses have been provisioned. Step 1: Launch stack \u00b6 This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Sign in to the AWS Management Console and click the Launch Stack link below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack Verify the launch region is Oregon Important The template must be launched in Oregon for this workshop. On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. We recommend naming it \"dvcon\". Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder scale-out-computing-on-aws/latest The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 10.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address See description REQUIRED The public-facing IP address that is permitted to log into the environment. You can leave it at default, but we recommend you change it to your public-facing IP address. You can find your public-facing IP address at http://checkip.amazonaws.com . Add the /32 suffix to the IP number. Key Pair Name ee-default-keypair REQUIRED Select the ee-default-keypair provided by the workshop. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. By now you've learned how to deploy Scale-Out Computing on AWS in an AWS account. For the purpose of this workshop, you'll login to a pre-built setup that has the following: Synopsys VCS software pre-installed, A license server with valid licenses, and Workshop test case You can now move on to the next lab. Click Next .","title":"Lab 1: Deploy Environment"},{"location":"workshops/DVCon/modules/01-deploy-env/#lab-1-deploy-environment","text":"We want you to have hands-on experience deploying your own cluster, and in this module we will have you walk through the process of launching a cluster in your temporary AWS account. This cluster will be provisioned in the background. For the subsequent modules you'll access a pre-built cluster where EDA tools and licenses have been provisioned.","title":"Lab 1: Deploy Environment"},{"location":"workshops/DVCon/modules/01-deploy-env/#step-1-launch-stack","text":"This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Sign in to the AWS Management Console and click the Launch Stack link below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack Verify the launch region is Oregon Important The template must be launched in Oregon for this workshop. On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. We recommend naming it \"dvcon\". Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder scale-out-computing-on-aws/latest The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 10.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address See description REQUIRED The public-facing IP address that is permitted to log into the environment. You can leave it at default, but we recommend you change it to your public-facing IP address. You can find your public-facing IP address at http://checkip.amazonaws.com . Add the /32 suffix to the IP number. Key Pair Name ee-default-keypair REQUIRED Select the ee-default-keypair provided by the workshop. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. By now you've learned how to deploy Scale-Out Computing on AWS in an AWS account. For the purpose of this workshop, you'll login to a pre-built setup that has the following: Synopsys VCS software pre-installed, A license server with valid licenses, and Workshop test case You can now move on to the next lab. Click Next .","title":"Step 1: Launch stack"},{"location":"workshops/DVCon/modules/02-web-login/","text":"Lab 2: Login to SOCA Console and Launch Remote Desktop Session \u00b6 The goal of this module is to login to SOCA console and start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management console to start and monitor the session. Step 1: Login to SOCA console \u00b6 Click on the link below to login to SOCA console Login to SOCA console Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you need to authorize the browser to trust the self-signed security certificate. In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint. Log in using the web UI using the following credentials: username: user + user id (for example: user1, user12, user24, etc...) password: provided in the session Step 2: Launch remote desktop server \u00b6 Follow these instructions to start a full remote desktop experience in your new cluster: Click Graphical Access on the left sidebar. Select 1 day in the Session Validity popup menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type popup menu. Click Launch my Session #1 After you click Launch my session , a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server. Note You can monitor the deployment of the remote desktop server by observing the status of the corresponding job under \"My Job Queue\" . Please wait till the desktop instance is ready before moving on to the next lab. Click Next once the job status on \"My Job Queue\" is RUNNING .","title":"Lab 2: Login to SOCA Console and Launch Remote Desktop Session"},{"location":"workshops/DVCon/modules/02-web-login/#lab-2-login-to-soca-console-and-launch-remote-desktop-session","text":"The goal of this module is to login to SOCA console and start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management console to start and monitor the session.","title":"Lab 2: Login to SOCA Console and Launch Remote Desktop Session"},{"location":"workshops/DVCon/modules/02-web-login/#step-1-login-to-soca-console","text":"Click on the link below to login to SOCA console Login to SOCA console Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you need to authorize the browser to trust the self-signed security certificate. In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint. Log in using the web UI using the following credentials: username: user + user id (for example: user1, user12, user24, etc...) password: provided in the session","title":"Step 1: Login to SOCA console"},{"location":"workshops/DVCon/modules/02-web-login/#step-2-launch-remote-desktop-server","text":"Follow these instructions to start a full remote desktop experience in your new cluster: Click Graphical Access on the left sidebar. Select 1 day in the Session Validity popup menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type popup menu. Click Launch my Session #1 After you click Launch my session , a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server. Note You can monitor the deployment of the remote desktop server by observing the status of the corresponding job under \"My Job Queue\" . Please wait till the desktop instance is ready before moving on to the next lab. Click Next once the job status on \"My Job Queue\" is RUNNING .","title":"Step 2: Launch remote desktop server"},{"location":"workshops/DVCon/modules/03-login-compile/","text":"Lab 3: Login to Remote Desktop and Compile Design \u00b6 The goal with this lab is to evaluate the remote visualization experience using a graphically intensive EDA tool. Step 1: Log into your session \u00b6 By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com Start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager. Step 2: Copy test case \u00b6 Copy the test case to your home directory by typing cp - r / data / NVDLA_export / fsx / `whoami` at the command prompt and hit enter Note /data is a mount point for Amazon Elastic File System which provides a simple, scalable elastic NFS file system. /fsx is a mount point for Amazon FSx for Lustre which provides a Lustre file system suitable for high performance computing (HPC) workloads such as EDA Change directory to test case cd / fsx / `whoami` / NVDLA_export and hit enter Source environment settings by typing . setup.sh and hit enter Step 3: Compile the test case \u00b6 Change directory to verif/sim cd verif/sim and hit enter Compile the test case by typing make clean comp comp_verdi then hit enter. You should see the test case is getting compiled and at the end should see: You've completed this lab. Click Next .","title":"Lab 3: Login to Remote Desktop and Compile Design"},{"location":"workshops/DVCon/modules/03-login-compile/#lab-3-login-to-remote-desktop-and-compile-design","text":"The goal with this lab is to evaluate the remote visualization experience using a graphically intensive EDA tool.","title":"Lab 3: Login to Remote Desktop and Compile Design"},{"location":"workshops/DVCon/modules/03-login-compile/#step-1-log-into-your-session","text":"By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com Start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager.","title":"Step 1: Log into your session"},{"location":"workshops/DVCon/modules/03-login-compile/#step-2-copy-test-case","text":"Copy the test case to your home directory by typing cp - r / data / NVDLA_export / fsx / `whoami` at the command prompt and hit enter Note /data is a mount point for Amazon Elastic File System which provides a simple, scalable elastic NFS file system. /fsx is a mount point for Amazon FSx for Lustre which provides a Lustre file system suitable for high performance computing (HPC) workloads such as EDA Change directory to test case cd / fsx / `whoami` / NVDLA_export and hit enter Source environment settings by typing . setup.sh and hit enter","title":"Step 2: Copy test case"},{"location":"workshops/DVCon/modules/03-login-compile/#step-3-compile-the-test-case","text":"Change directory to verif/sim cd verif/sim and hit enter Compile the test case by typing make clean comp comp_verdi then hit enter. You should see the test case is getting compiled and at the end should see: You've completed this lab. Click Next .","title":"Step 3: Compile the test case"},{"location":"workshops/DVCon/modules/04-submit-batch/","text":"Lab 4: Submit Batch Jobs \u00b6 This module provides instructions for running example batch jobs in the computing envronment created the Deploy environment module. The workload is a CPU- and IO-intensive logic simulation that is found in integrated circuit design workflows. Step 1: Submit jobs into the queue \u00b6 Next, you'll submit four jobs into the cluster, each job requesting a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Execute the run_tests.sh script which will submit 20 batch jobs to the queue by typing ./run_tests.sh then hit enter. You'll observe that the PBS scheduler will report the corresponding job idsfor each of these 20 jobs. You can examine the run_tests.sh script by typing cat run_tests.sh and observe that for each test we're specifying a different instance_type. This will usually depend on the CPU and memory requirements for the corresponding test. Step 2: Watch job status \u00b6 As soon as jobs are sent to the queue, SOCA automation scripts will create a new compute instance to execute each job. Run the qstat command to view the status of the jobs. You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar. You can run the pbsnodes -aSjL command to see the EC2 instances that have joined the cluster. Initially, the nodes will be in state-unknown,down till they boot-up and join the queue. Note The scheduler is configured to monitor the status of the queues every minute. It typically takes 5-6 minutes to launch a new EC2 instance, boot the operating system, configure it to join the cluster, and have the assigned job to start running. Step 3: Monitor test20 job \u00b6 Monitor the status of test20 job by refreshing the My Job Queue page in SOCA portal and look for the Status column for the job with test20 under Name column. You can also monitor the job status in the terminal by typing qstat command. Once the job is in the running state, look inside test20 directory for test.log and novas.fsdb by typing ls test20/* . Wait until test20/novas.fsdb is created as you'll need to use it in the next lab. Click Next to move to the next lab.","title":"Lab 4: Submit Batch Jobs"},{"location":"workshops/DVCon/modules/04-submit-batch/#lab-4-submit-batch-jobs","text":"This module provides instructions for running example batch jobs in the computing envronment created the Deploy environment module. The workload is a CPU- and IO-intensive logic simulation that is found in integrated circuit design workflows.","title":"Lab 4: Submit Batch Jobs"},{"location":"workshops/DVCon/modules/04-submit-batch/#step-1-submit-jobs-into-the-queue","text":"Next, you'll submit four jobs into the cluster, each job requesting a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Execute the run_tests.sh script which will submit 20 batch jobs to the queue by typing ./run_tests.sh then hit enter. You'll observe that the PBS scheduler will report the corresponding job idsfor each of these 20 jobs. You can examine the run_tests.sh script by typing cat run_tests.sh and observe that for each test we're specifying a different instance_type. This will usually depend on the CPU and memory requirements for the corresponding test.","title":"Step 1: Submit jobs into the queue"},{"location":"workshops/DVCon/modules/04-submit-batch/#step-2-watch-job-status","text":"As soon as jobs are sent to the queue, SOCA automation scripts will create a new compute instance to execute each job. Run the qstat command to view the status of the jobs. You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar. You can run the pbsnodes -aSjL command to see the EC2 instances that have joined the cluster. Initially, the nodes will be in state-unknown,down till they boot-up and join the queue. Note The scheduler is configured to monitor the status of the queues every minute. It typically takes 5-6 minutes to launch a new EC2 instance, boot the operating system, configure it to join the cluster, and have the assigned job to start running.","title":"Step 2: Watch job status"},{"location":"workshops/DVCon/modules/04-submit-batch/#step-3-monitor-test20-job","text":"Monitor the status of test20 job by refreshing the My Job Queue page in SOCA portal and look for the Status column for the job with test20 under Name column. You can also monitor the job status in the terminal by typing qstat command. Once the job is in the running state, look inside test20 directory for test.log and novas.fsdb by typing ls test20/* . Wait until test20/novas.fsdb is created as you'll need to use it in the next lab. Click Next to move to the next lab.","title":"Step 3: Monitor test20 job"},{"location":"workshops/DVCon/modules/05-load-verdi/","text":"Lab 5: Bring up Verdi \u00b6 This module provides instructions for loading a graphical tool to debug a design which is commonly used in integrated cicuit design workflows. Step 1: Start Verdi and load test11 database \u00b6 Start Verdi and load test20 waveform database by typing the command verdi -ssf test20/novas.fsdb . When Verdi GUI comes-up, click Ok to ignore the license expiration warning. Step 2: Select Signals \u00b6 On the lower pane, click on Signal then on the pop-up click on Get Signals On the left pane, expand + top(top) then expand + nvdla_top(NV_nvdla) then click on u_parition_a(NV_NVDLA_partition_a) In the middle pane, click on accu2sc_credit_size[2:0] then hit shift and click on mac_a2accu_data2[175:0] then click OK Step 3: Search for Signal Transition \u00b6 On the lower left pane, click on a2accu_dst_data0[175:0] and click twice on the Search Forward button to find the time where the bus changes to non-zero values and observe the timestamp 419,090,000 ps Step 4: Exit Verdi \u00b6 On the upper pane, click on File menu item then click on Exit to close Verdi. Click Yes on the exit confirmation question Click Next to move to the next lab.","title":"Lab 5: Bring up Verdi"},{"location":"workshops/DVCon/modules/05-load-verdi/#lab-5-bring-up-verdi","text":"This module provides instructions for loading a graphical tool to debug a design which is commonly used in integrated cicuit design workflows.","title":"Lab 5: Bring up Verdi"},{"location":"workshops/DVCon/modules/05-load-verdi/#step-1-start-verdi-and-load-test11-database","text":"Start Verdi and load test20 waveform database by typing the command verdi -ssf test20/novas.fsdb . When Verdi GUI comes-up, click Ok to ignore the license expiration warning.","title":"Step 1: Start Verdi and load test11 database"},{"location":"workshops/DVCon/modules/05-load-verdi/#step-2-select-signals","text":"On the lower pane, click on Signal then on the pop-up click on Get Signals On the left pane, expand + top(top) then expand + nvdla_top(NV_nvdla) then click on u_parition_a(NV_NVDLA_partition_a) In the middle pane, click on accu2sc_credit_size[2:0] then hit shift and click on mac_a2accu_data2[175:0] then click OK","title":"Step 2: Select Signals"},{"location":"workshops/DVCon/modules/05-load-verdi/#step-3-search-for-signal-transition","text":"On the lower left pane, click on a2accu_dst_data0[175:0] and click twice on the Search Forward button to find the time where the bus changes to non-zero values and observe the timestamp 419,090,000 ps","title":"Step 3: Search for Signal Transition"},{"location":"workshops/DVCon/modules/05-load-verdi/#step-4-exit-verdi","text":"On the upper pane, click on File menu item then click on Exit to close Verdi. Click Yes on the exit confirmation question Click Next to move to the next lab.","title":"Step 4: Exit Verdi"},{"location":"workshops/DVCon/modules/06-analytics/","text":"Lab 6: Explore Analytics Dashboard \u00b6 Step 1: Open Cluster Dashboard \u00b6 Return to the your cluster web UI and click on the Analytics section on the left sidebar. Step 2: Add Data to your Cluster \u00b6 By default, job information is ingested by the analytics system on an hourly basis. Log back into the scheduler host via SSH as ec2-user and run the follow command to force immediate ingestion into ElasticSearch: source /etc/environment ; /apps/python/latest/bin/python3 /apps/soca/cluster_analytics/job_tracking.py Step 3: Create Indexes \u00b6 Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data Index Information \u00b6 Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time Examples \u00b6 Cluster Node \u00b6 Job Metadata \u00b6 Generate Graph \u00b6 Money spent by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimate_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used.keyword Order By: metric: Sum of estimated_price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand Jobs per user split by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count Most active projects \u00b6 Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count Instance type launched by user \u00b6 Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count Number of nodes in the cluster \u00b6 Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute Find the price for a given simulation \u00b6 Each job comes with estimated_price_ondemand and estimated_price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Lab 6: Explore Analytics Dashboard"},{"location":"workshops/DVCon/modules/06-analytics/#lab-6-explore-analytics-dashboard","text":"","title":"Lab 6: Explore Analytics Dashboard"},{"location":"workshops/DVCon/modules/06-analytics/#step-1-open-cluster-dashboard","text":"Return to the your cluster web UI and click on the Analytics section on the left sidebar.","title":"Step 1: Open Cluster Dashboard"},{"location":"workshops/DVCon/modules/06-analytics/#step-2-add-data-to-your-cluster","text":"By default, job information is ingested by the analytics system on an hourly basis. Log back into the scheduler host via SSH as ec2-user and run the follow command to force immediate ingestion into ElasticSearch: source /etc/environment ; /apps/python/latest/bin/python3 /apps/soca/cluster_analytics/job_tracking.py","title":"Step 2: Add Data to your Cluster"},{"location":"workshops/DVCon/modules/06-analytics/#step-3-create-indexes","text":"Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data","title":"Step 3: Create Indexes"},{"location":"workshops/DVCon/modules/06-analytics/#index-information","text":"Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time","title":"Index Information"},{"location":"workshops/DVCon/modules/06-analytics/#examples","text":"","title":"Examples"},{"location":"workshops/DVCon/modules/06-analytics/#cluster-node","text":"","title":"Cluster Node"},{"location":"workshops/DVCon/modules/06-analytics/#job-metadata","text":"","title":"Job Metadata"},{"location":"workshops/DVCon/modules/06-analytics/#generate-graph","text":"","title":"Generate Graph"},{"location":"workshops/DVCon/modules/06-analytics/#money-spent-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimate_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used.keyword Order By: metric: Sum of estimated_price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand","title":"Money spent by instance type"},{"location":"workshops/DVCon/modules/06-analytics/#jobs-per-user-split-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count","title":"Jobs per user split by instance type"},{"location":"workshops/DVCon/modules/06-analytics/#most-active-projects","text":"Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count","title":"Most active projects"},{"location":"workshops/DVCon/modules/06-analytics/#instance-type-launched-by-user","text":"Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count","title":"Instance type launched by user"},{"location":"workshops/DVCon/modules/06-analytics/#number-of-nodes-in-the-cluster","text":"Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute","title":"Number of nodes in the cluster"},{"location":"workshops/DVCon/modules/06-analytics/#find-the-price-for-a-given-simulation","text":"Each job comes with estimated_price_ondemand and estimated_price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Find the price for a given simulation"},{"location":"workshops/DVCon/modules/09-wrap-up/","text":"Wrap-up \u00b6 Congratulations! You've completed this workshop. Next steps \u00b6 Please complete the session survey in the mobile app. Clean up \u00b6 No cleanup required! This responsibility falls to AWS.","title":"Wrap-up"},{"location":"workshops/DVCon/modules/09-wrap-up/#wrap-up","text":"Congratulations! You've completed this workshop.","title":"Wrap-up"},{"location":"workshops/DVCon/modules/09-wrap-up/#next-steps","text":"Please complete the session survey in the mobile app.","title":"Next steps"},{"location":"workshops/DVCon/modules/09-wrap-up/#clean-up","text":"No cleanup required! This responsibility falls to AWS.","title":"Clean up"},{"location":"workshops/TKO-Scale-Out-Computing/","text":"Scaling EDA with Vivado Workshop Overview \u00b6 Launch a turnkey scale-out compute environment in minutes on AWS \u00b6 The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line. Lab environment at a glance \u00b6 At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"Scaling EDA with Vivado Workshop Overview"},{"location":"workshops/TKO-Scale-Out-Computing/#scaling-eda-with-vivado-workshop-overview","text":"","title":"Scaling EDA with Vivado Workshop Overview"},{"location":"workshops/TKO-Scale-Out-Computing/#launch-a-turnkey-scale-out-compute-environment-in-minutes-on-aws","text":"The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line.","title":"Launch a turnkey scale-out compute environment in minutes on AWS"},{"location":"workshops/TKO-Scale-Out-Computing/#lab-environment-at-a-glance","text":"At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"Lab environment at a glance"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/","text":"Getting Started \u00b6 Account \u00b6 You will use your Isengard-managed AWS account for this workshop. You should log into an account with full admin privileges. Region \u00b6 You will deploy the CloudFormation stack into the US West (Oregon) . Other regions are not supported in this workshop at this time. Key Pair \u00b6 This solution requires SSH login, so be sure you have a Key Pair for the US West (Oregon) region.","title":"Getting Started"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/#account","text":"You will use your Isengard-managed AWS account for this workshop. You should log into an account with full admin privileges.","title":"Account"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/#region","text":"You will deploy the CloudFormation stack into the US West (Oregon) . Other regions are not supported in this workshop at this time.","title":"Region"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/#key-pair","text":"This solution requires SSH login, so be sure you have a Key Pair for the US West (Oregon) region.","title":"Key Pair"},{"location":"workshops/TKO-Scale-Out-Computing/modules/02-deploy-env/","text":"Lab 1: Deploy Environment \u00b6 Step 1: Launch stack \u00b6 This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Verify that you have a key pair in the US West (Oregon) region. If not, create a new key pair. Sign in to the AWS Management Console and click the link below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack in US West (Oregon) On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign the name \"soca\" to the stack. Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder scale-out-computing-on-aws/latest The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 110.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address 0.0.0.0/0 REQUIRED The public-facing IP address that is permitted to log into the environment. You can leave it at default, but we recommend you change it to your public-facing IP address. Add the /32 suffix to the IP number. Key Pair Name REQUIRED Select your key pair. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. Please wait for instructions from the workshop staff before moving on to the next lab.","title":"Lab 1: Deploy Environment"},{"location":"workshops/TKO-Scale-Out-Computing/modules/02-deploy-env/#lab-1-deploy-environment","text":"","title":"Lab 1: Deploy Environment"},{"location":"workshops/TKO-Scale-Out-Computing/modules/02-deploy-env/#step-1-launch-stack","text":"This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Verify that you have a key pair in the US West (Oregon) region. If not, create a new key pair. Sign in to the AWS Management Console and click the link below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack in US West (Oregon) On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign the name \"soca\" to the stack. Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder scale-out-computing-on-aws/latest The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 110.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address 0.0.0.0/0 REQUIRED The public-facing IP address that is permitted to log into the environment. You can leave it at default, but we recommend you change it to your public-facing IP address. Add the /32 suffix to the IP number. Key Pair Name REQUIRED Select your key pair. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. Please wait for instructions from the workshop staff before moving on to the next lab.","title":"Step 1: Launch stack"},{"location":"workshops/TKO-Scale-Out-Computing/modules/03-configure-desktop/","text":"Lab 2: Configure Remote Desktop \u00b6 Once the solution has been deployed, we will configure the environment to use a specific Amazon Machine Image (AMI) for booting the remote desktop server. As you saw in the architecture diagram , the DCV remote desktop will be your portal into the computing environment. This AMI provides the CentOS 7.5 Linux operating system and the applications you'll use later in the workshop. Obtain IP address of scheduler server In the AWS console, navigate to the CloudFormation page. Select the root stack named \"soca-xxxxxxxxxxxx\", where 'x' is a randomized alpha-numeric string, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Copy the value to the left of ConnectionString . We'll use this command to SSH into the scheduler instance. Connect to the instance over SSH For macOS, paste the SSH command into a terminal on the Mac. Be sure to use the full path to the private key you downloaded earlier. See the steps here in the AWS Connecting to Your Linux Instance using SSH page if you need assistance. For Windows, follow the instructions in the AWS Connect to your instance using PuTTY docs. Once logged in, as sudo, open the file /apps/soca/cluster_manager/settings/queue_mapping.yml using your favorite text editor. Example: sudo vi /apps/soca/cluster_manager/settings/queue_mapping.yml Change the the highlighted values in the file to match the example below. Note Indentation matters in this file. Therefore, we advise against copying and pasting this entire block into the file, as this can result in malformed formatting. Instead, please edit the value of each highlighted line individually. queue_type : compute : queues : [ \"high\" , \"normal\" , \"low\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.large\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" #scratch_size: \"100\" #scratch_iops: \"3600\" #efa_support: \"false\" # .. Refer to the doc for more supported parameters desktop : queues : [ \"desktop\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.2xlarge\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" Save the changes to the file. Keep this SSH session open; you will come back to it later. Click the Next to move on to the next module.","title":"Lab 2: Configure Remote Desktop"},{"location":"workshops/TKO-Scale-Out-Computing/modules/03-configure-desktop/#lab-2-configure-remote-desktop","text":"Once the solution has been deployed, we will configure the environment to use a specific Amazon Machine Image (AMI) for booting the remote desktop server. As you saw in the architecture diagram , the DCV remote desktop will be your portal into the computing environment. This AMI provides the CentOS 7.5 Linux operating system and the applications you'll use later in the workshop. Obtain IP address of scheduler server In the AWS console, navigate to the CloudFormation page. Select the root stack named \"soca-xxxxxxxxxxxx\", where 'x' is a randomized alpha-numeric string, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Copy the value to the left of ConnectionString . We'll use this command to SSH into the scheduler instance. Connect to the instance over SSH For macOS, paste the SSH command into a terminal on the Mac. Be sure to use the full path to the private key you downloaded earlier. See the steps here in the AWS Connecting to Your Linux Instance using SSH page if you need assistance. For Windows, follow the instructions in the AWS Connect to your instance using PuTTY docs. Once logged in, as sudo, open the file /apps/soca/cluster_manager/settings/queue_mapping.yml using your favorite text editor. Example: sudo vi /apps/soca/cluster_manager/settings/queue_mapping.yml Change the the highlighted values in the file to match the example below. Note Indentation matters in this file. Therefore, we advise against copying and pasting this entire block into the file, as this can result in malformed formatting. Instead, please edit the value of each highlighted line individually. queue_type : compute : queues : [ \"high\" , \"normal\" , \"low\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.large\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" #scratch_size: \"100\" #scratch_iops: \"3600\" #efa_support: \"false\" # .. Refer to the doc for more supported parameters desktop : queues : [ \"desktop\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.2xlarge\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" Save the changes to the file. Keep this SSH session open; you will come back to it later. Click the Next to move on to the next module.","title":"Lab 2: Configure Remote Desktop"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/","text":"Lab 3: Launch Remote Desktop Session \u00b6 The goal of this module is to start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management console to start and monitor the session. Step 1: Subscribe to AWS FPGA Developer AMI \u00b6 This workshop requires a subscription to the AWS FPGA Developer AMI in AWS Marketplace . This is the AMI you added to the /apps/soca/cluster_manager/settings/queue_mapping.yml file in the previous lab. It will be used to boot the remote desktop instance in AWS and contains software required later in the workshop. Return to the AWS console for your temporary AWS account. Click here to open the page for the AWS FPGA Developer AMI in AWS Marketplace, and then choose Continue to Subscribe . Review the terms and conditions for software usage, and then choose Accept Terms . When the subscription process is complete, exit out of AWS Marketplace without further action. Do not click Continue to Configure ; the workshop CloudFormation templates will deploy the AMI for you. Click here to verify the subscription within the Marketplace dashboard. You should see the FPGA Developer AMI in the list of subscriptions. Step 2: Log into web UI \u00b6 Select the root stack named \"mod-xxxxxxxx\" again, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Click on the link to the right of WebUserInterface to log into the Web UI Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you must authorize the browser to trust the self-signed security certificate. Log in using the web UI using credentials you provided in the CloudFormation template username and password parameters: Step 3: Launch remote desktop server \u00b6 Follow these instructions to start a full remote desktop experience in your new cluster: Click Graphical Access on the left sidebar. Select 1 day in the Session Validity popup menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type popup menu. Click Launch my Session #1 After you click Launch my session , a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server. Note You can monitor the deployment of the remote desktop server by observing the status of the CloudFormation stack with a name ending in job-0 . If after 5 minutes the status of the stack is not CREATE_COMPLETE , please raise your hand for assistance. Let's move on to the next step while we wait for the desktop instance to launch. Click Next .","title":"Lab 3: Launch Remote Desktop Session"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#lab-3-launch-remote-desktop-session","text":"The goal of this module is to start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management console to start and monitor the session.","title":"Lab 3: Launch Remote Desktop Session"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#step-1-subscribe-to-aws-fpga-developer-ami","text":"This workshop requires a subscription to the AWS FPGA Developer AMI in AWS Marketplace . This is the AMI you added to the /apps/soca/cluster_manager/settings/queue_mapping.yml file in the previous lab. It will be used to boot the remote desktop instance in AWS and contains software required later in the workshop. Return to the AWS console for your temporary AWS account. Click here to open the page for the AWS FPGA Developer AMI in AWS Marketplace, and then choose Continue to Subscribe . Review the terms and conditions for software usage, and then choose Accept Terms . When the subscription process is complete, exit out of AWS Marketplace without further action. Do not click Continue to Configure ; the workshop CloudFormation templates will deploy the AMI for you. Click here to verify the subscription within the Marketplace dashboard. You should see the FPGA Developer AMI in the list of subscriptions.","title":"Step 1: Subscribe to AWS FPGA Developer AMI"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#step-2-log-into-web-ui","text":"Select the root stack named \"mod-xxxxxxxx\" again, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Click on the link to the right of WebUserInterface to log into the Web UI Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you must authorize the browser to trust the self-signed security certificate. Log in using the web UI using credentials you provided in the CloudFormation template username and password parameters:","title":"Step 2: Log into web UI"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#step-3-launch-remote-desktop-server","text":"Follow these instructions to start a full remote desktop experience in your new cluster: Click Graphical Access on the left sidebar. Select 1 day in the Session Validity popup menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type popup menu. Click Launch my Session #1 After you click Launch my session , a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server. Note You can monitor the deployment of the remote desktop server by observing the status of the CloudFormation stack with a name ending in job-0 . If after 5 minutes the status of the stack is not CREATE_COMPLETE , please raise your hand for assistance. Let's move on to the next step while we wait for the desktop instance to launch. Click Next .","title":"Step 3: Launch remote desktop server"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/","text":"Lab 4: Launch Vivado GUI \u00b6 Launch the Vivado CAD Tool \u00b6 Now that your remote desktop is set up, you can launch the Vivado Design Suite (included in the AWS FPGA Developer AMI). The goal with this lab is to evaluate the remote visualization experience using a graphically intensive Computer-Aided Design (CAD) tool. Step 1: Log into your session \u00b6 By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com To launch Vivado, start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager. Type vivado at the command prompt and hit enter The Vivado GUI start and show the following screen: Step 2: Load sample project \u00b6 Next, load a sample workload using one of the included example projects: Go to the Quick Start section and select Open Project . Navigate to /projects/mfg405/ , select the project_1.xpr file, and click OK . Double-click on Open Block Diagram under IP INTEGRATOR in the left-side navigation panel. After the design opens you should see this: You can now click around the GUI and scroll and pan through the schematics to get a sense of the remote desktop experience. For extra credit, double-click on Open Synthesized Design in the navigation panel to see a more complext layout of the design. You've completed this lab. Click Next .","title":"Lab 4: Launch Vivado GUI"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/#lab-4-launch-vivado-gui","text":"","title":"Lab 4: Launch Vivado GUI"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/#launch-the-vivado-cad-tool","text":"Now that your remote desktop is set up, you can launch the Vivado Design Suite (included in the AWS FPGA Developer AMI). The goal with this lab is to evaluate the remote visualization experience using a graphically intensive Computer-Aided Design (CAD) tool.","title":"Launch the Vivado CAD Tool"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/#step-1-log-into-your-session","text":"By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com To launch Vivado, start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager. Type vivado at the command prompt and hit enter The Vivado GUI start and show the following screen:","title":"Step 1: Log into your session"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/#step-2-load-sample-project","text":"Next, load a sample workload using one of the included example projects: Go to the Quick Start section and select Open Project . Navigate to /projects/mfg405/ , select the project_1.xpr file, and click OK . Double-click on Open Block Diagram under IP INTEGRATOR in the left-side navigation panel. After the design opens you should see this: You can now click around the GUI and scroll and pan through the schematics to get a sense of the remote desktop experience. For extra credit, double-click on Open Synthesized Design in the navigation panel to see a more complext layout of the design. You've completed this lab. Click Next .","title":"Step 2: Load sample project"},{"location":"workshops/TKO-Scale-Out-Computing/modules/07-submit-batch/","text":"Lab 5: Submit Batch Workloads \u00b6 This module provides instructions for running an example batch workload in the computing envronment created the Deploy environment module. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows. The workload uses data contained in the public AWS F1 FPGA Development Kit and the Xilinx Vivado EDA software suite provided by the AWS FPGA Developer AMI that you subscribed to in the first tutorial. Although you'll be using data and tools from AWS FPGA developer resources, you will not be running on the F1 FPGA instance or executing any type of FPGA workload; we're simply running software simulations on EC2 compute instances using the design data, IP, and software that these kits provide for no additional charge. Step 1: Clone workload repo \u00b6 End your SSH sessions and log back into the DCV remote desktop session that you established Launch Remote Desktop Session lab. Minimize the Vivado GUI and open a new terminal window. Clone the example workload from the aws-fpga-sa-demo Github repo into your user's home directory on the NFS file system. cd $HOME git clone https://github.com/morrmt/aws-fpga-sa-demo.git Change into the repo's workshop directory. cd $HOME/aws-fpga-sa-demo/eda-workshop Step 2: Submit jobs into the queue \u00b6 Next, you'll submit four jobs into the cluster, each job requesting a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Submit jobs . Submit each of the jobs below: qsub -l instance_type=c5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=c4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME Watch job status . As soon as jobs are sent to the queue, the dispatcher script will start up a new compute instance to execute each job. Run the qstat command to view the status of the jobs. You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar. You can also run the pbsnodes -a command to see the EC2 instances that have joined the cluster. Click Next to move to the next section.","title":"Lab 5: Submit Batch Workloads"},{"location":"workshops/TKO-Scale-Out-Computing/modules/07-submit-batch/#lab-5-submit-batch-workloads","text":"This module provides instructions for running an example batch workload in the computing envronment created the Deploy environment module. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows. The workload uses data contained in the public AWS F1 FPGA Development Kit and the Xilinx Vivado EDA software suite provided by the AWS FPGA Developer AMI that you subscribed to in the first tutorial. Although you'll be using data and tools from AWS FPGA developer resources, you will not be running on the F1 FPGA instance or executing any type of FPGA workload; we're simply running software simulations on EC2 compute instances using the design data, IP, and software that these kits provide for no additional charge.","title":"Lab 5: Submit Batch Workloads"},{"location":"workshops/TKO-Scale-Out-Computing/modules/07-submit-batch/#step-1-clone-workload-repo","text":"End your SSH sessions and log back into the DCV remote desktop session that you established Launch Remote Desktop Session lab. Minimize the Vivado GUI and open a new terminal window. Clone the example workload from the aws-fpga-sa-demo Github repo into your user's home directory on the NFS file system. cd $HOME git clone https://github.com/morrmt/aws-fpga-sa-demo.git Change into the repo's workshop directory. cd $HOME/aws-fpga-sa-demo/eda-workshop","title":"Step 1: Clone workload repo"},{"location":"workshops/TKO-Scale-Out-Computing/modules/07-submit-batch/#step-2-submit-jobs-into-the-queue","text":"Next, you'll submit four jobs into the cluster, each job requesting a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Submit jobs . Submit each of the jobs below: qsub -l instance_type=c5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=c4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME Watch job status . As soon as jobs are sent to the queue, the dispatcher script will start up a new compute instance to execute each job. Run the qstat command to view the status of the jobs. You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar. You can also run the pbsnodes -a command to see the EC2 instances that have joined the cluster. Click Next to move to the next section.","title":"Step 2: Submit jobs into the queue"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/","text":"Lab 6: Configure Budgets \u00b6 This environment supports AWS Budgets and lets you create custom budgets assigned to users, teams, projects, or queues. To prevent over-spending, you can integrate the scheduler with AWS Budgets to take action when customer-defined budget thesholds have been reached. In this example, we will reject job submissions from users who are not assigned to a project that is associated with an AWS Budget . Step 1. Create an AWS Budget \u00b6 Go to the AWS Billing Dashboard , and click Budgets on the left sidebar. Click Create budget . Select Cost budget and click Set your budget . Name your budget \"Project 1\" and set Budgeted amount to $100 . Leave all other fields at their default values. Click Configure alerts . Set Alert threshold to 80 and add an email address to Email contacts . Click Confirm budget to review the configuration, then click Create . Step 2. Enable budget enforcement \u00b6 Next, you will enable the workload scheduler to be aware of the new budget you just created so that it will reject jobs from users who are not members of a project and associated budget. To enable this feature, you will first need to verify the project assigned to each job during submission time. Find the account ID for your temporary AWS account. Click here to go to the Account page of the AWS console. Copy the twelve digit Account Id number located underneath the Account Settings section. Log back into the scheduler instance via SSH and edit the /apps/soca/cluster_hooks/queuejob/check_project_budget.py script and paste the AWS account ID as the value for the aws_account_id variable. Save the file when done. # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/cluster_manager/settings/project_cost_manager.txt' Enable the integration with the scheduler by running the following commands on the scheduler host: sudo -i source /etc/environment qmgr -c \"create hook check_project_budget event=queuejob\" qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/cluster_hooks/queuejob/check_project_budget.py\" Step 3. Test budget enforcement \u00b6 Submit a job without budget assignment \u00b6 Switch to the admin cluster user. sudo su - admin Submit a job. qsub -- /bin/echo Hello This job will be rejected and you will see the following messages: qsub: Error. You tried to submit job without project. Specify project using -P parameter OK, let's add a project tag to comply with the policy. qsub -P \"Project 1\" -- /bin/echo Hello This job will also be rejected: qsub: User morrmt is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt Next, we'll associated the user with \"Project 1\" by adding the username to the project_cost_manager.txt mapping file. As sudo, open the /apps/soca/cluster_manager/settings/project_cost_manager.txt file and add \"Project 1\" budget and the \"admin\" user. # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https: //soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [ Project 1 ] admin Important The config section (\"Project 1\") must match the name of the budget your created in AWS Budgets (it's case sensitive) Save this file and try to submit a job. This time the job will be accepted. The script queries the AWS Budget in real-time. So, if your users are blocked because of a budget restriction, you can at any time edit the value on AWS Budget and unblock them. If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist. Done! On to the next lab. Click Next .","title":"Lab 6: Configure Budgets"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#lab-6-configure-budgets","text":"This environment supports AWS Budgets and lets you create custom budgets assigned to users, teams, projects, or queues. To prevent over-spending, you can integrate the scheduler with AWS Budgets to take action when customer-defined budget thesholds have been reached. In this example, we will reject job submissions from users who are not assigned to a project that is associated with an AWS Budget .","title":"Lab 6: Configure Budgets"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#step-1-create-an-aws-budget","text":"Go to the AWS Billing Dashboard , and click Budgets on the left sidebar. Click Create budget . Select Cost budget and click Set your budget . Name your budget \"Project 1\" and set Budgeted amount to $100 . Leave all other fields at their default values. Click Configure alerts . Set Alert threshold to 80 and add an email address to Email contacts . Click Confirm budget to review the configuration, then click Create .","title":"Step 1. Create an AWS Budget"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#step-2-enable-budget-enforcement","text":"Next, you will enable the workload scheduler to be aware of the new budget you just created so that it will reject jobs from users who are not members of a project and associated budget. To enable this feature, you will first need to verify the project assigned to each job during submission time. Find the account ID for your temporary AWS account. Click here to go to the Account page of the AWS console. Copy the twelve digit Account Id number located underneath the Account Settings section. Log back into the scheduler instance via SSH and edit the /apps/soca/cluster_hooks/queuejob/check_project_budget.py script and paste the AWS account ID as the value for the aws_account_id variable. Save the file when done. # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/cluster_manager/settings/project_cost_manager.txt' Enable the integration with the scheduler by running the following commands on the scheduler host: sudo -i source /etc/environment qmgr -c \"create hook check_project_budget event=queuejob\" qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/cluster_hooks/queuejob/check_project_budget.py\"","title":"Step 2. Enable budget enforcement"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#step-3-test-budget-enforcement","text":"","title":"Step 3. Test budget enforcement"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#submit-a-job-without-budget-assignment","text":"Switch to the admin cluster user. sudo su - admin Submit a job. qsub -- /bin/echo Hello This job will be rejected and you will see the following messages: qsub: Error. You tried to submit job without project. Specify project using -P parameter OK, let's add a project tag to comply with the policy. qsub -P \"Project 1\" -- /bin/echo Hello This job will also be rejected: qsub: User morrmt is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt Next, we'll associated the user with \"Project 1\" by adding the username to the project_cost_manager.txt mapping file. As sudo, open the /apps/soca/cluster_manager/settings/project_cost_manager.txt file and add \"Project 1\" budget and the \"admin\" user. # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https: //soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [ Project 1 ] admin Important The config section (\"Project 1\") must match the name of the budget your created in AWS Budgets (it's case sensitive) Save this file and try to submit a job. This time the job will be accepted. The script queries the AWS Budget in real-time. So, if your users are blocked because of a budget restriction, you can at any time edit the value on AWS Budget and unblock them. If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist. Done! On to the next lab. Click Next .","title":"Submit a job without budget assignment"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/","text":"Lab 7: Explore Analytics Dashboard \u00b6 Step 1: Open Cluster Dashboard \u00b6 Return to the your cluster web UI and click on the Analytics section on the left sidebar. Step 2: Add Data to your Cluster \u00b6 By default, job information is ingested by the analytics system on an hourly basis. Log back into the scheduler host via SSH as ec2-user and run the follow command to force immediate ingestion into ElasticSearch: source /etc/environment ; /apps/python/latest/bin/python3 /apps/soca/cluster_analytics/job_tracking.py Step 3: Create Indexes \u00b6 Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data Index Information \u00b6 Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time Examples \u00b6 Cluster Node \u00b6 Job Metadata \u00b6 Generate Graph \u00b6 Money spent by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimate_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used.keyword Order By: metric: Sum of estimated_price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand Jobs per user split by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count Most active projects \u00b6 Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count Instance type launched by user \u00b6 Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count Number of nodes in the cluster \u00b6 Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute Find the price for a given simulation \u00b6 Each job comes with estimated_price_ondemand and estimated_price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Lab 7: Explore Analytics Dashboard"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#lab-7-explore-analytics-dashboard","text":"","title":"Lab 7: Explore Analytics Dashboard"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#step-1-open-cluster-dashboard","text":"Return to the your cluster web UI and click on the Analytics section on the left sidebar.","title":"Step 1: Open Cluster Dashboard"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#step-2-add-data-to-your-cluster","text":"By default, job information is ingested by the analytics system on an hourly basis. Log back into the scheduler host via SSH as ec2-user and run the follow command to force immediate ingestion into ElasticSearch: source /etc/environment ; /apps/python/latest/bin/python3 /apps/soca/cluster_analytics/job_tracking.py","title":"Step 2: Add Data to your Cluster"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#step-3-create-indexes","text":"Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data","title":"Step 3: Create Indexes"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#index-information","text":"Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time","title":"Index Information"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#examples","text":"","title":"Examples"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#cluster-node","text":"","title":"Cluster Node"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#job-metadata","text":"","title":"Job Metadata"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#generate-graph","text":"","title":"Generate Graph"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#money-spent-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimate_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used.keyword Order By: metric: Sum of estimated_price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand","title":"Money spent by instance type"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#jobs-per-user-split-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count","title":"Jobs per user split by instance type"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#most-active-projects","text":"Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count","title":"Most active projects"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#instance-type-launched-by-user","text":"Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count","title":"Instance type launched by user"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#number-of-nodes-in-the-cluster","text":"Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute","title":"Number of nodes in the cluster"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#find-the-price-for-a-given-simulation","text":"Each job comes with estimated_price_ondemand and estimated_price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Find the price for a given simulation"},{"location":"workshops/TKO-Scale-Out-Computing/modules/09-wrap-up/","text":"Wrap-up \u00b6 Congratulations! You've completed this workshop. Next steps \u00b6 Try this at home! The code for this workshop is open source and publically available. Click on the link to the GitHub repo in the upper-right corner to view the code. Please complete the session survey in the mobile app. Clean up \u00b6 Delete the root stack to remove the resources from your account.","title":"Wrap-up"},{"location":"workshops/TKO-Scale-Out-Computing/modules/09-wrap-up/#wrap-up","text":"Congratulations! You've completed this workshop.","title":"Wrap-up"},{"location":"workshops/TKO-Scale-Out-Computing/modules/09-wrap-up/#next-steps","text":"Try this at home! The code for this workshop is open source and publically available. Click on the link to the GitHub repo in the upper-right corner to view the code. Please complete the session survey in the mobile app.","title":"Next steps"},{"location":"workshops/TKO-Scale-Out-Computing/modules/09-wrap-up/#clean-up","text":"Delete the root stack to remove the resources from your account.","title":"Clean up"},{"location":"workshops/reinvent19-MFG405/","text":"MFG405 Workshop Overview \u00b6 Launch a turnkey scale-out compute environment in minutes on AWS \u00b6 The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line. Lab environment at a glance \u00b6 At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"MFG405 Workshop Overview"},{"location":"workshops/reinvent19-MFG405/#mfg405-workshop-overview","text":"","title":"MFG405 Workshop Overview"},{"location":"workshops/reinvent19-MFG405/#launch-a-turnkey-scale-out-compute-environment-in-minutes-on-aws","text":"The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line.","title":"Launch a turnkey scale-out compute environment in minutes on AWS"},{"location":"workshops/reinvent19-MFG405/#lab-environment-at-a-glance","text":"At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"Lab environment at a glance"},{"location":"workshops/reinvent19-MFG405/getting-started/","text":"Getting Started \u00b6 To begin, you'll log into a temporary AWS account that will be provided to you for this workshop. Accessing your AWS account \u00b6 At the beginning of the workshop, you will be given a 12-character access code . This access code grants you access to a temporary AWS account that you'll use for this workshop. Step 1. Log in \u00b6 Go to https://dashboard.eventengine.run , and enter the access code in the Team Hash field. Click Proceed . Step 2. Get Credentials \u00b6 On the Team Dashboard , click SSH Key to download the SSH Keypair PEM file. You'll use this file later to SSH into an EC2 instance. If your using a Mac, change permissions of the PEM file that you just downloaded. This is an SSH security requirement. chmod 600 /path/to/file.pem Next, click AWS Console to begin the login process to the AWS account. Step 3. Open AWS Console \u00b6 Click Open AWS Console . For this workshop, you will not need the Credentials or CLI Snippets Awesome! Now that you are logged into your temporary AWS account, we can start the labs. Click Next .","title":"Getting Started"},{"location":"workshops/reinvent19-MFG405/getting-started/#getting-started","text":"To begin, you'll log into a temporary AWS account that will be provided to you for this workshop.","title":"Getting Started"},{"location":"workshops/reinvent19-MFG405/getting-started/#accessing-your-aws-account","text":"At the beginning of the workshop, you will be given a 12-character access code . This access code grants you access to a temporary AWS account that you'll use for this workshop.","title":"Accessing your AWS account"},{"location":"workshops/reinvent19-MFG405/getting-started/#step-1-log-in","text":"Go to https://dashboard.eventengine.run , and enter the access code in the Team Hash field. Click Proceed .","title":"Step 1. Log in"},{"location":"workshops/reinvent19-MFG405/getting-started/#step-2-get-credentials","text":"On the Team Dashboard , click SSH Key to download the SSH Keypair PEM file. You'll use this file later to SSH into an EC2 instance. If your using a Mac, change permissions of the PEM file that you just downloaded. This is an SSH security requirement. chmod 600 /path/to/file.pem Next, click AWS Console to begin the login process to the AWS account.","title":"Step 2. Get Credentials"},{"location":"workshops/reinvent19-MFG405/getting-started/#step-3-open-aws-console","text":"Click Open AWS Console . For this workshop, you will not need the Credentials or CLI Snippets Awesome! Now that you are logged into your temporary AWS account, we can start the labs. Click Next .","title":"Step 3. Open AWS Console"},{"location":"workshops/reinvent19-MFG405/modules/02-deploy-env/","text":"Lab 1: Deploy Environment \u00b6 Your temporary AWS account has been pre-provisioned with a multiuser computing environment. You'll use this pre-built cluster for the workshop labs . We also want you to have hands-on experience deploying you're own cluster, and in this module we will have you walk through the process of launching a second cluster in your temporary AWS account. This second cluster will be provisioned in the background while you work on the rest of the workshop in the pre-built cluster. Step 1: Launch stack \u00b6 This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Sign in to the AWS Management Console and click the link below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack Verify the launch region is Oregon Important The template must be launched in Oregon for this workshop. On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. We recommend naming it \"mfg405\". Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder scale-out-computing-on-aws/latest The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 110.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address 0.0.0.0/0 REQUIRED The public-facing IP address that is permitted to log into the environment. You can leave it at default, but we recommend you change it to your public-facing IP address. You can find your public-facing IP address at http://checkip.amazonaws.com . Add the /32 suffix to the IP number. Key Pair Name ee-default-keypair REQUIRED Select the ee-default-keypair provided by the workshop. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. You can now move on to the next lab. Click Next .","title":"Lab 1: Deploy Environment"},{"location":"workshops/reinvent19-MFG405/modules/02-deploy-env/#lab-1-deploy-environment","text":"Your temporary AWS account has been pre-provisioned with a multiuser computing environment. You'll use this pre-built cluster for the workshop labs . We also want you to have hands-on experience deploying you're own cluster, and in this module we will have you walk through the process of launching a second cluster in your temporary AWS account. This second cluster will be provisioned in the background while you work on the rest of the workshop in the pre-built cluster.","title":"Lab 1: Deploy Environment"},{"location":"workshops/reinvent19-MFG405/modules/02-deploy-env/#step-1-launch-stack","text":"This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Sign in to the AWS Management Console and click the link below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack Verify the launch region is Oregon Important The template must be launched in Oregon for this workshop. On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. We recommend naming it \"mfg405\". Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder scale-out-computing-on-aws/latest The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 110.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address 0.0.0.0/0 REQUIRED The public-facing IP address that is permitted to log into the environment. You can leave it at default, but we recommend you change it to your public-facing IP address. You can find your public-facing IP address at http://checkip.amazonaws.com . Add the /32 suffix to the IP number. Key Pair Name ee-default-keypair REQUIRED Select the ee-default-keypair provided by the workshop. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. You can now move on to the next lab. Click Next .","title":"Step 1: Launch stack"},{"location":"workshops/reinvent19-MFG405/modules/03-configure-desktop/","text":"Lab 2: Configure Remote Desktop \u00b6 While the cluster you deployed in the previous lab is running in the background, we'll configure your pre-built cluster to use a specific Amazon Machine Image (AMI) for booting the remote desktop server. As you saw in the architecture diagram , the DCV remote desktop will be your portal into the computing environment. This AMI provides the CentOS 7.5 Linux operating system and the applications you'll use later in the workshop. Obtain IP address of scheduler server In the AWS console, navigate to the CloudFormation page. Select the root stack named \"mod-xxxxxxxxxxxx\", where 'x' is a randomized alpha-numeric string, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Copy the value to the left of ConnectionString . We'll use this command to SSH into the scheduler instance. Connect to the instance over SSH For macOS, paste the SSH command into a terminal on the Mac. Be sure to use the full path to the private key you downloaded earlier. See the steps here in the AWS Connecting to Your Linux Instance using SSH page if you need assistance. For Windows, follow the instructions in the AWS Connect to your instance using PuTTY docs. Once logged in, as sudo, open the file /apps/soca/cluster_manager/settings/queue_mapping.yml using your favorite text editor. Example: sudo vi /apps/soca/cluster_manager/settings/queue_mapping.yml Change the the highlighted values in the file to match the example below. Note We strongly recommend that you do not copy and paste this entire block into the file. Instead, please edit the value of each highlighted line individually. queue_type : compute : queues : [ \"high\" , \"normal\" , \"low\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.large\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" #scratch_size: \"100\" #scratch_iops: \"3600\" #efa_support: \"false\" # .. Refer to the doc for more supported parameters desktop : queues : [ \"desktop\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.2xlarge\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" Save the changes to the file. Keep this SSH session open; you will come back to it later. Click the Next to move on to the next module.","title":"Lab 2: Configure Remote Desktop"},{"location":"workshops/reinvent19-MFG405/modules/03-configure-desktop/#lab-2-configure-remote-desktop","text":"While the cluster you deployed in the previous lab is running in the background, we'll configure your pre-built cluster to use a specific Amazon Machine Image (AMI) for booting the remote desktop server. As you saw in the architecture diagram , the DCV remote desktop will be your portal into the computing environment. This AMI provides the CentOS 7.5 Linux operating system and the applications you'll use later in the workshop. Obtain IP address of scheduler server In the AWS console, navigate to the CloudFormation page. Select the root stack named \"mod-xxxxxxxxxxxx\", where 'x' is a randomized alpha-numeric string, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Copy the value to the left of ConnectionString . We'll use this command to SSH into the scheduler instance. Connect to the instance over SSH For macOS, paste the SSH command into a terminal on the Mac. Be sure to use the full path to the private key you downloaded earlier. See the steps here in the AWS Connecting to Your Linux Instance using SSH page if you need assistance. For Windows, follow the instructions in the AWS Connect to your instance using PuTTY docs. Once logged in, as sudo, open the file /apps/soca/cluster_manager/settings/queue_mapping.yml using your favorite text editor. Example: sudo vi /apps/soca/cluster_manager/settings/queue_mapping.yml Change the the highlighted values in the file to match the example below. Note We strongly recommend that you do not copy and paste this entire block into the file. Instead, please edit the value of each highlighted line individually. queue_type : compute : queues : [ \"high\" , \"normal\" , \"low\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.large\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" #scratch_size: \"100\" #scratch_iops: \"3600\" #efa_support: \"false\" # .. Refer to the doc for more supported parameters desktop : queues : [ \"desktop\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.2xlarge\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" Save the changes to the file. Keep this SSH session open; you will come back to it later. Click the Next to move on to the next module.","title":"Lab 2: Configure Remote Desktop"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/","text":"Lab 3: Launch Remote Desktop Session \u00b6 The goal of this module is to start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management console to start and monitor the session. Step 1: Subscribe to AWS FPGA Developer AMI \u00b6 This workshop requires a subscription to the AWS FPGA Developer AMI in AWS Marketplace . This is the AMI you added to the /apps/soca/cluster_manager/settings/queue_mapping.yml file in the previous lab. It will be used to boot the remote desktop instance in AWS and contains software required later in the workshop. Return to the AWS console for your temporary AWS account. Click here to open the page for the AWS FPGA Developer AMI in AWS Marketplace, and then choose Continue to Subscribe . Review the terms and conditions for software usage, and then choose Accept Terms . When the subscription process is complete, exit out of AWS Marketplace without further action. Do not click Continue to Configure ; the workshop CloudFormation templates will deploy the AMI for you. Click here to verify the subscription within the Marketplace dashboard. You should see the FPGA Developer AMI in the list of subscriptions. Step 2: Log into web UI \u00b6 Select the root stack named \"mod-xxxxxxxx\" again, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Click on the link to the right of WebUserInterface to log into the Web UI Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you must authorize the browser to trust the self-signed security certificate. In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint. Log in using the web UI using the following credentials: username: admin password: passw0rd (use a zero instead of 'o') Step 3: Launch remote desktop server \u00b6 Follow these instructions to start a full remote desktop experience in your new cluster: Click Graphical Access on the left sidebar. Select 1 day in the Session Validity popup menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type popup menu. Click Launch my Session #1 After you click Launch my session , a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server. Note You can monitor the deployment of the remote desktop server by observing the status of the CloudFormation stack with a name ending in job-0 . If after 5 minutes the status of the stack is not CREATE_COMPLETE , please raise your hand for assistance. Let's move on to the next step while we wait for the desktop instance to launch. Click Next .","title":"Lab 3: Launch Remote Desktop Session"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/#lab-3-launch-remote-desktop-session","text":"The goal of this module is to start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management console to start and monitor the session.","title":"Lab 3: Launch Remote Desktop Session"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/#step-1-subscribe-to-aws-fpga-developer-ami","text":"This workshop requires a subscription to the AWS FPGA Developer AMI in AWS Marketplace . This is the AMI you added to the /apps/soca/cluster_manager/settings/queue_mapping.yml file in the previous lab. It will be used to boot the remote desktop instance in AWS and contains software required later in the workshop. Return to the AWS console for your temporary AWS account. Click here to open the page for the AWS FPGA Developer AMI in AWS Marketplace, and then choose Continue to Subscribe . Review the terms and conditions for software usage, and then choose Accept Terms . When the subscription process is complete, exit out of AWS Marketplace without further action. Do not click Continue to Configure ; the workshop CloudFormation templates will deploy the AMI for you. Click here to verify the subscription within the Marketplace dashboard. You should see the FPGA Developer AMI in the list of subscriptions.","title":"Step 1: Subscribe to AWS FPGA Developer AMI"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/#step-2-log-into-web-ui","text":"Select the root stack named \"mod-xxxxxxxx\" again, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Click on the link to the right of WebUserInterface to log into the Web UI Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you must authorize the browser to trust the self-signed security certificate. In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint. Log in using the web UI using the following credentials: username: admin password: passw0rd (use a zero instead of 'o')","title":"Step 2: Log into web UI"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/#step-3-launch-remote-desktop-server","text":"Follow these instructions to start a full remote desktop experience in your new cluster: Click Graphical Access on the left sidebar. Select 1 day in the Session Validity popup menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type popup menu. Click Launch my Session #1 After you click Launch my session , a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server. Note You can monitor the deployment of the remote desktop server by observing the status of the CloudFormation stack with a name ending in job-0 . If after 5 minutes the status of the stack is not CREATE_COMPLETE , please raise your hand for assistance. Let's move on to the next step while we wait for the desktop instance to launch. Click Next .","title":"Step 3: Launch remote desktop server"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/","text":"Lab 4: Launch Vivado GUI \u00b6 Launch the Vivado CAD Tool \u00b6 Now that your remote desktop is set up, you can launch the Vivado Design Suite (included in the AWS FPGA Developer AMI). The goal with this lab is to evaluate the remote visualization experience using a graphically intensive Computer-Aided Design (CAD) tool. Step 1: Log into your session \u00b6 By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com To launch Vivado, start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager. Type vivado at the command prompt and hit enter The Vivado GUI start and show the following screen: Step 2: Load sample project \u00b6 Next, load a sample workload using one of the included example projects: Go to the Quick Start section and select Open Project . Navigate to /projects/mfg405/ , select the project_1.xpr file, and click OK . Double-click on Open Block Diagram under IP INTEGRATOR in the left-side navigation panel. After the design opens you should see this: You can now click around the GUI and scroll and pan through the schematics to get a sense of the remote desktop experience. For extra credit, double-click on Open Synthesized Design in the navigation panel to see a more complext layout of the design. You've completed this lab. Click Next .","title":"Lab 4: Launch Vivado GUI"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/#lab-4-launch-vivado-gui","text":"","title":"Lab 4: Launch Vivado GUI"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/#launch-the-vivado-cad-tool","text":"Now that your remote desktop is set up, you can launch the Vivado Design Suite (included in the AWS FPGA Developer AMI). The goal with this lab is to evaluate the remote visualization experience using a graphically intensive Computer-Aided Design (CAD) tool.","title":"Launch the Vivado CAD Tool"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/#step-1-log-into-your-session","text":"By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com To launch Vivado, start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager. Type vivado at the command prompt and hit enter The Vivado GUI start and show the following screen:","title":"Step 1: Log into your session"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/#step-2-load-sample-project","text":"Next, load a sample workload using one of the included example projects: Go to the Quick Start section and select Open Project . Navigate to /projects/mfg405/ , select the project_1.xpr file, and click OK . Double-click on Open Block Diagram under IP INTEGRATOR in the left-side navigation panel. After the design opens you should see this: You can now click around the GUI and scroll and pan through the schematics to get a sense of the remote desktop experience. For extra credit, double-click on Open Synthesized Design in the navigation panel to see a more complext layout of the design. You've completed this lab. Click Next .","title":"Step 2: Load sample project"},{"location":"workshops/reinvent19-MFG405/modules/07-submit-batch/","text":"Lab 5: Submit Batch Workloads \u00b6 This module provides instructions for running an example batch workload in the computing envronment created the Deploy environment module. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows. The workload uses data contained in the public AWS F1 FPGA Development Kit and the Xilinx Vivado EDA software suite provided by the AWS FPGA Developer AMI that you subscribed to in the first tutorial. Although you'll be using data and tools from AWS FPGA developer resources, you will not be running on the F1 FPGA instance or executing any type of FPGA workload; we're simply running software simulations on EC2 compute instances using the design data, IP, and software that these kits provide for no additional charge. Step 1: Clone workload repo \u00b6 End your SSH sessions and log back into the DCV remote desktop session that you established Launch Remote Desktop Session lab. Minimize the Vivado GUI and open a new terminal window. Clone the example workload from the aws-fpga-sa-demo Github repo into your user's home directory on the NFS file system. cd $HOME git clone https://github.com/morrmt/aws-fpga-sa-demo.git Change into the repo's workshop directory. cd $HOME/aws-fpga-sa-demo/eda-workshop Step 2: Submit jobs into the queue \u00b6 Next, you'll submit four jobs into the cluster, each job requesting a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Submit jobs . Submit each of the jobs below: qsub -l instance_type=c5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=c4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME Watch job status . As soon as jobs are sent to the queue, the dispatcher script will start up a new compute instance to execute each job. Run the qstat command to view the status of the jobs. You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar. You can also run the pbsnodes -a command to see the EC2 instances that have joined the cluster. Click Next to move to the next section.","title":"Lab 5: Submit Batch Workloads"},{"location":"workshops/reinvent19-MFG405/modules/07-submit-batch/#lab-5-submit-batch-workloads","text":"This module provides instructions for running an example batch workload in the computing envronment created the Deploy environment module. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows. The workload uses data contained in the public AWS F1 FPGA Development Kit and the Xilinx Vivado EDA software suite provided by the AWS FPGA Developer AMI that you subscribed to in the first tutorial. Although you'll be using data and tools from AWS FPGA developer resources, you will not be running on the F1 FPGA instance or executing any type of FPGA workload; we're simply running software simulations on EC2 compute instances using the design data, IP, and software that these kits provide for no additional charge.","title":"Lab 5: Submit Batch Workloads"},{"location":"workshops/reinvent19-MFG405/modules/07-submit-batch/#step-1-clone-workload-repo","text":"End your SSH sessions and log back into the DCV remote desktop session that you established Launch Remote Desktop Session lab. Minimize the Vivado GUI and open a new terminal window. Clone the example workload from the aws-fpga-sa-demo Github repo into your user's home directory on the NFS file system. cd $HOME git clone https://github.com/morrmt/aws-fpga-sa-demo.git Change into the repo's workshop directory. cd $HOME/aws-fpga-sa-demo/eda-workshop","title":"Step 1: Clone workload repo"},{"location":"workshops/reinvent19-MFG405/modules/07-submit-batch/#step-2-submit-jobs-into-the-queue","text":"Next, you'll submit four jobs into the cluster, each job requesting a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Submit jobs . Submit each of the jobs below: qsub -l instance_type=c5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=c4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME Watch job status . As soon as jobs are sent to the queue, the dispatcher script will start up a new compute instance to execute each job. Run the qstat command to view the status of the jobs. You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar. You can also run the pbsnodes -a command to see the EC2 instances that have joined the cluster. Click Next to move to the next section.","title":"Step 2: Submit jobs into the queue"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/","text":"Lab 6: Explore Analytics Dashboard \u00b6 Step 1: Open Cluster Dashboard \u00b6 Return to the your cluster web UI and click on the Analytics section on the left sidebar. Step 2: Add Data to your Cluster \u00b6 By default, job information is ingested by the analytics system on an hourly basis. Log back into the scheduler host via SSH as ec2-user and run the follow command to force immediate ingestion into ElasticSearch: source /etc/environment ; /apps/python/latest/bin/python3 /apps/soca/cluster_analytics/job_tracking.py Step 3: Create Indexes \u00b6 Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data Index Information \u00b6 Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time Examples \u00b6 Cluster Node \u00b6 Job Metadata \u00b6 Generate Graph \u00b6 Money spent by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimate_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used.keyword Order By: metric: Sum of estimated_price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand Jobs per user split by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count Most active projects \u00b6 Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count Instance type launched by user \u00b6 Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count Number of nodes in the cluster \u00b6 Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute Find the price for a given simulation \u00b6 Each job comes with estimated_price_ondemand and estimated_price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Lab 6: Explore Analytics Dashboard"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#lab-6-explore-analytics-dashboard","text":"","title":"Lab 6: Explore Analytics Dashboard"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#step-1-open-cluster-dashboard","text":"Return to the your cluster web UI and click on the Analytics section on the left sidebar.","title":"Step 1: Open Cluster Dashboard"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#step-2-add-data-to-your-cluster","text":"By default, job information is ingested by the analytics system on an hourly basis. Log back into the scheduler host via SSH as ec2-user and run the follow command to force immediate ingestion into ElasticSearch: source /etc/environment ; /apps/python/latest/bin/python3 /apps/soca/cluster_analytics/job_tracking.py","title":"Step 2: Add Data to your Cluster"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#step-3-create-indexes","text":"Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data","title":"Step 3: Create Indexes"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#index-information","text":"Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on ElasticSearch) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time","title":"Index Information"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#examples","text":"","title":"Examples"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#cluster-node","text":"","title":"Cluster Node"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#job-metadata","text":"","title":"Job Metadata"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#generate-graph","text":"","title":"Generate Graph"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#money-spent-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimate_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used.keyword Order By: metric: Sum of estimated_price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand","title":"Money spent by instance type"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#jobs-per-user-split-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count","title":"Jobs per user split by instance type"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#most-active-projects","text":"Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count","title":"Most active projects"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#instance-type-launched-by-user","text":"Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count","title":"Instance type launched by user"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#number-of-nodes-in-the-cluster","text":"Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute","title":"Number of nodes in the cluster"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#find-the-price-for-a-given-simulation","text":"Each job comes with estimated_price_ondemand and estimated_price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Find the price for a given simulation"},{"location":"workshops/reinvent19-MFG405/modules/09-wrap-up/","text":"Wrap-up \u00b6 Congratulations! You've completed this workshop. Next steps \u00b6 Try this at home! The code for this workshop is open source and publically available. Please complete the session survey in the mobile app. Clean up \u00b6 No cleanup required! This responsibility falls to AWS.","title":"Wrap-up"},{"location":"workshops/reinvent19-MFG405/modules/09-wrap-up/#wrap-up","text":"Congratulations! You've completed this workshop.","title":"Wrap-up"},{"location":"workshops/reinvent19-MFG405/modules/09-wrap-up/#next-steps","text":"Try this at home! The code for this workshop is open source and publically available. Please complete the session survey in the mobile app.","title":"Next steps"},{"location":"workshops/reinvent19-MFG405/modules/09-wrap-up/#clean-up","text":"No cleanup required! This responsibility falls to AWS.","title":"Clean up"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/","text":". \u00b6 Lab 4 Configure Budgets \u00b6 This environment supports AWS Budgets and lets you create custom budgets assigned to users, teams, projects, or queues. To prevent over-spending, you can integrate the scheduler with AWS Budgets to take action when customer-defined budget thesholds have been reached. In this example, we will reject job submissions from users who are not assigned to a project that is associated with an AWS Budget . Step 1. Create an AWS Budget \u00b6 Go to the AWS Billing Dashboard , and click Budgets on the left sidebar. Click Create budget . Select Cost budget and click Set your budget . Name your budget \"Project 1\" and set Budgeted amount to $100 . Leave all other fields at their default values. Click Configure alerts . Set Alert threshold to 80 and add an email address to Email contacts . Click Confirm budget to review the configuration, then click Create . Step 2. Enable budget enforcement \u00b6 Next, you will enable the workload scheduler to be aware of the new budget you just created so that it will reject jobs from users who are not members of a project and associated budget. To enable this feature, you will first need to verify the project assigned to each job during submission time. Find the account ID for your temporary AWS account. Click here to go to the Account page of the AWS console. Copy the twelve digit Account Id number located underneath the Account Settings section. Log back into the scheduler instance via SSH and edit the /apps/soca/cluster_hooks/queuejob/check_project_budget.py script and paste the AWS account ID as the value for the aws_account_id variable. Save the file when done. # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/cluster_manager/settings/project_cost_manager.txt' Enable the integration with the scheduler by running the following commands on the scheduler host: sudo - i source / etc / environment qmgr - c \"create hook check_project_budget event=queuejob\" qmgr - c \"import hook check_project_budget application/x-python default /apps/soca/cluster_hooks/queuejob/check_project_budget.py\" Step 3. Test budget enforcement \u00b6 Submit a job without budget assignment \u00b6 Switch to the admin cluster user. sudo su - admin Submit a job. qsub -- /bin/echo Hello This job will be rejected and you will see the following messages: qsub : Error . You tried to submit job without project . Specify project using - P parameter OK, let's add a project tag to comply with the policy. qsub -P \"Project 1\" -- /bin/echo Hello This job will also be rejected: qsub : User morrmt is not assigned to any project . See /apps/soca/cluster_manager/settings/ project_cost_manager . txt Next, we'll associated the user with \"Project 1\" by adding the username to the project_cost_manager.txt mapping file. As sudo, open the /apps/soca/cluster_manager/settings/project_cost_manager.txt file and add \"Project 1\" budget and the \"admin\" user. # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https: //soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [ Project 1 ] admin Important The config section (\"Project 1\") must match the name of the budget your created in AWS Budgets (it's case sensitive) Save this file and try to submit a job. This time the job will be accepted. The script queries the AWS Budget in real-time. So, if your users are blocked because of a budget restriction, you can at any time edit the value on AWS Budget and unblock them. If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist. Done! On to the next lab. Click Next .","title":"."},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#_1","text":"","title":"."},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#lab-4-configure-budgets","text":"This environment supports AWS Budgets and lets you create custom budgets assigned to users, teams, projects, or queues. To prevent over-spending, you can integrate the scheduler with AWS Budgets to take action when customer-defined budget thesholds have been reached. In this example, we will reject job submissions from users who are not assigned to a project that is associated with an AWS Budget .","title":"Lab 4 Configure Budgets"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#step-1-create-an-aws-budget","text":"Go to the AWS Billing Dashboard , and click Budgets on the left sidebar. Click Create budget . Select Cost budget and click Set your budget . Name your budget \"Project 1\" and set Budgeted amount to $100 . Leave all other fields at their default values. Click Configure alerts . Set Alert threshold to 80 and add an email address to Email contacts . Click Confirm budget to review the configuration, then click Create .","title":"Step 1. Create an AWS Budget"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#step-2-enable-budget-enforcement","text":"Next, you will enable the workload scheduler to be aware of the new budget you just created so that it will reject jobs from users who are not members of a project and associated budget. To enable this feature, you will first need to verify the project assigned to each job during submission time. Find the account ID for your temporary AWS account. Click here to go to the Account page of the AWS console. Copy the twelve digit Account Id number located underneath the Account Settings section. Log back into the scheduler instance via SSH and edit the /apps/soca/cluster_hooks/queuejob/check_project_budget.py script and paste the AWS account ID as the value for the aws_account_id variable. Save the file when done. # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/cluster_manager/settings/project_cost_manager.txt' Enable the integration with the scheduler by running the following commands on the scheduler host: sudo - i source / etc / environment qmgr - c \"create hook check_project_budget event=queuejob\" qmgr - c \"import hook check_project_budget application/x-python default /apps/soca/cluster_hooks/queuejob/check_project_budget.py\"","title":"Step 2. Enable budget enforcement"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#step-3-test-budget-enforcement","text":"","title":"Step 3. Test budget enforcement"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#submit-a-job-without-budget-assignment","text":"Switch to the admin cluster user. sudo su - admin Submit a job. qsub -- /bin/echo Hello This job will be rejected and you will see the following messages: qsub : Error . You tried to submit job without project . Specify project using - P parameter OK, let's add a project tag to comply with the policy. qsub -P \"Project 1\" -- /bin/echo Hello This job will also be rejected: qsub : User morrmt is not assigned to any project . See /apps/soca/cluster_manager/settings/ project_cost_manager . txt Next, we'll associated the user with \"Project 1\" by adding the username to the project_cost_manager.txt mapping file. As sudo, open the /apps/soca/cluster_manager/settings/project_cost_manager.txt file and add \"Project 1\" budget and the \"admin\" user. # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https: //soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [ Project 1 ] admin Important The config section (\"Project 1\") must match the name of the budget your created in AWS Budgets (it's case sensitive) Save this file and try to submit a job. This time the job will be accepted. The script queries the AWS Budget in real-time. So, if your users are blocked because of a budget restriction, you can at any time edit the value on AWS Budget and unblock them. If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist. Done! On to the next lab. Click Next .","title":"Submit a job without budget assignment"}]}